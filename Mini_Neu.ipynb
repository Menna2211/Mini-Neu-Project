{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GncZ_FwCxFXM"
      },
      "source": [
        "# Mini Neu\n",
        "### Only Numpy , Pandas , Matplotlib allowed\n",
        "\n",
        "In this project, you'll build your first neural network and use it to predict daily bike rental ridership."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "bQFMucF_xFXX"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/E2Edata/Bike-Sharing-Dataset')  # Fix This if your file is in a Certain Directory in Your Drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg1HGP6Xkdxn",
        "outputId": "53dfdfb6-69be-4bfc-c04d-cf981e42c9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJNeciTxxFXZ"
      },
      "source": [
        "### 1- Load the \"Hours csv\" File into a pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "cDqccvYtxFXa"
      },
      "source": [
        "rides =pd.read_csv('hour.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY7ZcsOVxFXa"
      },
      "source": [
        "### 2- Show the Head of the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "MjqiLTMNxFXb",
        "outputId": "93783457-10bf-4427-df1f-a6af7b11ecd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "rides.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
              "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
              "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
              "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
              "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
              "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
              "\n",
              "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
              "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
              "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
              "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
              "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
              "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-091102b9-ea24-40c4-b499-a4e0f5c4ca47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-091102b9-ea24-40c4-b499-a4e0f5c4ca47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-091102b9-ea24-40c4-b499-a4e0f5c4ca47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-091102b9-ea24-40c4-b499-a4e0f5c4ca47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09S4QEbzxFXd"
      },
      "source": [
        "## Let me Show you a Peek on the Data\n",
        "\n",
        "This dataset has the number of riders for each hour of each day from January 1 2011 to December 31 2012. The number of riders is split between casual and registered, summed up in the `cnt` column. You can see the first few rows of the data above.\n",
        "\n",
        "Below is a plot showing the number of bike riders over the first 10 days in the data set. You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You'll be trying to capture all this with your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "tZDl3Q2wxFXe",
        "outputId": "9fee06eb-35bb-40a6-f741-6642629cc67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "# A Pandas Plot\n",
        "rides[:24*10].plot(x='dteday', y='cnt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='dteday'>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature=[ 'season'\t, 'yr',\t'mnth'\t, 'hr'\t, 'holiday' , \t'weekday'\t, 'workingday'\t, 'weathersit']\n",
        "for col in feature:\n",
        "    print(rides[col].value_counts())\n",
        "    print(\"****\"*10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMQiYexXq9id",
        "outputId": "98c00589-4233-4a40-9899-38721780ce45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3    4496\n",
            "2    4409\n",
            "1    4242\n",
            "4    4232\n",
            "Name: season, dtype: int64\n",
            "****************************************\n",
            "1    8734\n",
            "0    8645\n",
            "Name: yr, dtype: int64\n",
            "****************************************\n",
            "5     1488\n",
            "7     1488\n",
            "12    1483\n",
            "8     1475\n",
            "3     1473\n",
            "10    1451\n",
            "6     1440\n",
            "4     1437\n",
            "9     1437\n",
            "11    1437\n",
            "1     1429\n",
            "2     1341\n",
            "Name: mnth, dtype: int64\n",
            "****************************************\n",
            "17    730\n",
            "16    730\n",
            "13    729\n",
            "15    729\n",
            "14    729\n",
            "12    728\n",
            "22    728\n",
            "21    728\n",
            "20    728\n",
            "19    728\n",
            "18    728\n",
            "23    728\n",
            "11    727\n",
            "10    727\n",
            "9     727\n",
            "8     727\n",
            "7     727\n",
            "0     726\n",
            "6     725\n",
            "1     724\n",
            "5     717\n",
            "2     715\n",
            "4     697\n",
            "3     697\n",
            "Name: hr, dtype: int64\n",
            "****************************************\n",
            "0    16879\n",
            "1      500\n",
            "Name: holiday, dtype: int64\n",
            "****************************************\n",
            "6    2512\n",
            "0    2502\n",
            "5    2487\n",
            "1    2479\n",
            "3    2475\n",
            "4    2471\n",
            "2    2453\n",
            "Name: weekday, dtype: int64\n",
            "****************************************\n",
            "1    11865\n",
            "0     5514\n",
            "Name: workingday, dtype: int64\n",
            "****************************************\n",
            "1    11413\n",
            "2     4544\n",
            "3     1419\n",
            "4        3\n",
            "Name: weathersit, dtype: int64\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msLy3MbZxFXe"
      },
      "source": [
        "### Dummy variables\n",
        "Here we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables. This is simple to do with Pandas thanks to `get_dummies()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaIjTm1nxFXf"
      },
      "source": [
        "None    # Some times you need to tell which columns you need to get dummies  df = pd.get_dummies(df, columns=dummy_cols)\n",
        "rides = pd.get_dummies(rides,drop_first=True,columns=[ 'season'\t,\t'mnth'\t, 'hr' , \t'weekday'\t, 'weathersit'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rides.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "2TXHxL3Gt81x",
        "outputId": "f34e3fa3-05e4-4ed0-e8a5-452c5f674a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   instant      dteday  yr  holiday  workingday  temp   atemp   hum  \\\n",
              "0        1  2011-01-01   0        0           0  0.24  0.2879  0.81   \n",
              "1        2  2011-01-01   0        0           0  0.22  0.2727  0.80   \n",
              "2        3  2011-01-01   0        0           0  0.22  0.2727  0.80   \n",
              "3        4  2011-01-01   0        0           0  0.24  0.2879  0.75   \n",
              "4        5  2011-01-01   0        0           0  0.24  0.2879  0.75   \n",
              "\n",
              "   windspeed  casual  ...  hr_23  weekday_1  weekday_2  weekday_3  weekday_4  \\\n",
              "0        0.0       3  ...      0          0          0          0          0   \n",
              "1        0.0       8  ...      0          0          0          0          0   \n",
              "2        0.0       5  ...      0          0          0          0          0   \n",
              "3        0.0       3  ...      0          0          0          0          0   \n",
              "4        0.0       0  ...      0          0          0          0          0   \n",
              "\n",
              "   weekday_5  weekday_6  weathersit_2  weathersit_3  weathersit_4  \n",
              "0          0          1             0             0             0  \n",
              "1          0          1             0             0             0  \n",
              "2          0          1             0             0             0  \n",
              "3          0          1             0             0             0  \n",
              "4          0          1             0             0             0  \n",
              "\n",
              "[5 rows x 58 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e4a4642-1022-400f-ba60-8052f1dbde3a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>yr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>...</th>\n",
              "      <th>hr_23</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "      <th>weathersit_2</th>\n",
              "      <th>weathersit_3</th>\n",
              "      <th>weathersit_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 58 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e4a4642-1022-400f-ba60-8052f1dbde3a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3e4a4642-1022-400f-ba60-8052f1dbde3a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3e4a4642-1022-400f-ba60-8052f1dbde3a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzvtgwOkxFXf"
      },
      "source": [
        "#### Drop Un Necessary Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pDnu_7QxFXg"
      },
      "source": [
        "rides.drop(['instant'],axis = 1, inplace= True)\n",
        "rides.drop(['dteday'],axis = 1, inplace= True)\n",
        "rides.drop(['casual'],axis = 1, inplace= True)\n",
        "rides.drop(['registered'],axis = 1, inplace= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6RQCzyrxFXg"
      },
      "source": [
        "#### Show Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rides"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "XbpFaWYZwP6G",
        "outputId": "809436cf-1e60-4878-9286-b11ed7413f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       yr  holiday  workingday  temp   atemp   hum  windspeed  cnt  season_2  \\\n",
              "0       0        0           0  0.24  0.2879  0.81     0.0000   16         0   \n",
              "1       0        0           0  0.22  0.2727  0.80     0.0000   40         0   \n",
              "2       0        0           0  0.22  0.2727  0.80     0.0000   32         0   \n",
              "3       0        0           0  0.24  0.2879  0.75     0.0000   13         0   \n",
              "4       0        0           0  0.24  0.2879  0.75     0.0000    1         0   \n",
              "...    ..      ...         ...   ...     ...   ...        ...  ...       ...   \n",
              "17374   1        0           1  0.26  0.2576  0.60     0.1642  119         0   \n",
              "17375   1        0           1  0.26  0.2576  0.60     0.1642   89         0   \n",
              "17376   1        0           1  0.26  0.2576  0.60     0.1642   90         0   \n",
              "17377   1        0           1  0.26  0.2727  0.56     0.1343   61         0   \n",
              "17378   1        0           1  0.26  0.2727  0.65     0.1343   49         0   \n",
              "\n",
              "       season_3  ...  hr_23  weekday_1  weekday_2  weekday_3  weekday_4  \\\n",
              "0             0  ...      0          0          0          0          0   \n",
              "1             0  ...      0          0          0          0          0   \n",
              "2             0  ...      0          0          0          0          0   \n",
              "3             0  ...      0          0          0          0          0   \n",
              "4             0  ...      0          0          0          0          0   \n",
              "...         ...  ...    ...        ...        ...        ...        ...   \n",
              "17374         0  ...      0          1          0          0          0   \n",
              "17375         0  ...      0          1          0          0          0   \n",
              "17376         0  ...      0          1          0          0          0   \n",
              "17377         0  ...      0          1          0          0          0   \n",
              "17378         0  ...      1          1          0          0          0   \n",
              "\n",
              "       weekday_5  weekday_6  weathersit_2  weathersit_3  weathersit_4  \n",
              "0              0          1             0             0             0  \n",
              "1              0          1             0             0             0  \n",
              "2              0          1             0             0             0  \n",
              "3              0          1             0             0             0  \n",
              "4              0          1             0             0             0  \n",
              "...          ...        ...           ...           ...           ...  \n",
              "17374          0          0             1             0             0  \n",
              "17375          0          0             1             0             0  \n",
              "17376          0          0             0             0             0  \n",
              "17377          0          0             0             0             0  \n",
              "17378          0          0             0             0             0  \n",
              "\n",
              "[17379 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c687a07c-61a3-4829-a298-6675bcb3b670\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>yr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>cnt</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>...</th>\n",
              "      <th>hr_23</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "      <th>weathersit_2</th>\n",
              "      <th>weathersit_3</th>\n",
              "      <th>weathersit_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17374</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.1642</td>\n",
              "      <td>119</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17375</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.1642</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17376</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.1642</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17377</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17378</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17379 rows × 54 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c687a07c-61a3-4829-a298-6675bcb3b670')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c687a07c-61a3-4829-a298-6675bcb3b670 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c687a07c-61a3-4829-a298-6675bcb3b670');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKGT756UxFXi"
      },
      "source": [
        "### Scaling target variables\n",
        "To make training the network easier, we'll standardize each of the continuous variables. That is, we'll shift and scale the variables such that they have zero mean and a standard deviation of 1.\n",
        "\n",
        "The scaling factors are saved so we can go backwards when we use the network for predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "776W4VMDxFXi"
      },
      "source": [
        "quant_features = [ 'cnt','temp', 'hum', 'windspeed']\n",
        "# Store scalings in a dictionary so we can convert back later\n",
        "scaled_features = {}\n",
        "for each in quant_features:\n",
        "    mean, std = rides[each].mean(), rides[each].std()\n",
        "    scaled_features[each] = [mean, std]\n",
        "    # STANDARDIZE\n",
        "    rides.loc[:, each] = (rides[each] - mean ) /std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2j-vRyOxFXj",
        "outputId": "a4eea544-f2e2-4421-88a3-f4a1e00db05f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "rides"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       yr  holiday  workingday      temp   atemp       hum  windspeed  \\\n",
              "0       0        0           0 -1.334609  0.2879  0.947345  -1.553844   \n",
              "1       0        0           0 -1.438475  0.2727  0.895513  -1.553844   \n",
              "2       0        0           0 -1.438475  0.2727  0.895513  -1.553844   \n",
              "3       0        0           0 -1.334609  0.2879  0.636351  -1.553844   \n",
              "4       0        0           0 -1.334609  0.2879  0.636351  -1.553844   \n",
              "...    ..      ...         ...       ...     ...       ...        ...   \n",
              "17374   1        0           1 -1.230743  0.2576 -0.141133  -0.211685   \n",
              "17375   1        0           1 -1.230743  0.2576 -0.141133  -0.211685   \n",
              "17376   1        0           1 -1.230743  0.2576 -0.141133  -0.211685   \n",
              "17377   1        0           1 -1.230743  0.2727 -0.348463  -0.456086   \n",
              "17378   1        0           1 -1.230743  0.2727  0.118028  -0.456086   \n",
              "\n",
              "            cnt  season_2  season_3  ...  hr_23  weekday_1  weekday_2  \\\n",
              "0     -0.956312         0         0  ...      0          0          0   \n",
              "1     -0.823998         0         0  ...      0          0          0   \n",
              "2     -0.868103         0         0  ...      0          0          0   \n",
              "3     -0.972851         0         0  ...      0          0          0   \n",
              "4     -1.039008         0         0  ...      0          0          0   \n",
              "...         ...       ...       ...  ...    ...        ...        ...   \n",
              "17374 -0.388467         0         0  ...      0          1          0   \n",
              "17375 -0.553859         0         0  ...      0          1          0   \n",
              "17376 -0.548346         0         0  ...      0          1          0   \n",
              "17377 -0.708224         0         0  ...      0          1          0   \n",
              "17378 -0.774381         0         0  ...      1          1          0   \n",
              "\n",
              "       weekday_3  weekday_4  weekday_5  weekday_6  weathersit_2  weathersit_3  \\\n",
              "0              0          0          0          1             0             0   \n",
              "1              0          0          0          1             0             0   \n",
              "2              0          0          0          1             0             0   \n",
              "3              0          0          0          1             0             0   \n",
              "4              0          0          0          1             0             0   \n",
              "...          ...        ...        ...        ...           ...           ...   \n",
              "17374          0          0          0          0             1             0   \n",
              "17375          0          0          0          0             1             0   \n",
              "17376          0          0          0          0             0             0   \n",
              "17377          0          0          0          0             0             0   \n",
              "17378          0          0          0          0             0             0   \n",
              "\n",
              "       weathersit_4  \n",
              "0                 0  \n",
              "1                 0  \n",
              "2                 0  \n",
              "3                 0  \n",
              "4                 0  \n",
              "...             ...  \n",
              "17374             0  \n",
              "17375             0  \n",
              "17376             0  \n",
              "17377             0  \n",
              "17378             0  \n",
              "\n",
              "[17379 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-045d0221-9606-4654-be91-795308efd24b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>yr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>cnt</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>...</th>\n",
              "      <th>hr_23</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "      <th>weathersit_2</th>\n",
              "      <th>weathersit_3</th>\n",
              "      <th>weathersit_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.334609</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.947345</td>\n",
              "      <td>-1.553844</td>\n",
              "      <td>-0.956312</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.438475</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.895513</td>\n",
              "      <td>-1.553844</td>\n",
              "      <td>-0.823998</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.438475</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.895513</td>\n",
              "      <td>-1.553844</td>\n",
              "      <td>-0.868103</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.334609</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.636351</td>\n",
              "      <td>-1.553844</td>\n",
              "      <td>-0.972851</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.334609</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.636351</td>\n",
              "      <td>-1.553844</td>\n",
              "      <td>-1.039008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17374</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.230743</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>-0.141133</td>\n",
              "      <td>-0.211685</td>\n",
              "      <td>-0.388467</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17375</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.230743</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>-0.141133</td>\n",
              "      <td>-0.211685</td>\n",
              "      <td>-0.553859</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17376</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.230743</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>-0.141133</td>\n",
              "      <td>-0.211685</td>\n",
              "      <td>-0.548346</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17377</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.230743</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>-0.348463</td>\n",
              "      <td>-0.456086</td>\n",
              "      <td>-0.708224</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17378</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.230743</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.118028</td>\n",
              "      <td>-0.456086</td>\n",
              "      <td>-0.774381</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17379 rows × 54 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-045d0221-9606-4654-be91-795308efd24b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-045d0221-9606-4654-be91-795308efd24b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-045d0221-9606-4654-be91-795308efd24b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thT1L5KnxFXj"
      },
      "source": [
        "#### Splitting the data into training, testing, and validation sets\n",
        "\n",
        "We'll save the last 21 days of the data to use as a test set after we've trained the network. We'll use this set to make predictions and compare them with the actual number of riders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "iHNsxu-qxFXk"
      },
      "source": [
        "# Save the last 21 days\n",
        "test_data = rides[-21*24:] #  For Testing\n",
        "data = rides[:-21*24]      # For Training\n",
        "\n",
        "# Separate the data into features and targets\n",
        "target_fields = ['cnt']\n",
        "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
        "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7zZT6ENxFXk"
      },
      "source": [
        "We'll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, we'll train on historical data, then try to predict on future data (the validation set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "vaQ2OGvmxFXl"
      },
      "source": [
        "# Hold out the last 60 days of the remaining data as a validation set\n",
        "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
        "val_features, val_targets = features[-60*24:], targets[-60*24:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wkPIUvCxFXl"
      },
      "source": [
        "## Time to build the network\n",
        "\n",
        "Below, you have these tasks:\n",
        "1. Implement the sigmoid function to use as the activation function. Set `self.activation_function` in `__init__` to your sigmoid function.\n",
        "2. Implement the forward pass in the `train` method.\n",
        "3. Implement the backpropagation algorithm in the `train` method, including calculating the output error.\n",
        "4. Implement the forward pass in the `run` method.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Mk2QfgxFXm",
        "outputId": "38486a14-d3f6-45e3-a78c-047f44b1a690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Hint save it for later\n",
        "# From List To 2D Array\n",
        "a = np.array([1,2,3,4], ndmin=2).T\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [2],\n",
              "       [3],\n",
              "       [4]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsXMyXtkxFXm"
      },
      "source": [
        "#### Build Sigmoid Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ump7SIZxxFXn"
      },
      "source": [
        "![sigmoid-equation.png](attachment:sigmoid-equation.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBQol5GixFXn"
      },
      "source": [
        "def sigmoid(x):\n",
        " # x = x.astype('float128')\n",
        "  return  1/(1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TtBcX4UxFXn",
        "outputId": "30d8ee0a-5769-4d24-ef88-51af13cd993e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Test it\n",
        "x= 0.9\n",
        "sigmoid(x)\n",
        "# Output should be 0.622 for X = 0.5\n",
        "# Output should be 0.71 for X = 0.9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7109495026250039"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "0OQvnUD7xFXo"
      },
      "source": [
        "class NeuralNetwork():\n",
        "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "\n",
        "\n",
        "        # Set number of nodes in input, hidden and output layers.\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        # Initialize weights\n",
        "\n",
        "        # Lets Set Layer Weights with Mean = 0 and Std = 1/sqrt(Next Layer Number of Nodes)\n",
        "        # The Matrix Shape  of Weights is based on the Current Layer and the Next Layer\n",
        "\n",
        "        self.weights_input_to_hidden = np.random.normal(0.0, self.hidden_nodes**-0.5,\n",
        "                                       (self.hidden_nodes, self.input_nodes))\n",
        "\n",
        "        self.weights_hidden_to_output = np.random.normal(0.0, self.output_nodes**-0.5,\n",
        "                                       (self.output_nodes, self.hidden_nodes))\n",
        "        self.lr =learning_rate\n",
        "\n",
        "        #### Set this to your implemented sigmoid function ####\n",
        "        # Activation function is the sigmoid function\n",
        "\n",
        "        self.activation_function =sigmoid\n",
        "\n",
        "    def train(self, inputs_list, targets_list):\n",
        "        # Convert inputs list to 2d array  USE THE HINT\n",
        "        inputs = np.array(inputs_list, ndmin=2).T\n",
        "        # Convert Outputs  list to 2d array USE THE HINT\n",
        "        targets = np.array(targets_list, ndmin=2).T\n",
        "        #### Implement the forward pass here ####\n",
        "        ### Forward pass ###\n",
        "        # TODO: Hidden layer\n",
        "\n",
        "        #The Heirarchy\n",
        "        # We have already inputs\n",
        "        # And we calculated input to Hidden Weights\n",
        "        # So we need firstly to  Apply the Dot Product between These Weights and our inputs  and the result be the Hidden inputs\n",
        "        hidden_inputs = np.dot(self.weights_input_to_hidden , inputs)\n",
        "        # Then We need to use Activation Function on the Hidden input to produce the Hidden Output\n",
        "        hidden_outputs = self.activation_function(hidden_inputs)\n",
        "        #print(hidden_outputs)\n",
        "        # TODO: Output layer\n",
        "        # Repat what we did but now we need to use the Weights of Hidden to Output with the Hidden outputs\n",
        "        final_inputs =np.dot(self.weights_hidden_to_output , hidden_outputs )\n",
        "        # Here Actually the Final Outputs layer value is the Final Inputs Cause its Regression output\n",
        "        final_outputs = final_inputs\n",
        "\n",
        "        #### Implement the backward pass here ####\n",
        "        ### Backward pass ###\n",
        "\n",
        "        #TODO: Output error\n",
        "        #Subtract Values our Prediction and The Truth\n",
        "\n",
        "        output_errors = targets -final_outputs\n",
        "\n",
        "        # TODO: Backpropagated error\n",
        "         #Now We are going from Back to Forth So we will take the  transposed Weights of hidden to output with the output errors in Dot Product\n",
        "        hidden_errors = np.dot(output_errors, self.weights_hidden_to_output).T\n",
        "        # and  here we will calculate Gradient let me do it for you\n",
        "        hidden_grad = hidden_errors *  ((hidden_outputs)*(1-hidden_outputs))\n",
        "\n",
        "        # TODO: Update the weights\n",
        "\n",
        "        #Fill this by yourself\n",
        "        self.weights_hidden_to_output += (self.lr) * (hidden_outputs.T *  output_errors)\n",
        "        self.weights_input_to_hidden += (self.lr) * (( inputs.T * hidden_grad))\n",
        "\n",
        "\n",
        "    def run(self, inputs_list):\n",
        "        # Run a forward pass through the network\n",
        "        # You have done it before\n",
        "        # Convery inputs lits to 2D Array  use the Hint\n",
        "        inputs = np.array(inputs_list, ndmin=2).T\n",
        "\n",
        "        #### Implement the forward pass here ####\n",
        "        # TODO: Hidden layer\n",
        "\n",
        "        hidden_inputs = np.dot(self.weights_input_to_hidden , inputs)\n",
        "        hidden_outputs =self.activation_function(hidden_inputs)\n",
        "\n",
        "        # TODO: Output layer\n",
        "        final_inputs = np.dot(self.weights_hidden_to_output , hidden_outputs)\n",
        "        final_outputs = final_inputs\n",
        "\n",
        "        return final_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl9y_XFMxFXv"
      },
      "source": [
        "def MSE(y, Y):\n",
        "    mm= np.mean((y-Y)**2)\n",
        "    print(mm)\n",
        "    return mm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71iU5c7SxFX0"
      },
      "source": [
        "## Training the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OdVcwj6xFX0"
      },
      "source": [
        "import sys\n",
        "\n",
        "### Set the hyperparameters here ###\n",
        "epochs = 5000\n",
        "learning_rate = 0.01\n",
        "hidden_nodes = 28\n",
        "output_nodes = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "JuFwYUd9xFX1",
        "outputId": "3316be91-d3ea-442f-a3d9-5381e152caa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "N_i = train_features.shape[1]\n",
        "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
        "\n",
        "losses = {'train':[], 'validation':[]}\n",
        "\n",
        "\n",
        "for e in range(epochs):\n",
        "    # Go through a random batch of 128 records from the training data set\n",
        "    batch = np.random.choice(train_features.index, size=128)\n",
        "    for record, target in zip(train_features.loc[batch].values,train_targets.loc[batch]['cnt']):\n",
        "        network.train(record, target)\n",
        "\n",
        "    # Printing out the training progress\n",
        "    train_loss = MSE(network.run(train_features), train_targets['cnt'].values)\n",
        "    val_loss = MSE(network.run(val_features), val_targets['cnt'].values)\n",
        "    sys.stdout.write(\"\\rProgress: \" + str(100 * e/float(epochs))[:4] \\\n",
        "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
        "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
        "\n",
        "    losses['train'].append(train_loss)\n",
        "    losses['validation'].append(val_loss)\n",
        "print('')\n",
        "print('Finally')\n",
        "print(\"Finished training.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0.16544040596917597\n",
            "Progress: 50.0% ... Training loss: 0.067 ... Validation loss: 0.1650.066300277440734\n",
            "0.17095699611826032\n",
            "Progress: 50.0% ... Training loss: 0.066 ... Validation loss: 0.1700.0704623690818287\n",
            "0.20931089217014862\n",
            "Progress: 50.0% ... Training loss: 0.070 ... Validation loss: 0.2090.06583650162892823\n",
            "0.1796666369332\n",
            "Progress: 50.0% ... Training loss: 0.065 ... Validation loss: 0.1790.06853754192923273\n",
            "0.169255544654163\n",
            "Progress: 50.1% ... Training loss: 0.068 ... Validation loss: 0.1690.06626335882943409\n",
            "0.15946574032538607\n",
            "Progress: 50.1% ... Training loss: 0.066 ... Validation loss: 0.1590.08675357948198983\n",
            "0.16844822800207318\n",
            "Progress: 50.1% ... Training loss: 0.086 ... Validation loss: 0.1680.07474964620924213\n",
            "0.15803671557416543\n",
            "Progress: 50.1% ... Training loss: 0.074 ... Validation loss: 0.1580.06804717072464626\n",
            "0.15795221423698913\n",
            "Progress: 50.1% ... Training loss: 0.068 ... Validation loss: 0.1570.06484968968301036\n",
            "0.16173374754382586\n",
            "Progress: 50.2% ... Training loss: 0.064 ... Validation loss: 0.1610.06436134885619277\n",
            "0.15687119739089372\n",
            "Progress: 50.2% ... Training loss: 0.064 ... Validation loss: 0.1560.06951058613348356\n",
            "0.17642862077752738\n",
            "Progress: 50.2% ... Training loss: 0.069 ... Validation loss: 0.1760.06643198347793071\n",
            "0.15518348115859273\n",
            "Progress: 50.2% ... Training loss: 0.066 ... Validation loss: 0.1550.06515489223336543\n",
            "0.16855897190820426\n",
            "Progress: 50.2% ... Training loss: 0.065 ... Validation loss: 0.1680.06979552949630682\n",
            "0.18436138350565787\n",
            "Progress: 50.3% ... Training loss: 0.069 ... Validation loss: 0.1840.09255788839217673\n",
            "0.1592566437303081\n",
            "Progress: 50.3% ... Training loss: 0.092 ... Validation loss: 0.1590.06537652035381163\n",
            "0.15784157639539115\n",
            "Progress: 50.3% ... Training loss: 0.065 ... Validation loss: 0.1570.06540465544348122\n",
            "0.15621219589261529\n",
            "Progress: 50.3% ... Training loss: 0.065 ... Validation loss: 0.1560.07203189639271447\n",
            "0.18312879390693643\n",
            "Progress: 50.3% ... Training loss: 0.072 ... Validation loss: 0.1830.06417098452185516\n",
            "0.16302812552794588\n",
            "Progress: 50.4% ... Training loss: 0.064 ... Validation loss: 0.1630.06877304299183563\n",
            "0.15408759806903052\n",
            "Progress: 50.4% ... Training loss: 0.068 ... Validation loss: 0.1540.0668194371250992\n",
            "0.1629864833501959\n",
            "Progress: 50.4% ... Training loss: 0.066 ... Validation loss: 0.1620.06474872562125576\n",
            "0.15881439615005058\n",
            "Progress: 50.4% ... Training loss: 0.064 ... Validation loss: 0.1580.0731070447924554\n",
            "0.15988413152415712\n",
            "Progress: 50.4% ... Training loss: 0.073 ... Validation loss: 0.1590.07109297543005627\n",
            "0.16438109811256757\n",
            "Progress: 50.5% ... Training loss: 0.071 ... Validation loss: 0.1640.06845662388016228\n",
            "0.18445861384365306\n",
            "Progress: 50.5% ... Training loss: 0.068 ... Validation loss: 0.1840.06923882530243476\n",
            "0.21395871265706923\n",
            "Progress: 50.5% ... Training loss: 0.069 ... Validation loss: 0.2130.06513521982963918\n",
            "0.18394107976194393\n",
            "Progress: 50.5% ... Training loss: 0.065 ... Validation loss: 0.1830.06420156865330721\n",
            "0.18346582402962494\n",
            "Progress: 50.5% ... Training loss: 0.064 ... Validation loss: 0.1830.06860462843345798\n",
            "0.18427236048468432\n",
            "Progress: 50.6% ... Training loss: 0.068 ... Validation loss: 0.1840.0665401902529489\n",
            "0.17681337228877314\n",
            "Progress: 50.6% ... Training loss: 0.066 ... Validation loss: 0.1760.06537429079375628\n",
            "0.1851826747969511\n",
            "Progress: 50.6% ... Training loss: 0.065 ... Validation loss: 0.1850.080842351678054\n",
            "0.16779125432360414\n",
            "Progress: 50.6% ... Training loss: 0.080 ... Validation loss: 0.1670.06584571678025698\n",
            "0.16343285423273513\n",
            "Progress: 50.6% ... Training loss: 0.065 ... Validation loss: 0.1630.06638361873397627\n",
            "0.17165274097799854\n",
            "Progress: 50.7% ... Training loss: 0.066 ... Validation loss: 0.1710.06898410434989788\n",
            "0.1851866544281879\n",
            "Progress: 50.7% ... Training loss: 0.068 ... Validation loss: 0.1850.07310417933865862\n",
            "0.1857473166633896\n",
            "Progress: 50.7% ... Training loss: 0.073 ... Validation loss: 0.1850.07112441251591442\n",
            "0.1693372837854114\n",
            "Progress: 50.7% ... Training loss: 0.071 ... Validation loss: 0.1690.07279811544692204\n",
            "0.20461992432739634\n",
            "Progress: 50.7% ... Training loss: 0.072 ... Validation loss: 0.2040.06663236016064542\n",
            "0.17865057773660778\n",
            "Progress: 50.8% ... Training loss: 0.066 ... Validation loss: 0.1780.07219335624706906\n",
            "0.1635093932620119\n",
            "Progress: 50.8% ... Training loss: 0.072 ... Validation loss: 0.1630.0712651718937912\n",
            "0.203730872890391\n",
            "Progress: 50.8% ... Training loss: 0.071 ... Validation loss: 0.2030.06681383746795047\n",
            "0.18372639247975497\n",
            "Progress: 50.8% ... Training loss: 0.066 ... Validation loss: 0.1830.06656276171726858\n",
            "0.17482689616278635\n",
            "Progress: 50.8% ... Training loss: 0.066 ... Validation loss: 0.1740.06761108595949947\n",
            "0.15790279027224594\n",
            "Progress: 50.9% ... Training loss: 0.067 ... Validation loss: 0.1570.07174790683593558\n",
            "0.19912358950516146\n",
            "Progress: 50.9% ... Training loss: 0.071 ... Validation loss: 0.1990.0648489524696941\n",
            "0.1768377446121811\n",
            "Progress: 50.9% ... Training loss: 0.064 ... Validation loss: 0.1760.06962848893133079\n",
            "0.1672603000559203\n",
            "Progress: 50.9% ... Training loss: 0.069 ... Validation loss: 0.1670.06910141756182242\n",
            "0.16399075915536107\n",
            "Progress: 50.9% ... Training loss: 0.069 ... Validation loss: 0.1630.06525079249529087\n",
            "0.1632878533517631\n",
            "Progress: 51.0% ... Training loss: 0.065 ... Validation loss: 0.1630.06472875413443517\n",
            "0.1635555378479711\n",
            "Progress: 51.0% ... Training loss: 0.064 ... Validation loss: 0.1630.064642135946243\n",
            "0.1589679389970937\n",
            "Progress: 51.0% ... Training loss: 0.064 ... Validation loss: 0.1580.07765078374933372\n",
            "0.17089263537677302\n",
            "Progress: 51.0% ... Training loss: 0.077 ... Validation loss: 0.1700.0686334097064334\n",
            "0.18741463596239838\n",
            "Progress: 51.0% ... Training loss: 0.068 ... Validation loss: 0.1870.06505963898891266\n",
            "0.1745679098323769\n",
            "Progress: 51.1% ... Training loss: 0.065 ... Validation loss: 0.1740.06766758949258887\n",
            "0.18796205793396029\n",
            "Progress: 51.1% ... Training loss: 0.067 ... Validation loss: 0.1870.06454716994402546\n",
            "0.16913991485562566\n",
            "Progress: 51.1% ... Training loss: 0.064 ... Validation loss: 0.1690.06711710466813953\n",
            "0.20057947842548648\n",
            "Progress: 51.1% ... Training loss: 0.067 ... Validation loss: 0.2000.06747287245843185\n",
            "0.19382318342763177\n",
            "Progress: 51.1% ... Training loss: 0.067 ... Validation loss: 0.1930.06587250871901702\n",
            "0.18822232290581803\n",
            "Progress: 51.2% ... Training loss: 0.065 ... Validation loss: 0.1880.06586609127648806\n",
            "0.16419273338061482\n",
            "Progress: 51.2% ... Training loss: 0.065 ... Validation loss: 0.1640.06409669626887392\n",
            "0.16812985760240814\n",
            "Progress: 51.2% ... Training loss: 0.064 ... Validation loss: 0.1680.06942100355466883\n",
            "0.1514064749421411\n",
            "Progress: 51.2% ... Training loss: 0.069 ... Validation loss: 0.1510.06640477692929296\n",
            "0.17313244573042433\n",
            "Progress: 51.2% ... Training loss: 0.066 ... Validation loss: 0.1730.0793892438932634\n",
            "0.21540508287090576\n",
            "Progress: 51.3% ... Training loss: 0.079 ... Validation loss: 0.2150.06441220901414169\n",
            "0.15593990109984993\n",
            "Progress: 51.3% ... Training loss: 0.064 ... Validation loss: 0.1550.06652039911264031\n",
            "0.1631856516576921\n",
            "Progress: 51.3% ... Training loss: 0.066 ... Validation loss: 0.1630.06509330481057009\n",
            "0.15245417083377977\n",
            "Progress: 51.3% ... Training loss: 0.065 ... Validation loss: 0.1520.06902444891575739\n",
            "0.16669851966420018\n",
            "Progress: 51.3% ... Training loss: 0.069 ... Validation loss: 0.1660.06369922808622817\n",
            "0.15747520618749825\n",
            "Progress: 51.4% ... Training loss: 0.063 ... Validation loss: 0.1570.06775403958626346\n",
            "0.1564035816622313\n",
            "Progress: 51.4% ... Training loss: 0.067 ... Validation loss: 0.1560.07070542174561362\n",
            "0.2135865314567985\n",
            "Progress: 51.4% ... Training loss: 0.070 ... Validation loss: 0.2130.06512019503477255\n",
            "0.17526272398258927\n",
            "Progress: 51.4% ... Training loss: 0.065 ... Validation loss: 0.1750.0779863728564005\n",
            "0.15351115069185173\n",
            "Progress: 51.4% ... Training loss: 0.077 ... Validation loss: 0.1530.06637860501192303\n",
            "0.16848656233106726\n",
            "Progress: 51.5% ... Training loss: 0.066 ... Validation loss: 0.1680.06737859492927213\n",
            "0.16534132163283832\n",
            "Progress: 51.5% ... Training loss: 0.067 ... Validation loss: 0.1650.06458957261460783\n",
            "0.16081766438912198\n",
            "Progress: 51.5% ... Training loss: 0.064 ... Validation loss: 0.1600.06445036911244287\n",
            "0.1676658042007034\n",
            "Progress: 51.5% ... Training loss: 0.064 ... Validation loss: 0.1670.06637996437244208\n",
            "0.15553219602478707\n",
            "Progress: 51.5% ... Training loss: 0.066 ... Validation loss: 0.1550.06688316599016678\n",
            "0.1507485015140779\n",
            "Progress: 51.6% ... Training loss: 0.066 ... Validation loss: 0.1500.06583291983741439\n",
            "0.15577941249603725\n",
            "Progress: 51.6% ... Training loss: 0.065 ... Validation loss: 0.1550.0661862317963859\n",
            "0.16197317878499237\n",
            "Progress: 51.6% ... Training loss: 0.066 ... Validation loss: 0.1610.06429144182130923\n",
            "0.1635912582715588\n",
            "Progress: 51.6% ... Training loss: 0.064 ... Validation loss: 0.1630.0669020952018619\n",
            "0.18167895419509353\n",
            "Progress: 51.6% ... Training loss: 0.066 ... Validation loss: 0.1810.06489503473796181\n",
            "0.1693097070412738\n",
            "Progress: 51.7% ... Training loss: 0.064 ... Validation loss: 0.1690.07194001834457996\n",
            "0.20323106320192974\n",
            "Progress: 51.7% ... Training loss: 0.071 ... Validation loss: 0.2030.06499703684134821\n",
            "0.18250034885383232\n",
            "Progress: 51.7% ... Training loss: 0.064 ... Validation loss: 0.1820.06565954559794111\n",
            "0.20187453850714748\n",
            "Progress: 51.7% ... Training loss: 0.065 ... Validation loss: 0.2010.067333974764049\n",
            "0.1923895945516879\n",
            "Progress: 51.7% ... Training loss: 0.067 ... Validation loss: 0.1920.06459936606505844\n",
            "0.15575211578536594\n",
            "Progress: 51.8% ... Training loss: 0.064 ... Validation loss: 0.1550.06364693163190764\n",
            "0.16586812599061718\n",
            "Progress: 51.8% ... Training loss: 0.063 ... Validation loss: 0.1650.06457330503847396\n",
            "0.16146152168616754\n",
            "Progress: 51.8% ... Training loss: 0.064 ... Validation loss: 0.1610.0688288425886584\n",
            "0.15261939282796866\n",
            "Progress: 51.8% ... Training loss: 0.068 ... Validation loss: 0.1520.06636684366977334\n",
            "0.1567502229550747\n",
            "Progress: 51.8% ... Training loss: 0.066 ... Validation loss: 0.1560.0631788432821145\n",
            "0.15849229007959\n",
            "Progress: 51.9% ... Training loss: 0.063 ... Validation loss: 0.1580.06906729574848781\n",
            "0.1950234067079443\n",
            "Progress: 51.9% ... Training loss: 0.069 ... Validation loss: 0.1950.07144617420209476\n",
            "0.14394840704709766\n",
            "Progress: 51.9% ... Training loss: 0.071 ... Validation loss: 0.1430.0703955220395346\n",
            "0.1654469466684474\n",
            "Progress: 51.9% ... Training loss: 0.070 ... Validation loss: 0.1650.06559654874503931\n",
            "0.16918340057596526\n",
            "Progress: 51.9% ... Training loss: 0.065 ... Validation loss: 0.1690.06952477220796574\n",
            "0.1709317512185998\n",
            "Progress: 52.0% ... Training loss: 0.069 ... Validation loss: 0.1700.07020397313981556\n",
            "0.16866891201337733\n",
            "Progress: 52.0% ... Training loss: 0.070 ... Validation loss: 0.1680.06663116846570141\n",
            "0.19331041447008473\n",
            "Progress: 52.0% ... Training loss: 0.066 ... Validation loss: 0.1930.06399343538033078\n",
            "0.17115593611765034\n",
            "Progress: 52.0% ... Training loss: 0.063 ... Validation loss: 0.1710.06568379595390002\n",
            "0.16345556510320786\n",
            "Progress: 52.0% ... Training loss: 0.065 ... Validation loss: 0.1630.06536234434157849\n",
            "0.1591769195114405\n",
            "Progress: 52.1% ... Training loss: 0.065 ... Validation loss: 0.1590.06564052908556346\n",
            "0.16942599703431382\n",
            "Progress: 52.1% ... Training loss: 0.065 ... Validation loss: 0.1690.06931990147073598\n",
            "0.15012791782818877\n",
            "Progress: 52.1% ... Training loss: 0.069 ... Validation loss: 0.1500.06581103742912235\n",
            "0.15958336274800058\n",
            "Progress: 52.1% ... Training loss: 0.065 ... Validation loss: 0.1590.06825739669760497\n",
            "0.15973426538888194\n",
            "Progress: 52.1% ... Training loss: 0.068 ... Validation loss: 0.1590.06574112230190919\n",
            "0.1727710843142895\n",
            "Progress: 52.2% ... Training loss: 0.065 ... Validation loss: 0.1720.06373802941773529\n",
            "0.1602536518966166\n",
            "Progress: 52.2% ... Training loss: 0.063 ... Validation loss: 0.1600.06627755012455835\n",
            "0.16876394453018287\n",
            "Progress: 52.2% ... Training loss: 0.066 ... Validation loss: 0.1680.0644829680013137\n",
            "0.16069959442458476\n",
            "Progress: 52.2% ... Training loss: 0.064 ... Validation loss: 0.1600.06978572006426922\n",
            "0.18529961465103198\n",
            "Progress: 52.2% ... Training loss: 0.069 ... Validation loss: 0.1850.07117582586016775\n",
            "0.14098224183123817\n",
            "Progress: 52.3% ... Training loss: 0.071 ... Validation loss: 0.1400.06716084299836042\n",
            "0.14704314819167555\n",
            "Progress: 52.3% ... Training loss: 0.067 ... Validation loss: 0.1470.06296646686566847\n",
            "0.15889087373729843\n",
            "Progress: 52.3% ... Training loss: 0.062 ... Validation loss: 0.1580.07076161796592885\n",
            "0.18826631026546342\n",
            "Progress: 52.3% ... Training loss: 0.070 ... Validation loss: 0.1880.06688977028671737\n",
            "0.18982690495698762\n",
            "Progress: 52.3% ... Training loss: 0.066 ... Validation loss: 0.1890.06354118001398933\n",
            "0.16427235801830714\n",
            "Progress: 52.4% ... Training loss: 0.063 ... Validation loss: 0.1640.07104190334240042\n",
            "0.19356661232199202\n",
            "Progress: 52.4% ... Training loss: 0.071 ... Validation loss: 0.1930.064002211892264\n",
            "0.1655843304082819\n",
            "Progress: 52.4% ... Training loss: 0.064 ... Validation loss: 0.1650.06483296239527112\n",
            "0.16333308901201352\n",
            "Progress: 52.4% ... Training loss: 0.064 ... Validation loss: 0.1630.07102937945286357\n",
            "0.18757248796752354\n",
            "Progress: 52.4% ... Training loss: 0.071 ... Validation loss: 0.1870.06518102682125397\n",
            "0.16693326636762032\n",
            "Progress: 52.5% ... Training loss: 0.065 ... Validation loss: 0.1660.0698055622939843\n",
            "0.16308999285318304\n",
            "Progress: 52.5% ... Training loss: 0.069 ... Validation loss: 0.1630.06606135805889735\n",
            "0.18096888863667193\n",
            "Progress: 52.5% ... Training loss: 0.066 ... Validation loss: 0.1800.06625559630597248\n",
            "0.15549253762680004\n",
            "Progress: 52.5% ... Training loss: 0.066 ... Validation loss: 0.1550.0644511195550575\n",
            "0.17279836539594426\n",
            "Progress: 52.5% ... Training loss: 0.064 ... Validation loss: 0.1720.066676048744374\n",
            "0.1585951118118833\n",
            "Progress: 52.6% ... Training loss: 0.066 ... Validation loss: 0.1580.06569944944644034\n",
            "0.1771744306130555\n",
            "Progress: 52.6% ... Training loss: 0.065 ... Validation loss: 0.1770.06547075067076173\n",
            "0.15911060511914582\n",
            "Progress: 52.6% ... Training loss: 0.065 ... Validation loss: 0.1590.06903810959977486\n",
            "0.18401607594432437\n",
            "Progress: 52.6% ... Training loss: 0.069 ... Validation loss: 0.1840.06491926913436485\n",
            "0.17491075229646627\n",
            "Progress: 52.6% ... Training loss: 0.064 ... Validation loss: 0.1740.06545332677720207\n",
            "0.16400250355540005\n",
            "Progress: 52.7% ... Training loss: 0.065 ... Validation loss: 0.1640.06834178784851076\n",
            "0.16475886156617942\n",
            "Progress: 52.7% ... Training loss: 0.068 ... Validation loss: 0.1640.06855958241334029\n",
            "0.17434658978754358\n",
            "Progress: 52.7% ... Training loss: 0.068 ... Validation loss: 0.1740.0760949247961381\n",
            "0.2036220527152752\n",
            "Progress: 52.7% ... Training loss: 0.076 ... Validation loss: 0.2030.06393338712306486\n",
            "0.1783401702585411\n",
            "Progress: 52.7% ... Training loss: 0.063 ... Validation loss: 0.1780.06604250663412413\n",
            "0.16767531713859204\n",
            "Progress: 52.8% ... Training loss: 0.066 ... Validation loss: 0.1670.06778403248910644\n",
            "0.1997128805577275\n",
            "Progress: 52.8% ... Training loss: 0.067 ... Validation loss: 0.1990.0772636668540779\n",
            "0.15425049452756978\n",
            "Progress: 52.8% ... Training loss: 0.077 ... Validation loss: 0.1540.06737068422616019\n",
            "0.15856934670445885\n",
            "Progress: 52.8% ... Training loss: 0.067 ... Validation loss: 0.1580.06575800271274675\n",
            "0.15273906636674092\n",
            "Progress: 52.8% ... Training loss: 0.065 ... Validation loss: 0.1520.07168069857746771\n",
            "0.1910335580500447\n",
            "Progress: 52.9% ... Training loss: 0.071 ... Validation loss: 0.1910.06883299259894228\n",
            "0.15638355997605857\n",
            "Progress: 52.9% ... Training loss: 0.068 ... Validation loss: 0.1560.06474875432358572\n",
            "0.16447714741784566\n",
            "Progress: 52.9% ... Training loss: 0.064 ... Validation loss: 0.1640.0639983403054324\n",
            "0.16272899172102576\n",
            "Progress: 52.9% ... Training loss: 0.063 ... Validation loss: 0.1620.06443183340327688\n",
            "0.16012794120790685\n",
            "Progress: 52.9% ... Training loss: 0.064 ... Validation loss: 0.1600.08391016639174212\n",
            "0.2023954652893216\n",
            "Progress: 53.0% ... Training loss: 0.083 ... Validation loss: 0.2020.06734467543217801\n",
            "0.17737556911517774\n",
            "Progress: 53.0% ... Training loss: 0.067 ... Validation loss: 0.1770.06613900815421087\n",
            "0.1783061826846367\n",
            "Progress: 53.0% ... Training loss: 0.066 ... Validation loss: 0.1780.06671728720520934\n",
            "0.17342149653063593\n",
            "Progress: 53.0% ... Training loss: 0.066 ... Validation loss: 0.1730.06495264769461327\n",
            "0.16295627609409766\n",
            "Progress: 53.0% ... Training loss: 0.064 ... Validation loss: 0.1620.06488703684648298\n",
            "0.1672070049900902\n",
            "Progress: 53.1% ... Training loss: 0.064 ... Validation loss: 0.1670.06464107979211572\n",
            "0.16659778222575092\n",
            "Progress: 53.1% ... Training loss: 0.064 ... Validation loss: 0.1660.07065694008176485\n",
            "0.15948171325992522\n",
            "Progress: 53.1% ... Training loss: 0.070 ... Validation loss: 0.1590.06762920726261511\n",
            "0.1645123821487884\n",
            "Progress: 53.1% ... Training loss: 0.067 ... Validation loss: 0.1640.06558768393807884\n",
            "0.17714074993406384\n",
            "Progress: 53.1% ... Training loss: 0.065 ... Validation loss: 0.1770.06663993816974226\n",
            "0.16437088934357785\n",
            "Progress: 53.2% ... Training loss: 0.066 ... Validation loss: 0.1640.06451888938933475\n",
            "0.1740899305853015\n",
            "Progress: 53.2% ... Training loss: 0.064 ... Validation loss: 0.1740.06468604438715855\n",
            "0.17149927758912048\n",
            "Progress: 53.2% ... Training loss: 0.064 ... Validation loss: 0.1710.06305071168814885\n",
            "0.16971704763388926\n",
            "Progress: 53.2% ... Training loss: 0.063 ... Validation loss: 0.1690.0638529055570305\n",
            "0.1713534621268903\n",
            "Progress: 53.2% ... Training loss: 0.063 ... Validation loss: 0.1710.0647676852321856\n",
            "0.17221415057198608\n",
            "Progress: 53.3% ... Training loss: 0.064 ... Validation loss: 0.1720.06617244919593278\n",
            "0.16361422891664262\n",
            "Progress: 53.3% ... Training loss: 0.066 ... Validation loss: 0.1630.065039861164933\n",
            "0.1573150569820252\n",
            "Progress: 53.3% ... Training loss: 0.065 ... Validation loss: 0.1570.0714493493875153\n",
            "0.1535540714751561\n",
            "Progress: 53.3% ... Training loss: 0.071 ... Validation loss: 0.1530.06459134749478951\n",
            "0.18162939398281266\n",
            "Progress: 53.3% ... Training loss: 0.064 ... Validation loss: 0.1810.06487503568308005\n",
            "0.1585730143367766\n",
            "Progress: 53.4% ... Training loss: 0.064 ... Validation loss: 0.1580.0634677427145236\n",
            "0.1778625566408176\n",
            "Progress: 53.4% ... Training loss: 0.063 ... Validation loss: 0.1770.07449356380726056\n",
            "0.16386587635654473\n",
            "Progress: 53.4% ... Training loss: 0.074 ... Validation loss: 0.1630.06572024745013952\n",
            "0.17599002925382543\n",
            "Progress: 53.4% ... Training loss: 0.065 ... Validation loss: 0.1750.06776234461791851\n",
            "0.14916369380920275\n",
            "Progress: 53.4% ... Training loss: 0.067 ... Validation loss: 0.1490.06368211993894235\n",
            "0.17215635438342494\n",
            "Progress: 53.5% ... Training loss: 0.063 ... Validation loss: 0.1720.0644579019548635\n",
            "0.1641109963127463\n",
            "Progress: 53.5% ... Training loss: 0.064 ... Validation loss: 0.1640.06825286097941508\n",
            "0.15860826319960922\n",
            "Progress: 53.5% ... Training loss: 0.068 ... Validation loss: 0.1580.06618890217448183\n",
            "0.1870060916985214\n",
            "Progress: 53.5% ... Training loss: 0.066 ... Validation loss: 0.1870.06419279601216532\n",
            "0.1659610236826915\n",
            "Progress: 53.5% ... Training loss: 0.064 ... Validation loss: 0.1650.06766751763230458\n",
            "0.18252388274070705\n",
            "Progress: 53.6% ... Training loss: 0.067 ... Validation loss: 0.1820.06514530868482303\n",
            "0.18596454469629492\n",
            "Progress: 53.6% ... Training loss: 0.065 ... Validation loss: 0.1850.06919881873031614\n",
            "0.19407841065125997\n",
            "Progress: 53.6% ... Training loss: 0.069 ... Validation loss: 0.1940.06385920345226827\n",
            "0.18149960283748143\n",
            "Progress: 53.6% ... Training loss: 0.063 ... Validation loss: 0.1810.06658169803008473\n",
            "0.20098713625733858\n",
            "Progress: 53.6% ... Training loss: 0.066 ... Validation loss: 0.2000.06372709616908674\n",
            "0.17040833669951425\n",
            "Progress: 53.7% ... Training loss: 0.063 ... Validation loss: 0.1700.06603451418056933\n",
            "0.18148121526199293\n",
            "Progress: 53.7% ... Training loss: 0.066 ... Validation loss: 0.1810.08379574649601211\n",
            "0.22989618425224245\n",
            "Progress: 53.7% ... Training loss: 0.083 ... Validation loss: 0.2290.06697028916957092\n",
            "0.17675984144181064\n",
            "Progress: 53.7% ... Training loss: 0.066 ... Validation loss: 0.1760.06715905830566941\n",
            "0.18902119862773692\n",
            "Progress: 53.7% ... Training loss: 0.067 ... Validation loss: 0.1890.0663850987842485\n",
            "0.19374281006264635\n",
            "Progress: 53.8% ... Training loss: 0.066 ... Validation loss: 0.1930.066103924643964\n",
            "0.1938052184958292\n",
            "Progress: 53.8% ... Training loss: 0.066 ... Validation loss: 0.1930.06574233208240383\n",
            "0.18519948946162426\n",
            "Progress: 53.8% ... Training loss: 0.065 ... Validation loss: 0.1850.064212358646045\n",
            "0.1670320246713494\n",
            "Progress: 53.8% ... Training loss: 0.064 ... Validation loss: 0.1670.07166058247697576\n",
            "0.1968814004965865\n",
            "Progress: 53.8% ... Training loss: 0.071 ... Validation loss: 0.1960.06364204358477675\n",
            "0.18452448315581224\n",
            "Progress: 53.9% ... Training loss: 0.063 ... Validation loss: 0.1840.06904985922663477\n",
            "0.16678941224233362\n",
            "Progress: 53.9% ... Training loss: 0.069 ... Validation loss: 0.1660.06508005219329092\n",
            "0.1593403553356971\n",
            "Progress: 53.9% ... Training loss: 0.065 ... Validation loss: 0.1590.06591310992136723\n",
            "0.1810736118967507\n",
            "Progress: 53.9% ... Training loss: 0.065 ... Validation loss: 0.1810.06461563966984263\n",
            "0.1719757061245874\n",
            "Progress: 53.9% ... Training loss: 0.064 ... Validation loss: 0.1710.06518035918708508\n",
            "0.18007041501300963\n",
            "Progress: 54.0% ... Training loss: 0.065 ... Validation loss: 0.1800.06847798078696185\n",
            "0.17196891263189318\n",
            "Progress: 54.0% ... Training loss: 0.068 ... Validation loss: 0.1710.0696907318072981\n",
            "0.20272133724027203\n",
            "Progress: 54.0% ... Training loss: 0.069 ... Validation loss: 0.2020.06694892866340989\n",
            "0.15708697765405524\n",
            "Progress: 54.0% ... Training loss: 0.066 ... Validation loss: 0.1570.06473139644451358\n",
            "0.15620867879291842\n",
            "Progress: 54.0% ... Training loss: 0.064 ... Validation loss: 0.1560.07180491393524054\n",
            "0.16527741317846148\n",
            "Progress: 54.1% ... Training loss: 0.071 ... Validation loss: 0.1650.06443656866448538\n",
            "0.15617449833195415\n",
            "Progress: 54.1% ... Training loss: 0.064 ... Validation loss: 0.1560.06941573513757095\n",
            "0.17029511731784794\n",
            "Progress: 54.1% ... Training loss: 0.069 ... Validation loss: 0.1700.06709636893378795\n",
            "0.1507540961870929\n",
            "Progress: 54.1% ... Training loss: 0.067 ... Validation loss: 0.1500.07444616157128496\n",
            "0.14707796254572597\n",
            "Progress: 54.1% ... Training loss: 0.074 ... Validation loss: 0.1470.06573510933933403\n",
            "0.16223374348781003\n",
            "Progress: 54.2% ... Training loss: 0.065 ... Validation loss: 0.1620.06365147758791381\n",
            "0.14768400080842545\n",
            "Progress: 54.2% ... Training loss: 0.063 ... Validation loss: 0.1470.06709979722665177\n",
            "0.148207179451157\n",
            "Progress: 54.2% ... Training loss: 0.067 ... Validation loss: 0.1480.07662378253420711\n",
            "0.15325101598187488\n",
            "Progress: 54.2% ... Training loss: 0.076 ... Validation loss: 0.1530.06503790468058997\n",
            "0.18520323324473492\n",
            "Progress: 54.2% ... Training loss: 0.065 ... Validation loss: 0.1850.07654866525936835\n",
            "0.1596201279907745\n",
            "Progress: 54.3% ... Training loss: 0.076 ... Validation loss: 0.1590.06616739024425813\n",
            "0.171585207272481\n",
            "Progress: 54.3% ... Training loss: 0.066 ... Validation loss: 0.1710.06339734249893507\n",
            "0.1683155079105016\n",
            "Progress: 54.3% ... Training loss: 0.063 ... Validation loss: 0.1680.06388637526711663\n",
            "0.15794058424353719\n",
            "Progress: 54.3% ... Training loss: 0.063 ... Validation loss: 0.1570.06437810952356612\n",
            "0.16445411742899813\n",
            "Progress: 54.3% ... Training loss: 0.064 ... Validation loss: 0.1640.06450839292402347\n",
            "0.1646233484256069\n",
            "Progress: 54.4% ... Training loss: 0.064 ... Validation loss: 0.1640.07062945211539068\n",
            "0.15505187731547798\n",
            "Progress: 54.4% ... Training loss: 0.070 ... Validation loss: 0.1550.06599260230930297\n",
            "0.17649812456823738\n",
            "Progress: 54.4% ... Training loss: 0.065 ... Validation loss: 0.1760.06467413354428769\n",
            "0.1573048376753352\n",
            "Progress: 54.4% ... Training loss: 0.064 ... Validation loss: 0.1570.06811993726843715\n",
            "0.14389913079205388\n",
            "Progress: 54.4% ... Training loss: 0.068 ... Validation loss: 0.1430.06506810742715545\n",
            "0.1596674845256558\n",
            "Progress: 54.5% ... Training loss: 0.065 ... Validation loss: 0.1590.06673125093379675\n",
            "0.15532763127467666\n",
            "Progress: 54.5% ... Training loss: 0.066 ... Validation loss: 0.1550.06481499159338296\n",
            "0.16979551104560098\n",
            "Progress: 54.5% ... Training loss: 0.064 ... Validation loss: 0.1690.06576865149812439\n",
            "0.1716365845625038\n",
            "Progress: 54.5% ... Training loss: 0.065 ... Validation loss: 0.1710.06646984348224626\n",
            "0.15209686216836357\n",
            "Progress: 54.5% ... Training loss: 0.066 ... Validation loss: 0.1520.07124430529723744\n",
            "0.1984242272238552\n",
            "Progress: 54.6% ... Training loss: 0.071 ... Validation loss: 0.1980.06690042872399708\n",
            "0.15097597990240874\n",
            "Progress: 54.6% ... Training loss: 0.066 ... Validation loss: 0.1500.06421115454809685\n",
            "0.17145778928967614\n",
            "Progress: 54.6% ... Training loss: 0.064 ... Validation loss: 0.1710.06819309582607667\n",
            "0.1640613448357802\n",
            "Progress: 54.6% ... Training loss: 0.068 ... Validation loss: 0.1640.06400637792005699\n",
            "0.160212829132787\n",
            "Progress: 54.6% ... Training loss: 0.064 ... Validation loss: 0.1600.06542864429421912\n",
            "0.15077514110284593\n",
            "Progress: 54.7% ... Training loss: 0.065 ... Validation loss: 0.1500.0652810369518277\n",
            "0.16310536786634278\n",
            "Progress: 54.7% ... Training loss: 0.065 ... Validation loss: 0.1630.06510300877567385\n",
            "0.16397062489493475\n",
            "Progress: 54.7% ... Training loss: 0.065 ... Validation loss: 0.1630.0639537069688689\n",
            "0.17136741005742911\n",
            "Progress: 54.7% ... Training loss: 0.063 ... Validation loss: 0.1710.06532236958981426\n",
            "0.17118059098651386\n",
            "Progress: 54.7% ... Training loss: 0.065 ... Validation loss: 0.1710.07472574914414065\n",
            "0.16274832573536888\n",
            "Progress: 54.8% ... Training loss: 0.074 ... Validation loss: 0.1620.06650487101360746\n",
            "0.1563985477103198\n",
            "Progress: 54.8% ... Training loss: 0.066 ... Validation loss: 0.1560.06370895533217472\n",
            "0.16852830587608936\n",
            "Progress: 54.8% ... Training loss: 0.063 ... Validation loss: 0.1680.06368896017322821\n",
            "0.16414418022221114\n",
            "Progress: 54.8% ... Training loss: 0.063 ... Validation loss: 0.1640.06622630202139276\n",
            "0.17866458933312965\n",
            "Progress: 54.8% ... Training loss: 0.066 ... Validation loss: 0.1780.06606726294167212\n",
            "0.15304452489726392\n",
            "Progress: 54.9% ... Training loss: 0.066 ... Validation loss: 0.1530.06285729869373749\n",
            "0.16579627140464745\n",
            "Progress: 54.9% ... Training loss: 0.062 ... Validation loss: 0.1650.06425511385075153\n",
            "0.17046317694213123\n",
            "Progress: 54.9% ... Training loss: 0.064 ... Validation loss: 0.1700.06622040202453008\n",
            "0.1655742822549307\n",
            "Progress: 54.9% ... Training loss: 0.066 ... Validation loss: 0.1650.06688135637742477\n",
            "0.1814412219364538\n",
            "Progress: 54.9% ... Training loss: 0.066 ... Validation loss: 0.1810.06993430193183954\n",
            "0.19451917271341476\n",
            "Progress: 55.0% ... Training loss: 0.069 ... Validation loss: 0.1940.06299069183243323\n",
            "0.15680665481778788\n",
            "Progress: 55.0% ... Training loss: 0.062 ... Validation loss: 0.1560.06404086340217828\n",
            "0.17630062931951376\n",
            "Progress: 55.0% ... Training loss: 0.064 ... Validation loss: 0.1760.06362433883501546\n",
            "0.17988572776831876\n",
            "Progress: 55.0% ... Training loss: 0.063 ... Validation loss: 0.1790.06377030084450126\n",
            "0.18675953493728956\n",
            "Progress: 55.0% ... Training loss: 0.063 ... Validation loss: 0.1860.062276042126306747\n",
            "0.1720722104993757\n",
            "Progress: 55.1% ... Training loss: 0.062 ... Validation loss: 0.1720.0660676111372758\n",
            "0.1533715468038481\n",
            "Progress: 55.1% ... Training loss: 0.066 ... Validation loss: 0.1530.06304545887253145\n",
            "0.16437860093183895\n",
            "Progress: 55.1% ... Training loss: 0.063 ... Validation loss: 0.1640.07059196382944646\n",
            "0.16008879837848866\n",
            "Progress: 55.1% ... Training loss: 0.070 ... Validation loss: 0.1600.06612322835721214\n",
            "0.1580081500244904\n",
            "Progress: 55.1% ... Training loss: 0.066 ... Validation loss: 0.1580.06672748268851468\n",
            "0.17464666694183323\n",
            "Progress: 55.2% ... Training loss: 0.066 ... Validation loss: 0.1740.06638071771906764\n",
            "0.1747994331497998\n",
            "Progress: 55.2% ... Training loss: 0.066 ... Validation loss: 0.1740.06289187510357923\n",
            "0.16284953661513232\n",
            "Progress: 55.2% ... Training loss: 0.062 ... Validation loss: 0.1620.06866938830398653\n",
            "0.16616323368226002\n",
            "Progress: 55.2% ... Training loss: 0.068 ... Validation loss: 0.1660.06609846407756746\n",
            "0.16694032190550467\n",
            "Progress: 55.2% ... Training loss: 0.066 ... Validation loss: 0.1660.06956064012403508\n",
            "0.16151513871289758\n",
            "Progress: 55.3% ... Training loss: 0.069 ... Validation loss: 0.1610.06583667509205429\n",
            "0.1780531112297639\n",
            "Progress: 55.3% ... Training loss: 0.065 ... Validation loss: 0.1780.0727431748138788\n",
            "0.19916861205082886\n",
            "Progress: 55.3% ... Training loss: 0.072 ... Validation loss: 0.1990.06895978445399338\n",
            "0.2073833927363442\n",
            "Progress: 55.3% ... Training loss: 0.068 ... Validation loss: 0.2070.06891781610167187\n",
            "0.1849473595056772\n",
            "Progress: 55.3% ... Training loss: 0.068 ... Validation loss: 0.1840.062155587062002596\n",
            "0.1658235296091833\n",
            "Progress: 55.4% ... Training loss: 0.062 ... Validation loss: 0.1650.06296279332766358\n",
            "0.16333480947500112\n",
            "Progress: 55.4% ... Training loss: 0.062 ... Validation loss: 0.1630.06272923454951979\n",
            "0.15295832940675955\n",
            "Progress: 55.4% ... Training loss: 0.062 ... Validation loss: 0.1520.06719065454480472\n",
            "0.15003857113270608\n",
            "Progress: 55.4% ... Training loss: 0.067 ... Validation loss: 0.1500.06684360988403033\n",
            "0.15534783957096485\n",
            "Progress: 55.4% ... Training loss: 0.066 ... Validation loss: 0.1550.06464293176978972\n",
            "0.17888391991573876\n",
            "Progress: 55.5% ... Training loss: 0.064 ... Validation loss: 0.1780.06756970657810095\n",
            "0.18885267536765926\n",
            "Progress: 55.5% ... Training loss: 0.067 ... Validation loss: 0.1880.06425469329101859\n",
            "0.1749044605827147\n",
            "Progress: 55.5% ... Training loss: 0.064 ... Validation loss: 0.1740.062266826929369155\n",
            "0.16450573283679265\n",
            "Progress: 55.5% ... Training loss: 0.062 ... Validation loss: 0.1640.0663339035923266\n",
            "0.1979108463642993\n",
            "Progress: 55.5% ... Training loss: 0.066 ... Validation loss: 0.1970.06498567519622131\n",
            "0.18667446649553057\n",
            "Progress: 55.6% ... Training loss: 0.064 ... Validation loss: 0.1860.06369867282959413\n",
            "0.17262597946882974\n",
            "Progress: 55.6% ... Training loss: 0.063 ... Validation loss: 0.1720.06439949173944524\n",
            "0.18386638757750356\n",
            "Progress: 55.6% ... Training loss: 0.064 ... Validation loss: 0.1830.08023995345090161\n",
            "0.16098149776724943\n",
            "Progress: 55.6% ... Training loss: 0.080 ... Validation loss: 0.1600.06610784596111267\n",
            "0.20751895840557488\n",
            "Progress: 55.6% ... Training loss: 0.066 ... Validation loss: 0.2070.06687861491297714\n",
            "0.202366664654706\n",
            "Progress: 55.7% ... Training loss: 0.066 ... Validation loss: 0.2020.06563628265935822\n",
            "0.15693839638686952\n",
            "Progress: 55.7% ... Training loss: 0.065 ... Validation loss: 0.1560.07288421508111641\n",
            "0.14871699124903487\n",
            "Progress: 55.7% ... Training loss: 0.072 ... Validation loss: 0.1480.06402232251077425\n",
            "0.17756197621614797\n",
            "Progress: 55.7% ... Training loss: 0.064 ... Validation loss: 0.1770.07532731400097338\n",
            "0.15243282477986603\n",
            "Progress: 55.7% ... Training loss: 0.075 ... Validation loss: 0.1520.06317190084899253\n",
            "0.17401221963562313\n",
            "Progress: 55.8% ... Training loss: 0.063 ... Validation loss: 0.1740.06459913136636164\n",
            "0.1742648023043096\n",
            "Progress: 55.8% ... Training loss: 0.064 ... Validation loss: 0.1740.06388846849141662\n",
            "0.18343409201602495\n",
            "Progress: 55.8% ... Training loss: 0.063 ... Validation loss: 0.1830.0654816226192014\n",
            "0.16779248000429006\n",
            "Progress: 55.8% ... Training loss: 0.065 ... Validation loss: 0.1670.06584565098671104\n",
            "0.17604257987165936\n",
            "Progress: 55.8% ... Training loss: 0.065 ... Validation loss: 0.1760.0636398613550756\n",
            "0.17987194481176816\n",
            "Progress: 55.9% ... Training loss: 0.063 ... Validation loss: 0.1790.06368248778691281\n",
            "0.16573938023266038\n",
            "Progress: 55.9% ... Training loss: 0.063 ... Validation loss: 0.1650.06653048302090551\n",
            "0.16227155359480175\n",
            "Progress: 55.9% ... Training loss: 0.066 ... Validation loss: 0.1620.06440942538667001\n",
            "0.18405990829408583\n",
            "Progress: 55.9% ... Training loss: 0.064 ... Validation loss: 0.1840.0677204244898111\n",
            "0.1889833254154019\n",
            "Progress: 55.9% ... Training loss: 0.067 ... Validation loss: 0.1880.06317013377260539\n",
            "0.1637823495446077\n",
            "Progress: 56.0% ... Training loss: 0.063 ... Validation loss: 0.1630.06455972105933817\n",
            "0.15936868804320248\n",
            "Progress: 56.0% ... Training loss: 0.064 ... Validation loss: 0.1590.06259033820945495\n",
            "0.1638363042144827\n",
            "Progress: 56.0% ... Training loss: 0.062 ... Validation loss: 0.1630.08034067931001511\n",
            "0.15553217729146937\n",
            "Progress: 56.0% ... Training loss: 0.080 ... Validation loss: 0.1550.06513494652355659\n",
            "0.1542277892069279\n",
            "Progress: 56.0% ... Training loss: 0.065 ... Validation loss: 0.1540.06535325318649633\n",
            "0.15092721080882576\n",
            "Progress: 56.1% ... Training loss: 0.065 ... Validation loss: 0.1500.06334140428311107\n",
            "0.15438121409268457\n",
            "Progress: 56.1% ... Training loss: 0.063 ... Validation loss: 0.1540.06480121032955362\n",
            "0.14586184144604533\n",
            "Progress: 56.1% ... Training loss: 0.064 ... Validation loss: 0.1450.06474852550325635\n",
            "0.16851420800795522\n",
            "Progress: 56.1% ... Training loss: 0.064 ... Validation loss: 0.1680.06327137147273607\n",
            "0.1508542027721682\n",
            "Progress: 56.1% ... Training loss: 0.063 ... Validation loss: 0.1500.07260137782305802\n",
            "0.15764254538811306\n",
            "Progress: 56.2% ... Training loss: 0.072 ... Validation loss: 0.1570.06610540420239333\n",
            "0.1673750104421616\n",
            "Progress: 56.2% ... Training loss: 0.066 ... Validation loss: 0.1670.06437496460504152\n",
            "0.14899193295719937\n",
            "Progress: 56.2% ... Training loss: 0.064 ... Validation loss: 0.1480.06495720316644053\n",
            "0.16149637739882175\n",
            "Progress: 56.2% ... Training loss: 0.064 ... Validation loss: 0.1610.06481051358215897\n",
            "0.1459619891842832\n",
            "Progress: 56.2% ... Training loss: 0.064 ... Validation loss: 0.1450.06304174393900275\n",
            "0.1575465458106118\n",
            "Progress: 56.3% ... Training loss: 0.063 ... Validation loss: 0.1570.06755421237281206\n",
            "0.1607195087627087\n",
            "Progress: 56.3% ... Training loss: 0.067 ... Validation loss: 0.1600.06621416348573934\n",
            "0.16521891266352112\n",
            "Progress: 56.3% ... Training loss: 0.066 ... Validation loss: 0.1650.06620854062696263\n",
            "0.1559588150850241\n",
            "Progress: 56.3% ... Training loss: 0.066 ... Validation loss: 0.1550.07479474878801899\n",
            "0.1823830316987507\n",
            "Progress: 56.3% ... Training loss: 0.074 ... Validation loss: 0.1820.07229359221541648\n",
            "0.17770200755350454\n",
            "Progress: 56.4% ... Training loss: 0.072 ... Validation loss: 0.1770.0666334916322983\n",
            "0.14901358826648914\n",
            "Progress: 56.4% ... Training loss: 0.066 ... Validation loss: 0.1490.07648441996191842\n",
            "0.1579511178259687\n",
            "Progress: 56.4% ... Training loss: 0.076 ... Validation loss: 0.1570.06499333437622884\n",
            "0.1502623840426402\n",
            "Progress: 56.4% ... Training loss: 0.064 ... Validation loss: 0.1500.06310755247081425\n",
            "0.151835436854975\n",
            "Progress: 56.4% ... Training loss: 0.063 ... Validation loss: 0.1510.0679888724710614\n",
            "0.17688422259720377\n",
            "Progress: 56.5% ... Training loss: 0.067 ... Validation loss: 0.1760.06336559858839018\n",
            "0.1562464535411158\n",
            "Progress: 56.5% ... Training loss: 0.063 ... Validation loss: 0.1560.06708838535936223\n",
            "0.15167347500739342\n",
            "Progress: 56.5% ... Training loss: 0.067 ... Validation loss: 0.1510.0667100079626887\n",
            "0.1720127571070987\n",
            "Progress: 56.5% ... Training loss: 0.066 ... Validation loss: 0.1720.06602396841296437\n",
            "0.16359089562665277\n",
            "Progress: 56.5% ... Training loss: 0.066 ... Validation loss: 0.1630.06312679277826465\n",
            "0.15097044746963184\n",
            "Progress: 56.6% ... Training loss: 0.063 ... Validation loss: 0.1500.06326734048492381\n",
            "0.15540416566022114\n",
            "Progress: 56.6% ... Training loss: 0.063 ... Validation loss: 0.1550.0688935464754363\n",
            "0.18833869206832748\n",
            "Progress: 56.6% ... Training loss: 0.068 ... Validation loss: 0.1880.06504738540938029\n",
            "0.1437281568885181\n",
            "Progress: 56.6% ... Training loss: 0.065 ... Validation loss: 0.1430.06378452440223747\n",
            "0.1485869791325707\n",
            "Progress: 56.6% ... Training loss: 0.063 ... Validation loss: 0.1480.06633430989579048\n",
            "0.16025492956888898\n",
            "Progress: 56.7% ... Training loss: 0.066 ... Validation loss: 0.1600.06491002819154815\n",
            "0.14739500532624827\n",
            "Progress: 56.7% ... Training loss: 0.064 ... Validation loss: 0.1470.06371858794570429\n",
            "0.15652444741283714\n",
            "Progress: 56.7% ... Training loss: 0.063 ... Validation loss: 0.1560.06888222080055847\n",
            "0.16999440599800839\n",
            "Progress: 56.7% ... Training loss: 0.068 ... Validation loss: 0.1690.06322207801472086\n",
            "0.15434716231317627\n",
            "Progress: 56.7% ... Training loss: 0.063 ... Validation loss: 0.1540.0640876122633059\n",
            "0.15056117369405383\n",
            "Progress: 56.8% ... Training loss: 0.064 ... Validation loss: 0.1500.06714525668245963\n",
            "0.14108610092229454\n",
            "Progress: 56.8% ... Training loss: 0.067 ... Validation loss: 0.1410.10751433050813393\n",
            "0.16119395009074178\n",
            "Progress: 56.8% ... Training loss: 0.107 ... Validation loss: 0.1610.0657651385577826\n",
            "0.15502112064233203\n",
            "Progress: 56.8% ... Training loss: 0.065 ... Validation loss: 0.1550.06332914203600172\n",
            "0.16025662615557904\n",
            "Progress: 56.8% ... Training loss: 0.063 ... Validation loss: 0.1600.06378224726341873\n",
            "0.15778360741689118\n",
            "Progress: 56.9% ... Training loss: 0.063 ... Validation loss: 0.1570.06347775952285886\n",
            "0.15983766748288994\n",
            "Progress: 56.9% ... Training loss: 0.063 ... Validation loss: 0.1590.06592485833069162\n",
            "0.1701585808089357\n",
            "Progress: 56.9% ... Training loss: 0.065 ... Validation loss: 0.1700.06914534803785\n",
            "0.19033406495466684\n",
            "Progress: 56.9% ... Training loss: 0.069 ... Validation loss: 0.1900.0750545032077314\n",
            "0.21192113251644523\n",
            "Progress: 56.9% ... Training loss: 0.075 ... Validation loss: 0.2110.06378328987266728\n",
            "0.1629775881761206\n",
            "Progress: 57.0% ... Training loss: 0.063 ... Validation loss: 0.1620.06635648695060363\n",
            "0.14902639727448427\n",
            "Progress: 57.0% ... Training loss: 0.066 ... Validation loss: 0.1490.06379526591979923\n",
            "0.15674602854083194\n",
            "Progress: 57.0% ... Training loss: 0.063 ... Validation loss: 0.1560.06481093267269765\n",
            "0.16707694977071375\n",
            "Progress: 57.0% ... Training loss: 0.064 ... Validation loss: 0.1670.06658934938733811\n",
            "0.14500480060197427\n",
            "Progress: 57.0% ... Training loss: 0.066 ... Validation loss: 0.1450.06727378135829454\n",
            "0.14954272500931903\n",
            "Progress: 57.1% ... Training loss: 0.067 ... Validation loss: 0.1490.06300478907958211\n",
            "0.15357557274205952\n",
            "Progress: 57.1% ... Training loss: 0.063 ... Validation loss: 0.1530.06542201059567664\n",
            "0.16717834712821744\n",
            "Progress: 57.1% ... Training loss: 0.065 ... Validation loss: 0.1670.06469452123074183\n",
            "0.15301736099387733\n",
            "Progress: 57.1% ... Training loss: 0.064 ... Validation loss: 0.1530.06495548894086697\n",
            "0.14834562070862864\n",
            "Progress: 57.1% ... Training loss: 0.064 ... Validation loss: 0.1480.0673363174250281\n",
            "0.16362878051930554\n",
            "Progress: 57.2% ... Training loss: 0.067 ... Validation loss: 0.1630.06681331941228309\n",
            "0.176801580616742\n",
            "Progress: 57.2% ... Training loss: 0.066 ... Validation loss: 0.1760.06623472644651031\n",
            "0.16498044308062929\n",
            "Progress: 57.2% ... Training loss: 0.066 ... Validation loss: 0.1640.0753389402361218\n",
            "0.15143686897693787\n",
            "Progress: 57.2% ... Training loss: 0.075 ... Validation loss: 0.1510.0656942179752091\n",
            "0.1659789821149377\n",
            "Progress: 57.2% ... Training loss: 0.065 ... Validation loss: 0.1650.061891426403013816\n",
            "0.1549698274044048\n",
            "Progress: 57.3% ... Training loss: 0.061 ... Validation loss: 0.1540.06524267007987909\n",
            "0.1507687256912336\n",
            "Progress: 57.3% ... Training loss: 0.065 ... Validation loss: 0.1500.06990288084890688\n",
            "0.1527628004948785\n",
            "Progress: 57.3% ... Training loss: 0.069 ... Validation loss: 0.1520.06346650156165859\n",
            "0.16961496039990925\n",
            "Progress: 57.3% ... Training loss: 0.063 ... Validation loss: 0.1690.06462756083365176\n",
            "0.15795251503917462\n",
            "Progress: 57.3% ... Training loss: 0.064 ... Validation loss: 0.1570.06373105480061235\n",
            "0.15789503921663056\n",
            "Progress: 57.4% ... Training loss: 0.063 ... Validation loss: 0.1570.07009674513560822\n",
            "0.1808629011576069\n",
            "Progress: 57.4% ... Training loss: 0.070 ... Validation loss: 0.1800.06382744021136857\n",
            "0.16240285258447842\n",
            "Progress: 57.4% ... Training loss: 0.063 ... Validation loss: 0.1620.06406303104441449\n",
            "0.15018389254316883\n",
            "Progress: 57.4% ... Training loss: 0.064 ... Validation loss: 0.1500.06724354913915014\n",
            "0.18776822872546045\n",
            "Progress: 57.4% ... Training loss: 0.067 ... Validation loss: 0.1870.063540102968794\n",
            "0.16710940453743575\n",
            "Progress: 57.5% ... Training loss: 0.063 ... Validation loss: 0.1670.06723756938878463\n",
            "0.14734961338492017\n",
            "Progress: 57.5% ... Training loss: 0.067 ... Validation loss: 0.1470.06811733070112456\n",
            "0.16728094344244326\n",
            "Progress: 57.5% ... Training loss: 0.068 ... Validation loss: 0.1670.06620636213581121\n",
            "0.14234077027430786\n",
            "Progress: 57.5% ... Training loss: 0.066 ... Validation loss: 0.1420.06292345755208079\n",
            "0.15890332324656578\n",
            "Progress: 57.5% ... Training loss: 0.062 ... Validation loss: 0.1580.0621666698858514\n",
            "0.16034526588234546\n",
            "Progress: 57.6% ... Training loss: 0.062 ... Validation loss: 0.1600.06849501860454091\n",
            "0.16706083333524743\n",
            "Progress: 57.6% ... Training loss: 0.068 ... Validation loss: 0.1670.06294786752542653\n",
            "0.15225780919945467\n",
            "Progress: 57.6% ... Training loss: 0.062 ... Validation loss: 0.1520.06438546487231116\n",
            "0.1559850810092821\n",
            "Progress: 57.6% ... Training loss: 0.064 ... Validation loss: 0.1550.06268235182850697\n",
            "0.1617093862708126\n",
            "Progress: 57.6% ... Training loss: 0.062 ... Validation loss: 0.1610.06436281763822072\n",
            "0.18060842813050612\n",
            "Progress: 57.7% ... Training loss: 0.064 ... Validation loss: 0.1800.08744024678922503\n",
            "0.16221209872961492\n",
            "Progress: 57.7% ... Training loss: 0.087 ... Validation loss: 0.1620.06278883130570134\n",
            "0.14863494129602323\n",
            "Progress: 57.7% ... Training loss: 0.062 ... Validation loss: 0.1480.07404844507681352\n",
            "0.1651464865371967\n",
            "Progress: 57.7% ... Training loss: 0.074 ... Validation loss: 0.1650.06365418534981974\n",
            "0.15860453280678136\n",
            "Progress: 57.7% ... Training loss: 0.063 ... Validation loss: 0.1580.06347554741070656\n",
            "0.16007739663236226\n",
            "Progress: 57.8% ... Training loss: 0.063 ... Validation loss: 0.1600.06803674378764987\n",
            "0.1547044247411993\n",
            "Progress: 57.8% ... Training loss: 0.068 ... Validation loss: 0.1540.06332758687549854\n",
            "0.1536397193504314\n",
            "Progress: 57.8% ... Training loss: 0.063 ... Validation loss: 0.1530.06242473494520088\n",
            "0.16503816337402158\n",
            "Progress: 57.8% ... Training loss: 0.062 ... Validation loss: 0.1650.06249385131918051\n",
            "0.16940853385083288\n",
            "Progress: 57.8% ... Training loss: 0.062 ... Validation loss: 0.1690.06351873493073283\n",
            "0.15436371476123406\n",
            "Progress: 57.9% ... Training loss: 0.063 ... Validation loss: 0.1540.061976653573167825\n",
            "0.16420544917928961\n",
            "Progress: 57.9% ... Training loss: 0.061 ... Validation loss: 0.1640.062396484670877814\n",
            "0.1616013470169847\n",
            "Progress: 57.9% ... Training loss: 0.062 ... Validation loss: 0.1610.07134499130196849\n",
            "0.1454838624524043\n",
            "Progress: 57.9% ... Training loss: 0.071 ... Validation loss: 0.1450.06447736924134176\n",
            "0.15253069832416596\n",
            "Progress: 57.9% ... Training loss: 0.064 ... Validation loss: 0.1520.0625187753555306\n",
            "0.16348820968886776\n",
            "Progress: 58.0% ... Training loss: 0.062 ... Validation loss: 0.1630.06408028878475795\n",
            "0.17558389343171255\n",
            "Progress: 58.0% ... Training loss: 0.064 ... Validation loss: 0.1750.07150364617835077\n",
            "0.1938937078508144\n",
            "Progress: 58.0% ... Training loss: 0.071 ... Validation loss: 0.1930.0618331314175475\n",
            "0.15221428093691639\n",
            "Progress: 58.0% ... Training loss: 0.061 ... Validation loss: 0.1520.06673510335742654\n",
            "0.13991296323441552\n",
            "Progress: 58.0% ... Training loss: 0.066 ... Validation loss: 0.1390.06337185005718755\n",
            "0.158005030506034\n",
            "Progress: 58.1% ... Training loss: 0.063 ... Validation loss: 0.1580.062285477483969336\n",
            "0.1538887428556145\n",
            "Progress: 58.1% ... Training loss: 0.062 ... Validation loss: 0.1530.06153454578013177\n",
            "0.16031596457067354\n",
            "Progress: 58.1% ... Training loss: 0.061 ... Validation loss: 0.1600.06383675991319467\n",
            "0.15493721157418483\n",
            "Progress: 58.1% ... Training loss: 0.063 ... Validation loss: 0.1540.0667121698298713\n",
            "0.1969394356876001\n",
            "Progress: 58.1% ... Training loss: 0.066 ... Validation loss: 0.1960.0639429967502708\n",
            "0.17946941916440137\n",
            "Progress: 58.2% ... Training loss: 0.063 ... Validation loss: 0.1790.07032351691516114\n",
            "0.1591089435558279\n",
            "Progress: 58.2% ... Training loss: 0.070 ... Validation loss: 0.1590.06229350805896442\n",
            "0.15946273896416344\n",
            "Progress: 58.2% ... Training loss: 0.062 ... Validation loss: 0.1590.062445742359122626\n",
            "0.16724337041771392\n",
            "Progress: 58.2% ... Training loss: 0.062 ... Validation loss: 0.1670.06208653494498952\n",
            "0.16563670691787152\n",
            "Progress: 58.2% ... Training loss: 0.062 ... Validation loss: 0.1650.06579710459748578\n",
            "0.18880459681535816\n",
            "Progress: 58.3% ... Training loss: 0.065 ... Validation loss: 0.1880.06580489828644684\n",
            "0.18128516114921364\n",
            "Progress: 58.3% ... Training loss: 0.065 ... Validation loss: 0.1810.06463154015666825\n",
            "0.1641149095377431\n",
            "Progress: 58.3% ... Training loss: 0.064 ... Validation loss: 0.1640.07055863449134449\n",
            "0.1952199053933774\n",
            "Progress: 58.3% ... Training loss: 0.070 ... Validation loss: 0.1950.0649330993546665\n",
            "0.148225673303585\n",
            "Progress: 58.3% ... Training loss: 0.064 ... Validation loss: 0.1480.06551452163060946\n",
            "0.15772327605606395\n",
            "Progress: 58.4% ... Training loss: 0.065 ... Validation loss: 0.1570.06399815026193342\n",
            "0.16984324779521745\n",
            "Progress: 58.4% ... Training loss: 0.063 ... Validation loss: 0.1690.0652008916566126\n",
            "0.18212843337314222\n",
            "Progress: 58.4% ... Training loss: 0.065 ... Validation loss: 0.1820.06330944141999524\n",
            "0.15527205005953584\n",
            "Progress: 58.4% ... Training loss: 0.063 ... Validation loss: 0.1550.062462729179727235\n",
            "0.1494704714273196\n",
            "Progress: 58.4% ... Training loss: 0.062 ... Validation loss: 0.1490.0633012865788476\n",
            "0.15303486609705735\n",
            "Progress: 58.5% ... Training loss: 0.063 ... Validation loss: 0.1530.06385520970102465\n",
            "0.17095893173891813\n",
            "Progress: 58.5% ... Training loss: 0.063 ... Validation loss: 0.1700.06221923535166474\n",
            "0.1672819774640107\n",
            "Progress: 58.5% ... Training loss: 0.062 ... Validation loss: 0.1670.06765215750841369\n",
            "0.14729718838420564\n",
            "Progress: 58.5% ... Training loss: 0.067 ... Validation loss: 0.1470.06581793910347077\n",
            "0.16433523074218206\n",
            "Progress: 58.5% ... Training loss: 0.065 ... Validation loss: 0.1640.06351427943567796\n",
            "0.1652110218411358\n",
            "Progress: 58.6% ... Training loss: 0.063 ... Validation loss: 0.1650.06369022352585696\n",
            "0.15381509543251276\n",
            "Progress: 58.6% ... Training loss: 0.063 ... Validation loss: 0.1530.06672422642463137\n",
            "0.16008485398677175\n",
            "Progress: 58.6% ... Training loss: 0.066 ... Validation loss: 0.1600.06457112805247103\n",
            "0.16696503630250661\n",
            "Progress: 58.6% ... Training loss: 0.064 ... Validation loss: 0.1660.06457968901065808\n",
            "0.15457422325848497\n",
            "Progress: 58.6% ... Training loss: 0.064 ... Validation loss: 0.1540.06333667788380969\n",
            "0.15060979388092502\n",
            "Progress: 58.7% ... Training loss: 0.063 ... Validation loss: 0.1500.06529756318821697\n",
            "0.1514754951623549\n",
            "Progress: 58.7% ... Training loss: 0.065 ... Validation loss: 0.1510.06424258646296158\n",
            "0.1609048904736972\n",
            "Progress: 58.7% ... Training loss: 0.064 ... Validation loss: 0.1600.06485700849584042\n",
            "0.16288608609777874\n",
            "Progress: 58.7% ... Training loss: 0.064 ... Validation loss: 0.1620.06774320355790636\n",
            "0.15239603993079218\n",
            "Progress: 58.7% ... Training loss: 0.067 ... Validation loss: 0.1520.0675774986846552\n",
            "0.15642033345468576\n",
            "Progress: 58.8% ... Training loss: 0.067 ... Validation loss: 0.1560.08063335141753182\n",
            "0.20771521829432743\n",
            "Progress: 58.8% ... Training loss: 0.080 ... Validation loss: 0.2070.06424080769052712\n",
            "0.152251016500325\n",
            "Progress: 58.8% ... Training loss: 0.064 ... Validation loss: 0.1520.07764974424355277\n",
            "0.1765367872603604\n",
            "Progress: 58.8% ... Training loss: 0.077 ... Validation loss: 0.1760.062409483747049\n",
            "0.15394906415293677\n",
            "Progress: 58.8% ... Training loss: 0.062 ... Validation loss: 0.1530.06682001540084241\n",
            "0.14743441790859602\n",
            "Progress: 58.9% ... Training loss: 0.066 ... Validation loss: 0.1470.0617572776286809\n",
            "0.1607145338516818\n",
            "Progress: 58.9% ... Training loss: 0.061 ... Validation loss: 0.1600.06285307031040822\n",
            "0.1655921296878269\n",
            "Progress: 58.9% ... Training loss: 0.062 ... Validation loss: 0.1650.07087969860210414\n",
            "0.21304087036991035\n",
            "Progress: 58.9% ... Training loss: 0.070 ... Validation loss: 0.2130.07124102325173795\n",
            "0.20704722548407098\n",
            "Progress: 58.9% ... Training loss: 0.071 ... Validation loss: 0.2070.06421355698689203\n",
            "0.15754690279501954\n",
            "Progress: 59.0% ... Training loss: 0.064 ... Validation loss: 0.1570.061888203783726836\n",
            "0.1726007223534748\n",
            "Progress: 59.0% ... Training loss: 0.061 ... Validation loss: 0.1720.0637478064153558\n",
            "0.17673312796073676\n",
            "Progress: 59.0% ... Training loss: 0.063 ... Validation loss: 0.1760.0654932764403578\n",
            "0.1667468242695433\n",
            "Progress: 59.0% ... Training loss: 0.065 ... Validation loss: 0.1660.06567019415394244\n",
            "0.18031870059115615\n",
            "Progress: 59.0% ... Training loss: 0.065 ... Validation loss: 0.1800.06789168793264654\n",
            "0.1529771102428539\n",
            "Progress: 59.1% ... Training loss: 0.067 ... Validation loss: 0.1520.0685164875609268\n",
            "0.17471965569339482\n",
            "Progress: 59.1% ... Training loss: 0.068 ... Validation loss: 0.1740.06856632705406274\n",
            "0.1539495970456238\n",
            "Progress: 59.1% ... Training loss: 0.068 ... Validation loss: 0.1530.06455546490072055\n",
            "0.1776947030461017\n",
            "Progress: 59.1% ... Training loss: 0.064 ... Validation loss: 0.1770.06900658803640189\n",
            "0.15236478193495306\n",
            "Progress: 59.1% ... Training loss: 0.069 ... Validation loss: 0.1520.06848032564965194\n",
            "0.18746775270218916\n",
            "Progress: 59.2% ... Training loss: 0.068 ... Validation loss: 0.1870.06286557342386619\n",
            "0.15526416948579908\n",
            "Progress: 59.2% ... Training loss: 0.062 ... Validation loss: 0.1550.06323376018056545\n",
            "0.1546862098811277\n",
            "Progress: 59.2% ... Training loss: 0.063 ... Validation loss: 0.1540.06250184992778877\n",
            "0.16352253838905123\n",
            "Progress: 59.2% ... Training loss: 0.062 ... Validation loss: 0.1630.06333880342609656\n",
            "0.16788450995418397\n",
            "Progress: 59.2% ... Training loss: 0.063 ... Validation loss: 0.1670.0618515050910591\n",
            "0.15788800589682286\n",
            "Progress: 59.3% ... Training loss: 0.061 ... Validation loss: 0.1570.06703405943405323\n",
            "0.15035955829230677\n",
            "Progress: 59.3% ... Training loss: 0.067 ... Validation loss: 0.1500.06291917550878594\n",
            "0.15573549031465916\n",
            "Progress: 59.3% ... Training loss: 0.062 ... Validation loss: 0.1550.0641271553646101\n",
            "0.18542854457438165\n",
            "Progress: 59.3% ... Training loss: 0.064 ... Validation loss: 0.1850.06101185152722833\n",
            "0.17148904635123327\n",
            "Progress: 59.3% ... Training loss: 0.061 ... Validation loss: 0.1710.06214354010695378\n",
            "0.16144401043868165\n",
            "Progress: 59.4% ... Training loss: 0.062 ... Validation loss: 0.1610.062175753116795365\n",
            "0.18433122625410647\n",
            "Progress: 59.4% ... Training loss: 0.062 ... Validation loss: 0.1840.06372028527077427\n",
            "0.19974990128398493\n",
            "Progress: 59.4% ... Training loss: 0.063 ... Validation loss: 0.1990.07530439349580849\n",
            "0.2181126739117768\n",
            "Progress: 59.4% ... Training loss: 0.075 ... Validation loss: 0.2180.06503718471780516\n",
            "0.1739963208798378\n",
            "Progress: 59.4% ... Training loss: 0.065 ... Validation loss: 0.1730.06792587019716163\n",
            "0.19922239989505633\n",
            "Progress: 59.5% ... Training loss: 0.067 ... Validation loss: 0.1990.06799898538386433\n",
            "0.15831917806470083\n",
            "Progress: 59.5% ... Training loss: 0.067 ... Validation loss: 0.1580.07811415204526649\n",
            "0.14801040440753566\n",
            "Progress: 59.5% ... Training loss: 0.078 ... Validation loss: 0.1480.06847202860180493\n",
            "0.16260215281696933\n",
            "Progress: 59.5% ... Training loss: 0.068 ... Validation loss: 0.1620.07449019641070698\n",
            "0.15471401051515987\n",
            "Progress: 59.5% ... Training loss: 0.074 ... Validation loss: 0.1540.06812287980409161\n",
            "0.15167447647491195\n",
            "Progress: 59.6% ... Training loss: 0.068 ... Validation loss: 0.1510.0692892466342811\n",
            "0.1868238569440163\n",
            "Progress: 59.6% ... Training loss: 0.069 ... Validation loss: 0.1860.06299750019922118\n",
            "0.15863827828283336\n",
            "Progress: 59.6% ... Training loss: 0.062 ... Validation loss: 0.1580.06192240416324372\n",
            "0.15534071947330474\n",
            "Progress: 59.6% ... Training loss: 0.061 ... Validation loss: 0.1550.06396432427448359\n",
            "0.14991903835234235\n",
            "Progress: 59.6% ... Training loss: 0.063 ... Validation loss: 0.1490.0645415277618911\n",
            "0.19139779296803677\n",
            "Progress: 59.7% ... Training loss: 0.064 ... Validation loss: 0.1910.06183236496927916\n",
            "0.17754639354813648\n",
            "Progress: 59.7% ... Training loss: 0.061 ... Validation loss: 0.1770.06344729966157944\n",
            "0.16483911499516113\n",
            "Progress: 59.7% ... Training loss: 0.063 ... Validation loss: 0.1640.06214055583739754\n",
            "0.17746086687025792\n",
            "Progress: 59.7% ... Training loss: 0.062 ... Validation loss: 0.1770.062481637013645945\n",
            "0.1883064668961016\n",
            "Progress: 59.7% ... Training loss: 0.062 ... Validation loss: 0.1880.06641750164172464\n",
            "0.1902911010415755\n",
            "Progress: 59.8% ... Training loss: 0.066 ... Validation loss: 0.1900.07329207250443945\n",
            "0.15196937772692856\n",
            "Progress: 59.8% ... Training loss: 0.073 ... Validation loss: 0.1510.06229751225607849\n",
            "0.16306682311616053\n",
            "Progress: 59.8% ... Training loss: 0.062 ... Validation loss: 0.1630.06754770316393653\n",
            "0.16780320477780372\n",
            "Progress: 59.8% ... Training loss: 0.067 ... Validation loss: 0.1670.062633766124103\n",
            "0.15981017847470827\n",
            "Progress: 59.8% ... Training loss: 0.062 ... Validation loss: 0.1590.06775959244015824\n",
            "0.16544996232492265\n",
            "Progress: 59.9% ... Training loss: 0.067 ... Validation loss: 0.1650.07212857752296614\n",
            "0.1516592447901989\n",
            "Progress: 59.9% ... Training loss: 0.072 ... Validation loss: 0.1510.0626694462529206\n",
            "0.16650270901067732\n",
            "Progress: 59.9% ... Training loss: 0.062 ... Validation loss: 0.1660.06536798012592446\n",
            "0.1512677742507468\n",
            "Progress: 59.9% ... Training loss: 0.065 ... Validation loss: 0.1510.06478368777893063\n",
            "0.17664909718445496\n",
            "Progress: 59.9% ... Training loss: 0.064 ... Validation loss: 0.1760.07492172364696108\n",
            "0.15081742694802436\n",
            "Progress: 60.0% ... Training loss: 0.074 ... Validation loss: 0.1500.06207625861934686\n",
            "0.15387222452827756\n",
            "Progress: 60.0% ... Training loss: 0.062 ... Validation loss: 0.1530.06187102642688603\n",
            "0.15540983203694536\n",
            "Progress: 60.0% ... Training loss: 0.061 ... Validation loss: 0.1550.06550963471477532\n",
            "0.14653295899296873\n",
            "Progress: 60.0% ... Training loss: 0.065 ... Validation loss: 0.1460.07646202262486895\n",
            "0.1506914131006776\n",
            "Progress: 60.0% ... Training loss: 0.076 ... Validation loss: 0.1500.06515717106249794\n",
            "0.18688010567453656\n",
            "Progress: 60.1% ... Training loss: 0.065 ... Validation loss: 0.1860.06470813597922116\n",
            "0.1611456304950788\n",
            "Progress: 60.1% ... Training loss: 0.064 ... Validation loss: 0.1610.06694758450701935\n",
            "0.20905521546617234\n",
            "Progress: 60.1% ... Training loss: 0.066 ... Validation loss: 0.2090.0643386140265474\n",
            "0.17144063624799571\n",
            "Progress: 60.1% ... Training loss: 0.064 ... Validation loss: 0.1710.061623549528820515\n",
            "0.1714758096719453\n",
            "Progress: 60.1% ... Training loss: 0.061 ... Validation loss: 0.1710.07024498059763237\n",
            "0.14923937877398508\n",
            "Progress: 60.2% ... Training loss: 0.070 ... Validation loss: 0.1490.06373762150840041\n",
            "0.1529663293615361\n",
            "Progress: 60.2% ... Training loss: 0.063 ... Validation loss: 0.1520.06229186310963474\n",
            "0.1715612137448084\n",
            "Progress: 60.2% ... Training loss: 0.062 ... Validation loss: 0.1710.06175482334973536\n",
            "0.14986844922495768\n",
            "Progress: 60.2% ... Training loss: 0.061 ... Validation loss: 0.1490.06380301458443988\n",
            "0.151204567455993\n",
            "Progress: 60.2% ... Training loss: 0.063 ... Validation loss: 0.1510.062316712246223226\n",
            "0.15665430193508398\n",
            "Progress: 60.3% ... Training loss: 0.062 ... Validation loss: 0.1560.06469181362992332\n",
            "0.17009003950952256\n",
            "Progress: 60.3% ... Training loss: 0.064 ... Validation loss: 0.1700.06214325694781437\n",
            "0.17323781211163786\n",
            "Progress: 60.3% ... Training loss: 0.062 ... Validation loss: 0.1730.06382702511639235\n",
            "0.17974719494536925\n",
            "Progress: 60.3% ... Training loss: 0.063 ... Validation loss: 0.1790.06206958627379\n",
            "0.15703490388915364\n",
            "Progress: 60.3% ... Training loss: 0.062 ... Validation loss: 0.1570.07615068986585956\n",
            "0.20234163974371622\n",
            "Progress: 60.4% ... Training loss: 0.076 ... Validation loss: 0.2020.06344030855754464\n",
            "0.1623318996778481\n",
            "Progress: 60.4% ... Training loss: 0.063 ... Validation loss: 0.1620.06422595667811212\n",
            "0.15143488621474316\n",
            "Progress: 60.4% ... Training loss: 0.064 ... Validation loss: 0.1510.061890209039591915\n",
            "0.1536946972710847\n",
            "Progress: 60.4% ... Training loss: 0.061 ... Validation loss: 0.1530.06508862588417652\n",
            "0.16665793285550082\n",
            "Progress: 60.4% ... Training loss: 0.065 ... Validation loss: 0.1660.061370670611642475\n",
            "0.15034979054670533\n",
            "Progress: 60.5% ... Training loss: 0.061 ... Validation loss: 0.1500.06273030557772902\n",
            "0.15084514262989457\n",
            "Progress: 60.5% ... Training loss: 0.062 ... Validation loss: 0.1500.06409476057146271\n",
            "0.15515599752978598\n",
            "Progress: 60.5% ... Training loss: 0.064 ... Validation loss: 0.1550.06415536112031925\n",
            "0.1542366725351821\n",
            "Progress: 60.5% ... Training loss: 0.064 ... Validation loss: 0.1540.06673421057903502\n",
            "0.17985336148888437\n",
            "Progress: 60.5% ... Training loss: 0.066 ... Validation loss: 0.1790.06435189620571245\n",
            "0.17229115008418847\n",
            "Progress: 60.6% ... Training loss: 0.064 ... Validation loss: 0.1720.0646422399264033\n",
            "0.15137867719317313\n",
            "Progress: 60.6% ... Training loss: 0.064 ... Validation loss: 0.1510.06448861503984075\n",
            "0.14702912193435946\n",
            "Progress: 60.6% ... Training loss: 0.064 ... Validation loss: 0.1470.06568558437021559\n",
            "0.15754532562084925\n",
            "Progress: 60.6% ... Training loss: 0.065 ... Validation loss: 0.1570.06198978296010627\n",
            "0.17230530753692191\n",
            "Progress: 60.6% ... Training loss: 0.061 ... Validation loss: 0.1720.06557641166920948\n",
            "0.1584930954087021\n",
            "Progress: 60.7% ... Training loss: 0.065 ... Validation loss: 0.1580.07949036268165072\n",
            "0.14743028483815435\n",
            "Progress: 60.7% ... Training loss: 0.079 ... Validation loss: 0.1470.06507834826436966\n",
            "0.19078149211435674\n",
            "Progress: 60.7% ... Training loss: 0.065 ... Validation loss: 0.1900.06090072323673191\n",
            "0.1677523479332138\n",
            "Progress: 60.7% ... Training loss: 0.060 ... Validation loss: 0.1670.06582970508041881\n",
            "0.1548828159909499\n",
            "Progress: 60.7% ... Training loss: 0.065 ... Validation loss: 0.1540.06509839270690114\n",
            "0.15404716309275135\n",
            "Progress: 60.8% ... Training loss: 0.065 ... Validation loss: 0.1540.06432458917929286\n",
            "0.15584360459600466\n",
            "Progress: 60.8% ... Training loss: 0.064 ... Validation loss: 0.1550.067376029529994\n",
            "0.19215674357547907\n",
            "Progress: 60.8% ... Training loss: 0.067 ... Validation loss: 0.1920.06580569202414394\n",
            "0.21334361405860547\n",
            "Progress: 60.8% ... Training loss: 0.065 ... Validation loss: 0.2130.061764638442200856\n",
            "0.16291194452924695\n",
            "Progress: 60.8% ... Training loss: 0.061 ... Validation loss: 0.1620.06686273217136751\n",
            "0.15021049984079216\n",
            "Progress: 60.9% ... Training loss: 0.066 ... Validation loss: 0.1500.06090454607746417\n",
            "0.17239737345757028\n",
            "Progress: 60.9% ... Training loss: 0.060 ... Validation loss: 0.1720.06599149550684999\n",
            "0.16408578869449952\n",
            "Progress: 60.9% ... Training loss: 0.065 ... Validation loss: 0.1640.0610844605226184\n",
            "0.16334868767856997\n",
            "Progress: 60.9% ... Training loss: 0.061 ... Validation loss: 0.1630.06240517160469606\n",
            "0.168785761144747\n",
            "Progress: 60.9% ... Training loss: 0.062 ... Validation loss: 0.1680.06535990810799551\n",
            "0.1475198794601193\n",
            "Progress: 61.0% ... Training loss: 0.065 ... Validation loss: 0.1470.06107617016313522\n",
            "0.16710604991808642\n",
            "Progress: 61.0% ... Training loss: 0.061 ... Validation loss: 0.1670.06292575717875706\n",
            "0.1745513890423865\n",
            "Progress: 61.0% ... Training loss: 0.062 ... Validation loss: 0.1740.06502998757480588\n",
            "0.15401792711078893\n",
            "Progress: 61.0% ... Training loss: 0.065 ... Validation loss: 0.1540.06388914804260971\n",
            "0.17450863664380678\n",
            "Progress: 61.0% ... Training loss: 0.063 ... Validation loss: 0.1740.061012900494348106\n",
            "0.1654629788990736\n",
            "Progress: 61.1% ... Training loss: 0.061 ... Validation loss: 0.1650.06356822217350275\n",
            "0.15729694278895745\n",
            "Progress: 61.1% ... Training loss: 0.063 ... Validation loss: 0.1570.06356397503464252\n",
            "0.16492426204281535\n",
            "Progress: 61.1% ... Training loss: 0.063 ... Validation loss: 0.1640.061266178133346165\n",
            "0.15237393460803908\n",
            "Progress: 61.1% ... Training loss: 0.061 ... Validation loss: 0.1520.07458596590334146\n",
            "0.14931450481999428\n",
            "Progress: 61.1% ... Training loss: 0.074 ... Validation loss: 0.1490.06292583481998335\n",
            "0.14929245154729426\n",
            "Progress: 61.2% ... Training loss: 0.062 ... Validation loss: 0.1490.0610810480525856\n",
            "0.15012227924282642\n",
            "Progress: 61.2% ... Training loss: 0.061 ... Validation loss: 0.1500.06667934266389261\n",
            "0.18087347521636968\n",
            "Progress: 61.2% ... Training loss: 0.066 ... Validation loss: 0.1800.06134896111453365\n",
            "0.14885231792629264\n",
            "Progress: 61.2% ... Training loss: 0.061 ... Validation loss: 0.1480.06697752550890458\n",
            "0.18220811142817683\n",
            "Progress: 61.2% ... Training loss: 0.066 ... Validation loss: 0.1820.06875204548874288\n",
            "0.14735405957182607\n",
            "Progress: 61.3% ... Training loss: 0.068 ... Validation loss: 0.1470.06107582636680166\n",
            "0.17229394824248553\n",
            "Progress: 61.3% ... Training loss: 0.061 ... Validation loss: 0.1720.06162526825863531\n",
            "0.17024656899828983\n",
            "Progress: 61.3% ... Training loss: 0.061 ... Validation loss: 0.1700.062057196119115365\n",
            "0.17297458392852974\n",
            "Progress: 61.3% ... Training loss: 0.062 ... Validation loss: 0.1720.061068347948308226\n",
            "0.1589142868502703\n",
            "Progress: 61.3% ... Training loss: 0.061 ... Validation loss: 0.1580.06118668479330261\n",
            "0.1626654788467012\n",
            "Progress: 61.4% ... Training loss: 0.061 ... Validation loss: 0.1620.061052232012076234\n",
            "0.14816983305325226\n",
            "Progress: 61.4% ... Training loss: 0.061 ... Validation loss: 0.1480.06252906640505594\n",
            "0.16252049601836158\n",
            "Progress: 61.4% ... Training loss: 0.062 ... Validation loss: 0.1620.06287298673449761\n",
            "0.16847870851110955\n",
            "Progress: 61.4% ... Training loss: 0.062 ... Validation loss: 0.1680.06518127280408055\n",
            "0.18000112677622024\n",
            "Progress: 61.4% ... Training loss: 0.065 ... Validation loss: 0.1800.06485140228316552\n",
            "0.18001627671762133\n",
            "Progress: 61.5% ... Training loss: 0.064 ... Validation loss: 0.1800.06242078072906307\n",
            "0.14881435424448317\n",
            "Progress: 61.5% ... Training loss: 0.062 ... Validation loss: 0.1480.06392105265212841\n",
            "0.1619825497533274\n",
            "Progress: 61.5% ... Training loss: 0.063 ... Validation loss: 0.1610.06244989980302196\n",
            "0.15327841690995495\n",
            "Progress: 61.5% ... Training loss: 0.062 ... Validation loss: 0.1530.0632700192271996\n",
            "0.16415344347124894\n",
            "Progress: 61.5% ... Training loss: 0.063 ... Validation loss: 0.1640.06222317809688944\n",
            "0.14733526283132317\n",
            "Progress: 61.6% ... Training loss: 0.062 ... Validation loss: 0.1470.06548837348458982\n",
            "0.1807162603136723\n",
            "Progress: 61.6% ... Training loss: 0.065 ... Validation loss: 0.1800.06774580841685263\n",
            "0.14940572133602584\n",
            "Progress: 61.6% ... Training loss: 0.067 ... Validation loss: 0.1490.06462609199655601\n",
            "0.14194675639552184\n",
            "Progress: 61.6% ... Training loss: 0.064 ... Validation loss: 0.1410.06355580689210025\n",
            "0.14769225791445734\n",
            "Progress: 61.6% ... Training loss: 0.063 ... Validation loss: 0.1470.061759909281325506\n",
            "0.16766288917098537\n",
            "Progress: 61.7% ... Training loss: 0.061 ... Validation loss: 0.1670.062303641707202984\n",
            "0.15940572543211526\n",
            "Progress: 61.7% ... Training loss: 0.062 ... Validation loss: 0.1590.07511328108006016\n",
            "0.14556607395691262\n",
            "Progress: 61.7% ... Training loss: 0.075 ... Validation loss: 0.1450.06257517835952812\n",
            "0.15121821930877596\n",
            "Progress: 61.7% ... Training loss: 0.062 ... Validation loss: 0.1510.06520652788022425\n",
            "0.14765970119308444\n",
            "Progress: 61.7% ... Training loss: 0.065 ... Validation loss: 0.1470.08029423937092516\n",
            "0.18577453152993606\n",
            "Progress: 61.8% ... Training loss: 0.080 ... Validation loss: 0.1850.06295954120904715\n",
            "0.14068668134769388\n",
            "Progress: 61.8% ... Training loss: 0.062 ... Validation loss: 0.1400.062207257135828406\n",
            "0.14440974629005454\n",
            "Progress: 61.8% ... Training loss: 0.062 ... Validation loss: 0.1440.062250173932656784\n",
            "0.14864399519148294\n",
            "Progress: 61.8% ... Training loss: 0.062 ... Validation loss: 0.1480.06134968212788816\n",
            "0.16388939834843047\n",
            "Progress: 61.8% ... Training loss: 0.061 ... Validation loss: 0.1630.06253646264248339\n",
            "0.15541910874043505\n",
            "Progress: 61.9% ... Training loss: 0.062 ... Validation loss: 0.1550.06389939091535445\n",
            "0.19415720385911367\n",
            "Progress: 61.9% ... Training loss: 0.063 ... Validation loss: 0.1940.0643934741253439\n",
            "0.14833819121919195\n",
            "Progress: 61.9% ... Training loss: 0.064 ... Validation loss: 0.1480.06549217994387728\n",
            "0.14396684213841746\n",
            "Progress: 61.9% ... Training loss: 0.065 ... Validation loss: 0.1430.06167961505609844\n",
            "0.15806312810828094\n",
            "Progress: 61.9% ... Training loss: 0.061 ... Validation loss: 0.1580.0617218905117151\n",
            "0.15121578777239764\n",
            "Progress: 62.0% ... Training loss: 0.061 ... Validation loss: 0.1510.07240626164239118\n",
            "0.15276709525262996\n",
            "Progress: 62.0% ... Training loss: 0.072 ... Validation loss: 0.1520.06404812145651662\n",
            "0.15892678321655607\n",
            "Progress: 62.0% ... Training loss: 0.064 ... Validation loss: 0.1580.06382071674638089\n",
            "0.15255938197954916\n",
            "Progress: 62.0% ... Training loss: 0.063 ... Validation loss: 0.1520.06252662700468727\n",
            "0.14983892205840996\n",
            "Progress: 62.0% ... Training loss: 0.062 ... Validation loss: 0.1490.06572660720252735\n",
            "0.15284113407313657\n",
            "Progress: 62.1% ... Training loss: 0.065 ... Validation loss: 0.1520.067242392864608\n",
            "0.18720206868838526\n",
            "Progress: 62.1% ... Training loss: 0.067 ... Validation loss: 0.1870.06290218484980917\n",
            "0.1519726841520082\n",
            "Progress: 62.1% ... Training loss: 0.062 ... Validation loss: 0.1510.062226661506147585\n",
            "0.15487891577656135\n",
            "Progress: 62.1% ... Training loss: 0.062 ... Validation loss: 0.1540.07634222597736262\n",
            "0.1501903234515167\n",
            "Progress: 62.1% ... Training loss: 0.076 ... Validation loss: 0.1500.06607092698159892\n",
            "0.1476019015476795\n",
            "Progress: 62.2% ... Training loss: 0.066 ... Validation loss: 0.1470.06485315967711122\n",
            "0.16376407792567826\n",
            "Progress: 62.2% ... Training loss: 0.064 ... Validation loss: 0.1630.0642770598758156\n",
            "0.16006659477833535\n",
            "Progress: 62.2% ... Training loss: 0.064 ... Validation loss: 0.1600.06895071364643876\n",
            "0.14730465745913163\n",
            "Progress: 62.2% ... Training loss: 0.068 ... Validation loss: 0.1470.06292824582151406\n",
            "0.15837596936660558\n",
            "Progress: 62.2% ... Training loss: 0.062 ... Validation loss: 0.1580.06553083713615247\n",
            "0.16670129439343545\n",
            "Progress: 62.3% ... Training loss: 0.065 ... Validation loss: 0.1660.06697915193757893\n",
            "0.1551736164666827\n",
            "Progress: 62.3% ... Training loss: 0.066 ... Validation loss: 0.1550.0632644742507503\n",
            "0.14956596635130712\n",
            "Progress: 62.3% ... Training loss: 0.063 ... Validation loss: 0.1490.06354147724347069\n",
            "0.16357018956675048\n",
            "Progress: 62.3% ... Training loss: 0.063 ... Validation loss: 0.1630.061628562808476894\n",
            "0.15159351191728537\n",
            "Progress: 62.3% ... Training loss: 0.061 ... Validation loss: 0.1510.061142731929571\n",
            "0.16721893679028615\n",
            "Progress: 62.4% ... Training loss: 0.061 ... Validation loss: 0.1670.06208349863874744\n",
            "0.16368106210391145\n",
            "Progress: 62.4% ... Training loss: 0.062 ... Validation loss: 0.1630.06978760813282117\n",
            "0.15128371012116573\n",
            "Progress: 62.4% ... Training loss: 0.069 ... Validation loss: 0.1510.06115818951630461\n",
            "0.1583511020845691\n",
            "Progress: 62.4% ... Training loss: 0.061 ... Validation loss: 0.1580.06199066410920178\n",
            "0.14724955527818365\n",
            "Progress: 62.4% ... Training loss: 0.061 ... Validation loss: 0.1470.061287819031266094\n",
            "0.15668359364894477\n",
            "Progress: 62.5% ... Training loss: 0.061 ... Validation loss: 0.1560.062124286169943754\n",
            "0.16642829718010457\n",
            "Progress: 62.5% ... Training loss: 0.062 ... Validation loss: 0.1660.06804431351843832\n",
            "0.15160893867746342\n",
            "Progress: 62.5% ... Training loss: 0.068 ... Validation loss: 0.1510.06069980256152145\n",
            "0.16459231170050206\n",
            "Progress: 62.5% ... Training loss: 0.060 ... Validation loss: 0.1640.06452556825519086\n",
            "0.167685428953601\n",
            "Progress: 62.5% ... Training loss: 0.064 ... Validation loss: 0.1670.06320343449240896\n",
            "0.15313416535710447\n",
            "Progress: 62.6% ... Training loss: 0.063 ... Validation loss: 0.1530.07968592415401482\n",
            "0.15300977374175745\n",
            "Progress: 62.6% ... Training loss: 0.079 ... Validation loss: 0.1530.06128597405617916\n",
            "0.15891734514744413\n",
            "Progress: 62.6% ... Training loss: 0.061 ... Validation loss: 0.1580.07195604043134568\n",
            "0.14780023320086322\n",
            "Progress: 62.6% ... Training loss: 0.071 ... Validation loss: 0.1470.06274858539190174\n",
            "0.14973419959932271\n",
            "Progress: 62.6% ... Training loss: 0.062 ... Validation loss: 0.1490.06800793812335984\n",
            "0.2040707596736213\n",
            "Progress: 62.7% ... Training loss: 0.068 ... Validation loss: 0.2040.06168917301789793\n",
            "0.16452643972778072\n",
            "Progress: 62.7% ... Training loss: 0.061 ... Validation loss: 0.1640.07031960310121471\n",
            "0.20692210605119302\n",
            "Progress: 62.7% ... Training loss: 0.070 ... Validation loss: 0.2060.08147520317156304\n",
            "0.1435614042777734\n",
            "Progress: 62.7% ... Training loss: 0.081 ... Validation loss: 0.1430.06646964820803483\n",
            "0.18872320069881177\n",
            "Progress: 62.7% ... Training loss: 0.066 ... Validation loss: 0.1880.06132886089669996\n",
            "0.1587383824757701\n",
            "Progress: 62.8% ... Training loss: 0.061 ... Validation loss: 0.1580.06537642277298338\n",
            "0.16947182899153712\n",
            "Progress: 62.8% ... Training loss: 0.065 ... Validation loss: 0.1690.061954321354819535\n",
            "0.15940642484254047\n",
            "Progress: 62.8% ... Training loss: 0.061 ... Validation loss: 0.1590.06143220403968472\n",
            "0.17543716363064668\n",
            "Progress: 62.8% ... Training loss: 0.061 ... Validation loss: 0.1750.061063070401301954\n",
            "0.17526926166813203\n",
            "Progress: 62.8% ... Training loss: 0.061 ... Validation loss: 0.1750.06059047960182241\n",
            "0.1675215059441699\n",
            "Progress: 62.9% ... Training loss: 0.060 ... Validation loss: 0.1670.07805520856344347\n",
            "0.15105874714335815\n",
            "Progress: 62.9% ... Training loss: 0.078 ... Validation loss: 0.1510.06666389411406902\n",
            "0.17643069592323574\n",
            "Progress: 62.9% ... Training loss: 0.066 ... Validation loss: 0.1760.07463946397935933\n",
            "0.20997006436861104\n",
            "Progress: 62.9% ... Training loss: 0.074 ... Validation loss: 0.2090.0639489520285341\n",
            "0.16109089324862305\n",
            "Progress: 62.9% ... Training loss: 0.063 ... Validation loss: 0.1610.06165087028578445\n",
            "0.18505219156991537\n",
            "Progress: 63.0% ... Training loss: 0.061 ... Validation loss: 0.1850.06589098661855171\n",
            "0.15681342045938101\n",
            "Progress: 63.0% ... Training loss: 0.065 ... Validation loss: 0.1560.061380936126982304\n",
            "0.1646594227374294\n",
            "Progress: 63.0% ... Training loss: 0.061 ... Validation loss: 0.1640.08036791488579635\n",
            "0.2147246076433943\n",
            "Progress: 63.0% ... Training loss: 0.080 ... Validation loss: 0.2140.06407913355437189\n",
            "0.14684564879168574\n",
            "Progress: 63.0% ... Training loss: 0.064 ... Validation loss: 0.1460.062082283684603924\n",
            "0.16427889229789763\n",
            "Progress: 63.1% ... Training loss: 0.062 ... Validation loss: 0.1640.06550501037166548\n",
            "0.1566306341590465\n",
            "Progress: 63.1% ... Training loss: 0.065 ... Validation loss: 0.1560.07121919138282412\n",
            "0.14800425759435962\n",
            "Progress: 63.1% ... Training loss: 0.071 ... Validation loss: 0.1480.06340444553401324\n",
            "0.15422722094234842\n",
            "Progress: 63.1% ... Training loss: 0.063 ... Validation loss: 0.1540.06422404936174612\n",
            "0.1646885147817271\n",
            "Progress: 63.1% ... Training loss: 0.064 ... Validation loss: 0.1640.06316755297278129\n",
            "0.14260096048054385\n",
            "Progress: 63.2% ... Training loss: 0.063 ... Validation loss: 0.1420.060747298569015604\n",
            "0.16697467017357648\n",
            "Progress: 63.2% ... Training loss: 0.060 ... Validation loss: 0.1660.06216717890579866\n",
            "0.15664255846088523\n",
            "Progress: 63.2% ... Training loss: 0.062 ... Validation loss: 0.1560.061536185668976034\n",
            "0.15096182409124567\n",
            "Progress: 63.2% ... Training loss: 0.061 ... Validation loss: 0.1500.06220618507741057\n",
            "0.15789376511059178\n",
            "Progress: 63.2% ... Training loss: 0.062 ... Validation loss: 0.1570.06230752359808075\n",
            "0.17233764284048209\n",
            "Progress: 63.3% ... Training loss: 0.062 ... Validation loss: 0.1720.0746945428463355\n",
            "0.17749204673292673\n",
            "Progress: 63.3% ... Training loss: 0.074 ... Validation loss: 0.1770.0611490358712027\n",
            "0.1576057450626011\n",
            "Progress: 63.3% ... Training loss: 0.061 ... Validation loss: 0.1570.06706282211236596\n",
            "0.19063623552032197\n",
            "Progress: 63.3% ... Training loss: 0.067 ... Validation loss: 0.1900.06451257617916979\n",
            "0.1723518261774916\n",
            "Progress: 63.3% ... Training loss: 0.064 ... Validation loss: 0.1720.06181408739439345\n",
            "0.16111578297346982\n",
            "Progress: 63.4% ... Training loss: 0.061 ... Validation loss: 0.1610.06076399507189561\n",
            "0.17326818845743866\n",
            "Progress: 63.4% ... Training loss: 0.060 ... Validation loss: 0.1730.06123393139721076\n",
            "0.16266723826125945\n",
            "Progress: 63.4% ... Training loss: 0.061 ... Validation loss: 0.1620.06368970963978665\n",
            "0.17722515877952588\n",
            "Progress: 63.4% ... Training loss: 0.063 ... Validation loss: 0.1770.060944071544998805\n",
            "0.16661789188126808\n",
            "Progress: 63.4% ... Training loss: 0.060 ... Validation loss: 0.1660.06185644476186263\n",
            "0.1704790925182539\n",
            "Progress: 63.5% ... Training loss: 0.061 ... Validation loss: 0.1700.06141244066007679\n",
            "0.1557367142474879\n",
            "Progress: 63.5% ... Training loss: 0.061 ... Validation loss: 0.1550.06906622511742208\n",
            "0.15600817216154306\n",
            "Progress: 63.5% ... Training loss: 0.069 ... Validation loss: 0.1560.07031811887268317\n",
            "0.1520091630244925\n",
            "Progress: 63.5% ... Training loss: 0.070 ... Validation loss: 0.1520.06387258281232026\n",
            "0.1553671805314823\n",
            "Progress: 63.5% ... Training loss: 0.063 ... Validation loss: 0.1550.06228720410044914\n",
            "0.16860392019747059\n",
            "Progress: 63.6% ... Training loss: 0.062 ... Validation loss: 0.1680.06499847056157729\n",
            "0.1950915204047148\n",
            "Progress: 63.6% ... Training loss: 0.064 ... Validation loss: 0.1950.061267931926942985\n",
            "0.17145080788264766\n",
            "Progress: 63.6% ... Training loss: 0.061 ... Validation loss: 0.1710.06081501201421691\n",
            "0.15437290202011347\n",
            "Progress: 63.6% ... Training loss: 0.060 ... Validation loss: 0.1540.062079807083531786\n",
            "0.16147518025656804\n",
            "Progress: 63.6% ... Training loss: 0.062 ... Validation loss: 0.1610.06560356241037903\n",
            "0.14873814601676683\n",
            "Progress: 63.7% ... Training loss: 0.065 ... Validation loss: 0.1480.06147520060381184\n",
            "0.14482652226280227\n",
            "Progress: 63.7% ... Training loss: 0.061 ... Validation loss: 0.1440.06519603736963465\n",
            "0.14302161968470253\n",
            "Progress: 63.7% ... Training loss: 0.065 ... Validation loss: 0.1430.06907838860544699\n",
            "0.15803108100406418\n",
            "Progress: 63.7% ... Training loss: 0.069 ... Validation loss: 0.1580.0694157378998299\n",
            "0.14787261454309092\n",
            "Progress: 63.7% ... Training loss: 0.069 ... Validation loss: 0.1470.08704225581799882\n",
            "0.19253956965552615\n",
            "Progress: 63.8% ... Training loss: 0.087 ... Validation loss: 0.1920.06212127481213246\n",
            "0.14414816366179994\n",
            "Progress: 63.8% ... Training loss: 0.062 ... Validation loss: 0.1440.06438268987098976\n",
            "0.14497475934833431\n",
            "Progress: 63.8% ... Training loss: 0.064 ... Validation loss: 0.1440.0629855328311772\n",
            "0.1809654745454084\n",
            "Progress: 63.8% ... Training loss: 0.062 ... Validation loss: 0.1800.06275167051637291\n",
            "0.15351764460604125\n",
            "Progress: 63.8% ... Training loss: 0.062 ... Validation loss: 0.1530.07066606077050229\n",
            "0.20212636797126338\n",
            "Progress: 63.9% ... Training loss: 0.070 ... Validation loss: 0.2020.06431165646088863\n",
            "0.16247341546089708\n",
            "Progress: 63.9% ... Training loss: 0.064 ... Validation loss: 0.1620.0678217854651691\n",
            "0.16276833378353633\n",
            "Progress: 63.9% ... Training loss: 0.067 ... Validation loss: 0.1620.06307835608422274\n",
            "0.1477172543616837\n",
            "Progress: 63.9% ... Training loss: 0.063 ... Validation loss: 0.1470.06849015975251847\n",
            "0.17454018187723497\n",
            "Progress: 63.9% ... Training loss: 0.068 ... Validation loss: 0.1740.061942312407983514\n",
            "0.1687694345311442\n",
            "Progress: 64.0% ... Training loss: 0.061 ... Validation loss: 0.1680.062017739718990716\n",
            "0.15091625385040086\n",
            "Progress: 64.0% ... Training loss: 0.062 ... Validation loss: 0.1500.06292232479900739\n",
            "0.1522886088802066\n",
            "Progress: 64.0% ... Training loss: 0.062 ... Validation loss: 0.1520.06617119199068891\n",
            "0.1622984917144891\n",
            "Progress: 64.0% ... Training loss: 0.066 ... Validation loss: 0.1620.06431481312943192\n",
            "0.14894785102003696\n",
            "Progress: 64.0% ... Training loss: 0.064 ... Validation loss: 0.1480.062821828816054\n",
            "0.15526247633208784\n",
            "Progress: 64.1% ... Training loss: 0.062 ... Validation loss: 0.1550.06325094127263779\n",
            "0.16263342999333827\n",
            "Progress: 64.1% ... Training loss: 0.063 ... Validation loss: 0.1620.06165340127651553\n",
            "0.15946050166317086\n",
            "Progress: 64.1% ... Training loss: 0.061 ... Validation loss: 0.1590.06287030233052142\n",
            "0.1450013397028056\n",
            "Progress: 64.1% ... Training loss: 0.062 ... Validation loss: 0.1450.06410364628868737\n",
            "0.17488399609089142\n",
            "Progress: 64.1% ... Training loss: 0.064 ... Validation loss: 0.1740.07681467095755168\n",
            "0.15644987060829252\n",
            "Progress: 64.2% ... Training loss: 0.076 ... Validation loss: 0.1560.059976611363768015\n",
            "0.15870839915166682\n",
            "Progress: 64.2% ... Training loss: 0.059 ... Validation loss: 0.1580.06314839306779883\n",
            "0.1553910657910463\n",
            "Progress: 64.2% ... Training loss: 0.063 ... Validation loss: 0.1550.06213172923140867\n",
            "0.15857675431550927\n",
            "Progress: 64.2% ... Training loss: 0.062 ... Validation loss: 0.1580.06130647390588923\n",
            "0.1592242324153276\n",
            "Progress: 64.2% ... Training loss: 0.061 ... Validation loss: 0.1590.06223084950557853\n",
            "0.16410342054391647\n",
            "Progress: 64.3% ... Training loss: 0.062 ... Validation loss: 0.1640.06120405345792757\n",
            "0.16301503532221004\n",
            "Progress: 64.3% ... Training loss: 0.061 ... Validation loss: 0.1630.07175224988627457\n",
            "0.1506297274057754\n",
            "Progress: 64.3% ... Training loss: 0.071 ... Validation loss: 0.1500.06403431000177119\n",
            "0.17576069539417105\n",
            "Progress: 64.3% ... Training loss: 0.064 ... Validation loss: 0.1750.062042761673390734\n",
            "0.1488390583474181\n",
            "Progress: 64.3% ... Training loss: 0.062 ... Validation loss: 0.1480.06109171885883573\n",
            "0.17416075210959972\n",
            "Progress: 64.4% ... Training loss: 0.061 ... Validation loss: 0.1740.0765560319546175\n",
            "0.22438426278580317\n",
            "Progress: 64.4% ... Training loss: 0.076 ... Validation loss: 0.2240.0628729011231734\n",
            "0.17280412621040867\n",
            "Progress: 64.4% ... Training loss: 0.062 ... Validation loss: 0.1720.06423606118883289\n",
            "0.15224543171120783\n",
            "Progress: 64.4% ... Training loss: 0.064 ... Validation loss: 0.1520.06304572054587451\n",
            "0.15798382420019663\n",
            "Progress: 64.4% ... Training loss: 0.063 ... Validation loss: 0.1570.06238639109592393\n",
            "0.16043112582214555\n",
            "Progress: 64.5% ... Training loss: 0.062 ... Validation loss: 0.1600.060638898055170636\n",
            "0.16294260161309237\n",
            "Progress: 64.5% ... Training loss: 0.060 ... Validation loss: 0.1620.06201915629851291\n",
            "0.17102222076129994\n",
            "Progress: 64.5% ... Training loss: 0.062 ... Validation loss: 0.1710.061709353896525154\n",
            "0.16703409514420495\n",
            "Progress: 64.5% ... Training loss: 0.061 ... Validation loss: 0.1670.060553776076371704\n",
            "0.17058895572516303\n",
            "Progress: 64.5% ... Training loss: 0.060 ... Validation loss: 0.1700.06397651298421918\n",
            "0.1846028747425483\n",
            "Progress: 64.6% ... Training loss: 0.063 ... Validation loss: 0.1840.06074841928207676\n",
            "0.16587616064876415\n",
            "Progress: 64.6% ... Training loss: 0.060 ... Validation loss: 0.1650.06154489200382288\n",
            "0.15069919451540342\n",
            "Progress: 64.6% ... Training loss: 0.061 ... Validation loss: 0.1500.061897840534295846\n",
            "0.16111718234289416\n",
            "Progress: 64.6% ... Training loss: 0.061 ... Validation loss: 0.1610.06424569755881594\n",
            "0.17306077860840388\n",
            "Progress: 64.6% ... Training loss: 0.064 ... Validation loss: 0.1730.06048479397465422\n",
            "0.1604361939185142\n",
            "Progress: 64.7% ... Training loss: 0.060 ... Validation loss: 0.1600.06282732188460716\n",
            "0.17121111379498025\n",
            "Progress: 64.7% ... Training loss: 0.062 ... Validation loss: 0.1710.0623858307305998\n",
            "0.16884042197628657\n",
            "Progress: 64.7% ... Training loss: 0.062 ... Validation loss: 0.1680.06086913723723332\n",
            "0.16133098715609195\n",
            "Progress: 64.7% ... Training loss: 0.060 ... Validation loss: 0.1610.06272673382840373\n",
            "0.1558413025172934\n",
            "Progress: 64.7% ... Training loss: 0.062 ... Validation loss: 0.1550.06298439506970004\n",
            "0.16787804006521517\n",
            "Progress: 64.8% ... Training loss: 0.062 ... Validation loss: 0.1670.061553584701888736\n",
            "0.1610298978122859\n",
            "Progress: 64.8% ... Training loss: 0.061 ... Validation loss: 0.1610.06002063535789859\n",
            "0.1624359461637195\n",
            "Progress: 64.8% ... Training loss: 0.060 ... Validation loss: 0.1620.06369152276897314\n",
            "0.15680675304233496\n",
            "Progress: 64.8% ... Training loss: 0.063 ... Validation loss: 0.1560.06314473790710204\n",
            "0.14388316245292634\n",
            "Progress: 64.8% ... Training loss: 0.063 ... Validation loss: 0.1430.06292106906747748\n",
            "0.16494171522548118\n",
            "Progress: 64.9% ... Training loss: 0.062 ... Validation loss: 0.1640.07096176044720355\n",
            "0.17056305179243647\n",
            "Progress: 64.9% ... Training loss: 0.070 ... Validation loss: 0.1700.0657793486536367\n",
            "0.17969973879331877\n",
            "Progress: 64.9% ... Training loss: 0.065 ... Validation loss: 0.1790.06419697764900008\n",
            "0.1786672646719002\n",
            "Progress: 64.9% ... Training loss: 0.064 ... Validation loss: 0.1780.062401015656318184\n",
            "0.14921519836930117\n",
            "Progress: 64.9% ... Training loss: 0.062 ... Validation loss: 0.1490.06554213627594764\n",
            "0.15750239379821848\n",
            "Progress: 65.0% ... Training loss: 0.065 ... Validation loss: 0.1570.061868921753105155\n",
            "0.15943380467605955\n",
            "Progress: 65.0% ... Training loss: 0.061 ... Validation loss: 0.1590.060234664316707376\n",
            "0.16875721062486496\n",
            "Progress: 65.0% ... Training loss: 0.060 ... Validation loss: 0.1680.06793081929046361\n",
            "0.15394065920257896\n",
            "Progress: 65.0% ... Training loss: 0.067 ... Validation loss: 0.1530.0650566473934248\n",
            "0.21166398505674552\n",
            "Progress: 65.0% ... Training loss: 0.065 ... Validation loss: 0.2110.061991858640377116\n",
            "0.16935484904247153\n",
            "Progress: 65.1% ... Training loss: 0.061 ... Validation loss: 0.1690.06579421795465551\n",
            "0.1645089960522263\n",
            "Progress: 65.1% ... Training loss: 0.065 ... Validation loss: 0.1640.06043965458446443\n",
            "0.15929441051047205\n",
            "Progress: 65.1% ... Training loss: 0.060 ... Validation loss: 0.1590.06294633510222601\n",
            "0.1651904725084433\n",
            "Progress: 65.1% ... Training loss: 0.062 ... Validation loss: 0.1650.062084513833061926\n",
            "0.1629221820496599\n",
            "Progress: 65.1% ... Training loss: 0.062 ... Validation loss: 0.1620.06262804712390357\n",
            "0.1568725725634316\n",
            "Progress: 65.2% ... Training loss: 0.062 ... Validation loss: 0.1560.061439776985829336\n",
            "0.16144778457893222\n",
            "Progress: 65.2% ... Training loss: 0.061 ... Validation loss: 0.1610.06141377374125698\n",
            "0.16047097609916225\n",
            "Progress: 65.2% ... Training loss: 0.061 ... Validation loss: 0.1600.06230618001352743\n",
            "0.16986439934005254\n",
            "Progress: 65.2% ... Training loss: 0.062 ... Validation loss: 0.1690.06046930998802934\n",
            "0.1661456242552121\n",
            "Progress: 65.2% ... Training loss: 0.060 ... Validation loss: 0.1660.07024273781283304\n",
            "0.1762595988641814\n",
            "Progress: 65.3% ... Training loss: 0.070 ... Validation loss: 0.1760.06105433342696028\n",
            "0.16265572849967924\n",
            "Progress: 65.3% ... Training loss: 0.061 ... Validation loss: 0.1620.06226992476042716\n",
            "0.17029889975498103\n",
            "Progress: 65.3% ... Training loss: 0.062 ... Validation loss: 0.1700.06354803956011557\n",
            "0.16985244304377156\n",
            "Progress: 65.3% ... Training loss: 0.063 ... Validation loss: 0.1690.07138118688650615\n",
            "0.1903740538964911\n",
            "Progress: 65.3% ... Training loss: 0.071 ... Validation loss: 0.1900.06104971431988044\n",
            "0.16055820448197175\n",
            "Progress: 65.4% ... Training loss: 0.061 ... Validation loss: 0.1600.060165926955835274\n",
            "0.15845313975277853\n",
            "Progress: 65.4% ... Training loss: 0.060 ... Validation loss: 0.1580.06087470968895152\n",
            "0.1669237815381045\n",
            "Progress: 65.4% ... Training loss: 0.060 ... Validation loss: 0.1660.06905763099696638\n",
            "0.15079660401889058\n",
            "Progress: 65.4% ... Training loss: 0.069 ... Validation loss: 0.1500.07313160318079755\n",
            "0.14710121943216675\n",
            "Progress: 65.4% ... Training loss: 0.073 ... Validation loss: 0.1470.06761505034468102\n",
            "0.18975807750127527\n",
            "Progress: 65.5% ... Training loss: 0.067 ... Validation loss: 0.1890.0664406502270577\n",
            "0.18275365417414452\n",
            "Progress: 65.5% ... Training loss: 0.066 ... Validation loss: 0.1820.06347530784793504\n",
            "0.15586199967237674\n",
            "Progress: 65.5% ... Training loss: 0.063 ... Validation loss: 0.1550.06237527610590027\n",
            "0.17610488060389312\n",
            "Progress: 65.5% ... Training loss: 0.062 ... Validation loss: 0.1760.060868146421088246\n",
            "0.1612730298095721\n",
            "Progress: 65.5% ... Training loss: 0.060 ... Validation loss: 0.1610.06568329622420603\n",
            "0.16545239174957554\n",
            "Progress: 65.6% ... Training loss: 0.065 ... Validation loss: 0.1650.07164602642047066\n",
            "0.15309807845066645\n",
            "Progress: 65.6% ... Training loss: 0.071 ... Validation loss: 0.1530.0678440096835294\n",
            "0.15173150107426628\n",
            "Progress: 65.6% ... Training loss: 0.067 ... Validation loss: 0.1510.06547780459610929\n",
            "0.17069117582908028\n",
            "Progress: 65.6% ... Training loss: 0.065 ... Validation loss: 0.1700.06125434484574897\n",
            "0.16719227284359045\n",
            "Progress: 65.6% ... Training loss: 0.061 ... Validation loss: 0.1670.06225332551592483\n",
            "0.1669370038030031\n",
            "Progress: 65.7% ... Training loss: 0.062 ... Validation loss: 0.1660.06499989246005558\n",
            "0.16913557620489075\n",
            "Progress: 65.7% ... Training loss: 0.064 ... Validation loss: 0.1690.06943211381072238\n",
            "0.13867344172101662\n",
            "Progress: 65.7% ... Training loss: 0.069 ... Validation loss: 0.1380.060676594355340695\n",
            "0.1493729062949488\n",
            "Progress: 65.7% ... Training loss: 0.060 ... Validation loss: 0.1490.061330844543527364\n",
            "0.15103224259698023\n",
            "Progress: 65.7% ... Training loss: 0.061 ... Validation loss: 0.1510.062333966447864805\n",
            "0.17136333839255583\n",
            "Progress: 65.8% ... Training loss: 0.062 ... Validation loss: 0.1710.069589446358612\n",
            "0.1420783270523283\n",
            "Progress: 65.8% ... Training loss: 0.069 ... Validation loss: 0.1420.06327599152910436\n",
            "0.17379278681507004\n",
            "Progress: 65.8% ... Training loss: 0.063 ... Validation loss: 0.1730.06175519814368941\n",
            "0.14613061691790663\n",
            "Progress: 65.8% ... Training loss: 0.061 ... Validation loss: 0.1460.06566977824474193\n",
            "0.17534526999522956\n",
            "Progress: 65.8% ... Training loss: 0.065 ... Validation loss: 0.1750.06100716577774164\n",
            "0.15773442192191137\n",
            "Progress: 65.9% ... Training loss: 0.061 ... Validation loss: 0.1570.059618587108201046\n",
            "0.16127018144054514\n",
            "Progress: 65.9% ... Training loss: 0.059 ... Validation loss: 0.1610.06086144866615307\n",
            "0.16915721229380884\n",
            "Progress: 65.9% ... Training loss: 0.060 ... Validation loss: 0.1690.06029406206302059\n",
            "0.14973299951353297\n",
            "Progress: 65.9% ... Training loss: 0.060 ... Validation loss: 0.1490.0653767393921851\n",
            "0.13824476374754202\n",
            "Progress: 65.9% ... Training loss: 0.065 ... Validation loss: 0.1380.06498711915515623\n",
            "0.17095029176293144\n",
            "Progress: 66.0% ... Training loss: 0.064 ... Validation loss: 0.1700.06138582845143739\n",
            "0.17934283383792357\n",
            "Progress: 66.0% ... Training loss: 0.061 ... Validation loss: 0.1790.06069778695873505\n",
            "0.16762869509436965\n",
            "Progress: 66.0% ... Training loss: 0.060 ... Validation loss: 0.1670.05976015230802792\n",
            "0.1622098739484364\n",
            "Progress: 66.0% ... Training loss: 0.059 ... Validation loss: 0.1620.0604515074328354\n",
            "0.16004650399845505\n",
            "Progress: 66.0% ... Training loss: 0.060 ... Validation loss: 0.1600.0630789117591012\n",
            "0.19837971605299415\n",
            "Progress: 66.1% ... Training loss: 0.063 ... Validation loss: 0.1980.06578727820315679\n",
            "0.150582800923314\n",
            "Progress: 66.1% ... Training loss: 0.065 ... Validation loss: 0.1500.06339696239342978\n",
            "0.18361025750714757\n",
            "Progress: 66.1% ... Training loss: 0.063 ... Validation loss: 0.1830.077718155634957\n",
            "0.21073694638030238\n",
            "Progress: 66.1% ... Training loss: 0.077 ... Validation loss: 0.2100.061461842842756395\n",
            "0.1483174862367251\n",
            "Progress: 66.1% ... Training loss: 0.061 ... Validation loss: 0.1480.07001849225950726\n",
            "0.17332388615821934\n",
            "Progress: 66.2% ... Training loss: 0.070 ... Validation loss: 0.1730.06582003079287203\n",
            "0.1794444426542583\n",
            "Progress: 66.2% ... Training loss: 0.065 ... Validation loss: 0.1790.07304721732951436\n",
            "0.157608568012941\n",
            "Progress: 66.2% ... Training loss: 0.073 ... Validation loss: 0.1570.06409711122935145\n",
            "0.1759110623310063\n",
            "Progress: 66.2% ... Training loss: 0.064 ... Validation loss: 0.1750.06672053194436357\n",
            "0.1522465166668108\n",
            "Progress: 66.2% ... Training loss: 0.066 ... Validation loss: 0.1520.06455739631495441\n",
            "0.1480755149993892\n",
            "Progress: 66.3% ... Training loss: 0.064 ... Validation loss: 0.1480.06111504262993567\n",
            "0.1657976482934388\n",
            "Progress: 66.3% ... Training loss: 0.061 ... Validation loss: 0.1650.05994958673893228\n",
            "0.17234613651243935\n",
            "Progress: 66.3% ... Training loss: 0.059 ... Validation loss: 0.1720.06649945782486635\n",
            "0.188553525616874\n",
            "Progress: 66.3% ... Training loss: 0.066 ... Validation loss: 0.1880.06870342209419554\n",
            "0.21304004003925167\n",
            "Progress: 66.3% ... Training loss: 0.068 ... Validation loss: 0.2130.06052871222297457\n",
            "0.15740354961617686\n",
            "Progress: 66.4% ... Training loss: 0.060 ... Validation loss: 0.1570.06388034591677778\n",
            "0.14542347269960076\n",
            "Progress: 66.4% ... Training loss: 0.063 ... Validation loss: 0.1450.059920235961795534\n",
            "0.16523502628579342\n",
            "Progress: 66.4% ... Training loss: 0.059 ... Validation loss: 0.1650.07217601268987787\n",
            "0.1891933558141063\n",
            "Progress: 66.4% ... Training loss: 0.072 ... Validation loss: 0.1890.06088843377391528\n",
            "0.16071753978972084\n",
            "Progress: 66.4% ... Training loss: 0.060 ... Validation loss: 0.1600.06927184876124762\n",
            "0.2001400405823667\n",
            "Progress: 66.5% ... Training loss: 0.069 ... Validation loss: 0.2000.07462488248995054\n",
            "0.22170554559762273\n",
            "Progress: 66.5% ... Training loss: 0.074 ... Validation loss: 0.2210.06169604919842688\n",
            "0.18420905031391163\n",
            "Progress: 66.5% ... Training loss: 0.061 ... Validation loss: 0.1840.06080426609452117\n",
            "0.15825661296979407\n",
            "Progress: 66.5% ... Training loss: 0.060 ... Validation loss: 0.1580.06368394967180298\n",
            "0.15713163907470495\n",
            "Progress: 66.5% ... Training loss: 0.063 ... Validation loss: 0.1570.06533182652534252\n",
            "0.1493929885280529\n",
            "Progress: 66.6% ... Training loss: 0.065 ... Validation loss: 0.1490.05964608105008454\n",
            "0.16539878152305373\n",
            "Progress: 66.6% ... Training loss: 0.059 ... Validation loss: 0.1650.06537508668960593\n",
            "0.18481105025779992\n",
            "Progress: 66.6% ... Training loss: 0.065 ... Validation loss: 0.1840.0605085988448214\n",
            "0.15335073658388895\n",
            "Progress: 66.6% ... Training loss: 0.060 ... Validation loss: 0.1530.060282361146499853\n",
            "0.14757929341643305\n",
            "Progress: 66.6% ... Training loss: 0.060 ... Validation loss: 0.1470.062458368271797446\n",
            "0.16032879655925217\n",
            "Progress: 66.7% ... Training loss: 0.062 ... Validation loss: 0.1600.0654735102176805\n",
            "0.18780030815026552\n",
            "Progress: 66.7% ... Training loss: 0.065 ... Validation loss: 0.1870.06023682248228737\n",
            "0.15454633710193866\n",
            "Progress: 66.7% ... Training loss: 0.060 ... Validation loss: 0.1540.06000228758069128\n",
            "0.14880626596322313\n",
            "Progress: 66.7% ... Training loss: 0.060 ... Validation loss: 0.1480.061265006452676844\n",
            "0.16869479271700688\n",
            "Progress: 66.7% ... Training loss: 0.061 ... Validation loss: 0.1680.05982823078728253\n",
            "0.14879913315379142\n",
            "Progress: 66.8% ... Training loss: 0.059 ... Validation loss: 0.1480.060657288460339936\n",
            "0.1505856712625036\n",
            "Progress: 66.8% ... Training loss: 0.060 ... Validation loss: 0.1500.06024511267318743\n",
            "0.1557957398725495\n",
            "Progress: 66.8% ... Training loss: 0.060 ... Validation loss: 0.1550.06620721753304054\n",
            "0.1498558807311777\n",
            "Progress: 66.8% ... Training loss: 0.066 ... Validation loss: 0.1490.06080861285042346\n",
            "0.1641028449089422\n",
            "Progress: 66.8% ... Training loss: 0.060 ... Validation loss: 0.1640.06746211051311472\n",
            "0.19362586203017706\n",
            "Progress: 66.9% ... Training loss: 0.067 ... Validation loss: 0.1930.06625271799813344\n",
            "0.14841215366782334\n",
            "Progress: 66.9% ... Training loss: 0.066 ... Validation loss: 0.1480.060440169496079645\n",
            "0.1523068165163123\n",
            "Progress: 66.9% ... Training loss: 0.060 ... Validation loss: 0.1520.06618733745856036\n",
            "0.17819365691162087\n",
            "Progress: 66.9% ... Training loss: 0.066 ... Validation loss: 0.1780.06223997175483098\n",
            "0.14735446018340753\n",
            "Progress: 66.9% ... Training loss: 0.062 ... Validation loss: 0.1470.06138359854342276\n",
            "0.15121662573309946\n",
            "Progress: 67.0% ... Training loss: 0.061 ... Validation loss: 0.1510.06395319637052893\n",
            "0.1711047904470169\n",
            "Progress: 67.0% ... Training loss: 0.063 ... Validation loss: 0.1710.060784738316267746\n",
            "0.15937841547401724\n",
            "Progress: 67.0% ... Training loss: 0.060 ... Validation loss: 0.1590.06020882056755104\n",
            "0.15568531972856373\n",
            "Progress: 67.0% ... Training loss: 0.060 ... Validation loss: 0.1550.06112582674373052\n",
            "0.16277069234673694\n",
            "Progress: 67.0% ... Training loss: 0.061 ... Validation loss: 0.1620.060756413146776066\n",
            "0.16502650948820366\n",
            "Progress: 67.1% ... Training loss: 0.060 ... Validation loss: 0.1650.06022006236943601\n",
            "0.1546590847130317\n",
            "Progress: 67.1% ... Training loss: 0.060 ... Validation loss: 0.1540.06974745018168929\n",
            "0.199864949876793\n",
            "Progress: 67.1% ... Training loss: 0.069 ... Validation loss: 0.1990.06238833430838146\n",
            "0.1704067718690071\n",
            "Progress: 67.1% ... Training loss: 0.062 ... Validation loss: 0.1700.060361394444181954\n",
            "0.15540342501622792\n",
            "Progress: 67.1% ... Training loss: 0.060 ... Validation loss: 0.1550.06094820290975709\n",
            "0.1536390169875566\n",
            "Progress: 67.2% ... Training loss: 0.060 ... Validation loss: 0.1530.0649354367769989\n",
            "0.15429608540428266\n",
            "Progress: 67.2% ... Training loss: 0.064 ... Validation loss: 0.1540.06156977895330584\n",
            "0.16932469381748894\n",
            "Progress: 67.2% ... Training loss: 0.061 ... Validation loss: 0.1690.06092709219057037\n",
            "0.16191562398390724\n",
            "Progress: 67.2% ... Training loss: 0.060 ... Validation loss: 0.1610.06145520074835538\n",
            "0.14811327326003254\n",
            "Progress: 67.2% ... Training loss: 0.061 ... Validation loss: 0.1480.06001340127127199\n",
            "0.14473779137997836\n",
            "Progress: 67.3% ... Training loss: 0.060 ... Validation loss: 0.1440.0629778179757478\n",
            "0.14467877407628674\n",
            "Progress: 67.3% ... Training loss: 0.062 ... Validation loss: 0.1440.05963399324211729\n",
            "0.16175339404521547\n",
            "Progress: 67.3% ... Training loss: 0.059 ... Validation loss: 0.1610.06156595740064221\n",
            "0.17031714600753592\n",
            "Progress: 67.3% ... Training loss: 0.061 ... Validation loss: 0.1700.060355583764068445\n",
            "0.1558533799489541\n",
            "Progress: 67.3% ... Training loss: 0.060 ... Validation loss: 0.1550.07034405707885825\n",
            "0.18810326539526412\n",
            "Progress: 67.4% ... Training loss: 0.070 ... Validation loss: 0.1880.07285823517085938\n",
            "0.1552469394519797\n",
            "Progress: 67.4% ... Training loss: 0.072 ... Validation loss: 0.1550.07037691089392671\n",
            "0.19598573554357188\n",
            "Progress: 67.4% ... Training loss: 0.070 ... Validation loss: 0.1950.06548981742330548\n",
            "0.1420931558975927\n",
            "Progress: 67.4% ... Training loss: 0.065 ... Validation loss: 0.1420.060698663990250895\n",
            "0.15450924280092157\n",
            "Progress: 67.4% ... Training loss: 0.060 ... Validation loss: 0.1540.06429677123945574\n",
            "0.16615745003432147\n",
            "Progress: 67.5% ... Training loss: 0.064 ... Validation loss: 0.1660.06406260759817393\n",
            "0.1546176175569407\n",
            "Progress: 67.5% ... Training loss: 0.064 ... Validation loss: 0.1540.0626767234321596\n",
            "0.15677258856194223\n",
            "Progress: 67.5% ... Training loss: 0.062 ... Validation loss: 0.1560.06152532414644916\n",
            "0.15617915326918055\n",
            "Progress: 67.5% ... Training loss: 0.061 ... Validation loss: 0.1560.06092689967186825\n",
            "0.16695696765195361\n",
            "Progress: 67.5% ... Training loss: 0.060 ... Validation loss: 0.1660.06114448983877665\n",
            "0.1554390628673896\n",
            "Progress: 67.6% ... Training loss: 0.061 ... Validation loss: 0.1550.06248875460692298\n",
            "0.16141799088076694\n",
            "Progress: 67.6% ... Training loss: 0.062 ... Validation loss: 0.1610.06169040520632465\n",
            "0.14936784925740593\n",
            "Progress: 67.6% ... Training loss: 0.061 ... Validation loss: 0.1490.061478211852998774\n",
            "0.15386009265549916\n",
            "Progress: 67.6% ... Training loss: 0.061 ... Validation loss: 0.1530.05990252545487034\n",
            "0.15688115864767166\n",
            "Progress: 67.6% ... Training loss: 0.059 ... Validation loss: 0.1560.06357083265025633\n",
            "0.15056949284451562\n",
            "Progress: 67.7% ... Training loss: 0.063 ... Validation loss: 0.1500.06200618881104136\n",
            "0.15457001018719524\n",
            "Progress: 67.7% ... Training loss: 0.062 ... Validation loss: 0.1540.0631267632913321\n",
            "0.17887022022084384\n",
            "Progress: 67.7% ... Training loss: 0.063 ... Validation loss: 0.1780.06633920393599105\n",
            "0.176766516995627\n",
            "Progress: 67.7% ... Training loss: 0.066 ... Validation loss: 0.1760.06242577489967966\n",
            "0.17133488621640405\n",
            "Progress: 67.7% ... Training loss: 0.062 ... Validation loss: 0.1710.06356172900294245\n",
            "0.18478468148036875\n",
            "Progress: 67.8% ... Training loss: 0.063 ... Validation loss: 0.1840.06097872387357949\n",
            "0.16221934180509676\n",
            "Progress: 67.8% ... Training loss: 0.060 ... Validation loss: 0.1620.08482130964266198\n",
            "0.2386373378678972\n",
            "Progress: 67.8% ... Training loss: 0.084 ... Validation loss: 0.2380.07591259522146358\n",
            "0.21391797144601593\n",
            "Progress: 67.8% ... Training loss: 0.075 ... Validation loss: 0.2130.0623682122006835\n",
            "0.17076046994372354\n",
            "Progress: 67.8% ... Training loss: 0.062 ... Validation loss: 0.1700.06415929008120912\n",
            "0.18949952194078523\n",
            "Progress: 67.9% ... Training loss: 0.064 ... Validation loss: 0.1890.06252647575082182\n",
            "0.17112978522944794\n",
            "Progress: 67.9% ... Training loss: 0.062 ... Validation loss: 0.1710.06366462250001849\n",
            "0.1587226635495579\n",
            "Progress: 67.9% ... Training loss: 0.063 ... Validation loss: 0.1580.06376219534536255\n",
            "0.15294059862785975\n",
            "Progress: 67.9% ... Training loss: 0.063 ... Validation loss: 0.1520.06298534556179415\n",
            "0.17428227722073944\n",
            "Progress: 67.9% ... Training loss: 0.062 ... Validation loss: 0.1740.06591690166898753\n",
            "0.15733186958137324\n",
            "Progress: 68.0% ... Training loss: 0.065 ... Validation loss: 0.1570.061567645367265465\n",
            "0.18149766936066938\n",
            "Progress: 68.0% ... Training loss: 0.061 ... Validation loss: 0.1810.06382720299292792\n",
            "0.15209655219623977\n",
            "Progress: 68.0% ... Training loss: 0.063 ... Validation loss: 0.1520.062014324383378706\n",
            "0.19116190957462295\n",
            "Progress: 68.0% ... Training loss: 0.062 ... Validation loss: 0.1910.06547609286864609\n",
            "0.18333029018103014\n",
            "Progress: 68.0% ... Training loss: 0.065 ... Validation loss: 0.1830.06595349858262967\n",
            "0.15391641865503408\n",
            "Progress: 68.1% ... Training loss: 0.065 ... Validation loss: 0.1530.06363855198629277\n",
            "0.1856136377486856\n",
            "Progress: 68.1% ... Training loss: 0.063 ... Validation loss: 0.1850.06466473090667291\n",
            "0.14066950738632986\n",
            "Progress: 68.1% ... Training loss: 0.064 ... Validation loss: 0.1400.06370317323784998\n",
            "0.18911757696784456\n",
            "Progress: 68.1% ... Training loss: 0.063 ... Validation loss: 0.1890.06399526761826158\n",
            "0.1801551969946993\n",
            "Progress: 68.1% ... Training loss: 0.063 ... Validation loss: 0.1800.06427979190526704\n",
            "0.18145621284352675\n",
            "Progress: 68.2% ... Training loss: 0.064 ... Validation loss: 0.1810.06073666159848374\n",
            "0.15171034370129896\n",
            "Progress: 68.2% ... Training loss: 0.060 ... Validation loss: 0.1510.05934597709723292\n",
            "0.15943769940001487\n",
            "Progress: 68.2% ... Training loss: 0.059 ... Validation loss: 0.1590.05927819208072501\n",
            "0.1672552962683143\n",
            "Progress: 68.2% ... Training loss: 0.059 ... Validation loss: 0.1670.06853235474255825\n",
            "0.19390684062026123\n",
            "Progress: 68.2% ... Training loss: 0.068 ... Validation loss: 0.1930.06325474987789373\n",
            "0.15153448462160446\n",
            "Progress: 68.3% ... Training loss: 0.063 ... Validation loss: 0.1510.06454778159018133\n",
            "0.15343030728641363\n",
            "Progress: 68.3% ... Training loss: 0.064 ... Validation loss: 0.1530.06870422813645444\n",
            "0.15508436681900284\n",
            "Progress: 68.3% ... Training loss: 0.068 ... Validation loss: 0.1550.066037855480848\n",
            "0.18985689982100504\n",
            "Progress: 68.3% ... Training loss: 0.066 ... Validation loss: 0.1890.060001510293636526\n",
            "0.15096192328250344\n",
            "Progress: 68.3% ... Training loss: 0.060 ... Validation loss: 0.1500.06113466733478096\n",
            "0.14524819348719678\n",
            "Progress: 68.4% ... Training loss: 0.061 ... Validation loss: 0.1450.0634246858712865\n",
            "0.15467484161019895\n",
            "Progress: 68.4% ... Training loss: 0.063 ... Validation loss: 0.1540.06128352484990985\n",
            "0.16233248182647786\n",
            "Progress: 68.4% ... Training loss: 0.061 ... Validation loss: 0.1620.05965412507139435\n",
            "0.16592316245797434\n",
            "Progress: 68.4% ... Training loss: 0.059 ... Validation loss: 0.1650.06057782687383839\n",
            "0.17343690784954374\n",
            "Progress: 68.4% ... Training loss: 0.060 ... Validation loss: 0.1730.06354938532820573\n",
            "0.1804795048472065\n",
            "Progress: 68.5% ... Training loss: 0.063 ... Validation loss: 0.1800.061077661487153184\n",
            "0.16186084405422016\n",
            "Progress: 68.5% ... Training loss: 0.061 ... Validation loss: 0.1610.06117737504758936\n",
            "0.14545435200314166\n",
            "Progress: 68.5% ... Training loss: 0.061 ... Validation loss: 0.1450.06643430781214671\n",
            "0.14531907851454945\n",
            "Progress: 68.5% ... Training loss: 0.066 ... Validation loss: 0.1450.06068429613957018\n",
            "0.1711178441575104\n",
            "Progress: 68.5% ... Training loss: 0.060 ... Validation loss: 0.1710.06286178715526893\n",
            "0.19609181888921046\n",
            "Progress: 68.6% ... Training loss: 0.062 ... Validation loss: 0.1960.06482744107549808\n",
            "0.20114942547206652\n",
            "Progress: 68.6% ... Training loss: 0.064 ... Validation loss: 0.2010.059645087527573255\n",
            "0.16687388358869445\n",
            "Progress: 68.6% ... Training loss: 0.059 ... Validation loss: 0.1660.05978566706315394\n",
            "0.16083505262445522\n",
            "Progress: 68.6% ... Training loss: 0.059 ... Validation loss: 0.1600.06131225261128725\n",
            "0.14936766404495122\n",
            "Progress: 68.6% ... Training loss: 0.061 ... Validation loss: 0.1490.06060720624738001\n",
            "0.15333067512060056\n",
            "Progress: 68.7% ... Training loss: 0.060 ... Validation loss: 0.1530.062232890497284005\n",
            "0.1721018603718152\n",
            "Progress: 68.7% ... Training loss: 0.062 ... Validation loss: 0.1720.06015843100089831\n",
            "0.15822355245057923\n",
            "Progress: 68.7% ... Training loss: 0.060 ... Validation loss: 0.1580.058873555993555815\n",
            "0.14841823069906404\n",
            "Progress: 68.7% ... Training loss: 0.058 ... Validation loss: 0.1480.061265725404433875\n",
            "0.16305184728522704\n",
            "Progress: 68.7% ... Training loss: 0.061 ... Validation loss: 0.1630.060170165062593414\n",
            "0.15684612908658085\n",
            "Progress: 68.8% ... Training loss: 0.060 ... Validation loss: 0.1560.060871262433342684\n",
            "0.1562560155024611\n",
            "Progress: 68.8% ... Training loss: 0.060 ... Validation loss: 0.1560.08537590606530625\n",
            "0.15153997266798877\n",
            "Progress: 68.8% ... Training loss: 0.085 ... Validation loss: 0.1510.06882866709362787\n",
            "0.1806050304647065\n",
            "Progress: 68.8% ... Training loss: 0.068 ... Validation loss: 0.1800.05994213953876488\n",
            "0.15965714132151926\n",
            "Progress: 68.8% ... Training loss: 0.059 ... Validation loss: 0.1590.060404412552485194\n",
            "0.16394154745796363\n",
            "Progress: 68.9% ... Training loss: 0.060 ... Validation loss: 0.1630.06037731183436946\n",
            "0.15625650872923316\n",
            "Progress: 68.9% ... Training loss: 0.060 ... Validation loss: 0.1560.07327893616257913\n",
            "0.1943576277286984\n",
            "Progress: 68.9% ... Training loss: 0.073 ... Validation loss: 0.1940.06262567213589162\n",
            "0.15980002694933454\n",
            "Progress: 68.9% ... Training loss: 0.062 ... Validation loss: 0.1590.05981442687500302\n",
            "0.16273306431086462\n",
            "Progress: 68.9% ... Training loss: 0.059 ... Validation loss: 0.1620.06162737048132033\n",
            "0.16792691775189109\n",
            "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.1670.06022461134096085\n",
            "0.1527287453102664\n",
            "Progress: 69.0% ... Training loss: 0.060 ... Validation loss: 0.1520.05932278938516686\n",
            "0.15163117538004783\n",
            "Progress: 69.0% ... Training loss: 0.059 ... Validation loss: 0.1510.06192353849361096\n",
            "0.15414919451909007\n",
            "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.1540.061505473697552174\n",
            "0.16620860700377355\n",
            "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.1660.06748999981701981\n",
            "0.13858070605945558\n",
            "Progress: 69.1% ... Training loss: 0.067 ... Validation loss: 0.1380.06666384557157805\n",
            "0.1616335502444456\n",
            "Progress: 69.1% ... Training loss: 0.066 ... Validation loss: 0.1610.05938851988504736\n",
            "0.15870884829281146\n",
            "Progress: 69.1% ... Training loss: 0.059 ... Validation loss: 0.1580.07015979163363797\n",
            "0.18948103238456093\n",
            "Progress: 69.1% ... Training loss: 0.070 ... Validation loss: 0.1890.06079328731537749\n",
            "0.16910082155919867\n",
            "Progress: 69.1% ... Training loss: 0.060 ... Validation loss: 0.1690.06614185101850593\n",
            "0.14992836884806582\n",
            "Progress: 69.2% ... Training loss: 0.066 ... Validation loss: 0.1490.06136641098094355\n",
            "0.16079576306440146\n",
            "Progress: 69.2% ... Training loss: 0.061 ... Validation loss: 0.1600.062431184194026684\n",
            "0.1557384858544639\n",
            "Progress: 69.2% ... Training loss: 0.062 ... Validation loss: 0.1550.06725590650770283\n",
            "0.16438157790589233\n",
            "Progress: 69.2% ... Training loss: 0.067 ... Validation loss: 0.1640.06121600700922673\n",
            "0.15368531755990486\n",
            "Progress: 69.2% ... Training loss: 0.061 ... Validation loss: 0.1530.07361390897790453\n",
            "0.15174105115203945\n",
            "Progress: 69.3% ... Training loss: 0.073 ... Validation loss: 0.1510.06873022993682897\n",
            "0.15761457955124258\n",
            "Progress: 69.3% ... Training loss: 0.068 ... Validation loss: 0.1570.06562541129596158\n",
            "0.15199744777433244\n",
            "Progress: 69.3% ... Training loss: 0.065 ... Validation loss: 0.1510.06077224323154931\n",
            "0.15368033552069263\n",
            "Progress: 69.3% ... Training loss: 0.060 ... Validation loss: 0.1530.06132274748859046\n",
            "0.1700049300795325\n",
            "Progress: 69.3% ... Training loss: 0.061 ... Validation loss: 0.1700.062024528001967345\n",
            "0.15058968334668468\n",
            "Progress: 69.4% ... Training loss: 0.062 ... Validation loss: 0.1500.07007148927529087\n",
            "0.18621138648029437\n",
            "Progress: 69.4% ... Training loss: 0.070 ... Validation loss: 0.1860.05930512764980388\n",
            "0.1598507458313443\n",
            "Progress: 69.4% ... Training loss: 0.059 ... Validation loss: 0.1590.061529746000991176\n",
            "0.15751456005513179\n",
            "Progress: 69.4% ... Training loss: 0.061 ... Validation loss: 0.1570.06344405022072637\n",
            "0.14410771588948823\n",
            "Progress: 69.4% ... Training loss: 0.063 ... Validation loss: 0.1440.06190731968246705\n",
            "0.15239613930120857\n",
            "Progress: 69.5% ... Training loss: 0.061 ... Validation loss: 0.1520.0641894289683399\n",
            "0.14507210752862998\n",
            "Progress: 69.5% ... Training loss: 0.064 ... Validation loss: 0.1450.060165946638248195\n",
            "0.14653235873852877\n",
            "Progress: 69.5% ... Training loss: 0.060 ... Validation loss: 0.1460.06310171349905024\n",
            "0.15197925629425693\n",
            "Progress: 69.5% ... Training loss: 0.063 ... Validation loss: 0.1510.05987578983850675\n",
            "0.1510903344245591\n",
            "Progress: 69.5% ... Training loss: 0.059 ... Validation loss: 0.1510.06085612490013534\n",
            "0.1638244473674108\n",
            "Progress: 69.6% ... Training loss: 0.060 ... Validation loss: 0.1630.06236978887080285\n",
            "0.16736435497225224\n",
            "Progress: 69.6% ... Training loss: 0.062 ... Validation loss: 0.1670.06506068479272656\n",
            "0.1591576831979124\n",
            "Progress: 69.6% ... Training loss: 0.065 ... Validation loss: 0.1590.06636457145644573\n",
            "0.18603948208912102\n",
            "Progress: 69.6% ... Training loss: 0.066 ... Validation loss: 0.1860.06750778101528317\n",
            "0.18408201051399264\n",
            "Progress: 69.6% ... Training loss: 0.067 ... Validation loss: 0.1840.060303792054811005\n",
            "0.15647703457359224\n",
            "Progress: 69.7% ... Training loss: 0.060 ... Validation loss: 0.1560.060610459866664024\n",
            "0.14726457206544205\n",
            "Progress: 69.7% ... Training loss: 0.060 ... Validation loss: 0.1470.059423204478751455\n",
            "0.16018396330105195\n",
            "Progress: 69.7% ... Training loss: 0.059 ... Validation loss: 0.1600.06128542923769966\n",
            "0.15014185851278314\n",
            "Progress: 69.7% ... Training loss: 0.061 ... Validation loss: 0.1500.07081242967981653\n",
            "0.20151154308255886\n",
            "Progress: 69.7% ... Training loss: 0.070 ... Validation loss: 0.2010.06070140836378309\n",
            "0.14766301765652412\n",
            "Progress: 69.8% ... Training loss: 0.060 ... Validation loss: 0.1470.05938110174144723\n",
            "0.15293031403185994\n",
            "Progress: 69.8% ... Training loss: 0.059 ... Validation loss: 0.1520.059846788696834456\n",
            "0.1620596463540739\n",
            "Progress: 69.8% ... Training loss: 0.059 ... Validation loss: 0.1620.05985437096525426\n",
            "0.16245437173026778\n",
            "Progress: 69.8% ... Training loss: 0.059 ... Validation loss: 0.1620.059408804483755136\n",
            "0.160618182742737\n",
            "Progress: 69.8% ... Training loss: 0.059 ... Validation loss: 0.1600.0598507872014524\n",
            "0.16606679235990227\n",
            "Progress: 69.9% ... Training loss: 0.059 ... Validation loss: 0.1660.061279714465423546\n",
            "0.1778749079004455\n",
            "Progress: 69.9% ... Training loss: 0.061 ... Validation loss: 0.1770.05972759272516474\n",
            "0.15322635813560456\n",
            "Progress: 69.9% ... Training loss: 0.059 ... Validation loss: 0.1530.06330839423907932\n",
            "0.14237149424493398\n",
            "Progress: 69.9% ... Training loss: 0.063 ... Validation loss: 0.1420.059883193667637495\n",
            "0.15258289458026514\n",
            "Progress: 69.9% ... Training loss: 0.059 ... Validation loss: 0.1520.059590334734101996\n",
            "0.1537918633483204\n",
            "Progress: 70.0% ... Training loss: 0.059 ... Validation loss: 0.1530.05886659369332619\n",
            "0.15628385898661865\n",
            "Progress: 70.0% ... Training loss: 0.058 ... Validation loss: 0.1560.06739156074413555\n",
            "0.1811142471081428\n",
            "Progress: 70.0% ... Training loss: 0.067 ... Validation loss: 0.1810.060491907846049506\n",
            "0.14550073854160997\n",
            "Progress: 70.0% ... Training loss: 0.060 ... Validation loss: 0.1450.06070780025631333\n",
            "0.1477603071694087\n",
            "Progress: 70.0% ... Training loss: 0.060 ... Validation loss: 0.1470.06162229174267487\n",
            "0.1477072681531647\n",
            "Progress: 70.1% ... Training loss: 0.061 ... Validation loss: 0.1470.060896401010118416\n",
            "0.15708529311793515\n",
            "Progress: 70.1% ... Training loss: 0.060 ... Validation loss: 0.1570.06086285481454376\n",
            "0.15480448802761737\n",
            "Progress: 70.1% ... Training loss: 0.060 ... Validation loss: 0.1540.0726348636998793\n",
            "0.1416153436572737\n",
            "Progress: 70.1% ... Training loss: 0.072 ... Validation loss: 0.1410.061808736968772\n",
            "0.14483787689872074\n",
            "Progress: 70.1% ... Training loss: 0.061 ... Validation loss: 0.1440.060343531600187725\n",
            "0.14506079351484533\n",
            "Progress: 70.2% ... Training loss: 0.060 ... Validation loss: 0.1450.06328156378914525\n",
            "0.15283750461457143\n",
            "Progress: 70.2% ... Training loss: 0.063 ... Validation loss: 0.1520.06216073806263446\n",
            "0.16465383466546654\n",
            "Progress: 70.2% ... Training loss: 0.062 ... Validation loss: 0.1640.06095768117487211\n",
            "0.1572762357462096\n",
            "Progress: 70.2% ... Training loss: 0.060 ... Validation loss: 0.1570.06376460587859585\n",
            "0.16025185281951873\n",
            "Progress: 70.2% ... Training loss: 0.063 ... Validation loss: 0.1600.06028393783111134\n",
            "0.15451218896585517\n",
            "Progress: 70.3% ... Training loss: 0.060 ... Validation loss: 0.1540.05923905126658658\n",
            "0.1515867948469857\n",
            "Progress: 70.3% ... Training loss: 0.059 ... Validation loss: 0.1510.06533140053787899\n",
            "0.16427379003276424\n",
            "Progress: 70.3% ... Training loss: 0.065 ... Validation loss: 0.1640.063347129068499\n",
            "0.16197020571313075\n",
            "Progress: 70.3% ... Training loss: 0.063 ... Validation loss: 0.1610.06778157212937774\n",
            "0.18926643948907723\n",
            "Progress: 70.3% ... Training loss: 0.067 ... Validation loss: 0.1890.06331299169668843\n",
            "0.14849806803214724\n",
            "Progress: 70.4% ... Training loss: 0.063 ... Validation loss: 0.1480.06241106964911255\n",
            "0.1705620175901804\n",
            "Progress: 70.4% ... Training loss: 0.062 ... Validation loss: 0.1700.05945694849295723\n",
            "0.15920399038164448\n",
            "Progress: 70.4% ... Training loss: 0.059 ... Validation loss: 0.1590.06734589263152861\n",
            "0.17143223734555085\n",
            "Progress: 70.4% ... Training loss: 0.067 ... Validation loss: 0.1710.06572470011513375\n",
            "0.14704467695936665\n",
            "Progress: 70.4% ... Training loss: 0.065 ... Validation loss: 0.1470.06771573582709038\n",
            "0.13676283817373805\n",
            "Progress: 70.5% ... Training loss: 0.067 ... Validation loss: 0.1360.060802173801398775\n",
            "0.1453832328997437\n",
            "Progress: 70.5% ... Training loss: 0.060 ... Validation loss: 0.1450.05990298933623163\n",
            "0.14359247660417948\n",
            "Progress: 70.5% ... Training loss: 0.059 ... Validation loss: 0.1430.06361185925893814\n",
            "0.14875117390752854\n",
            "Progress: 70.5% ... Training loss: 0.063 ... Validation loss: 0.1480.06066072824716285\n",
            "0.14850437989789844\n",
            "Progress: 70.5% ... Training loss: 0.060 ... Validation loss: 0.1480.06397322378394074\n",
            "0.15690032996363695\n",
            "Progress: 70.6% ... Training loss: 0.063 ... Validation loss: 0.1560.062318461426193184\n",
            "0.15005594126004354\n",
            "Progress: 70.6% ... Training loss: 0.062 ... Validation loss: 0.1500.06322325208709685\n",
            "0.1498758522840341\n",
            "Progress: 70.6% ... Training loss: 0.063 ... Validation loss: 0.1490.06427685559790104\n",
            "0.14812370088723525\n",
            "Progress: 70.6% ... Training loss: 0.064 ... Validation loss: 0.1480.06070744926639045\n",
            "0.16498273394532129\n",
            "Progress: 70.6% ... Training loss: 0.060 ... Validation loss: 0.1640.06441829652982714\n",
            "0.15118565442752668\n",
            "Progress: 70.7% ... Training loss: 0.064 ... Validation loss: 0.1510.05922653914916353\n",
            "0.15308926041809895\n",
            "Progress: 70.7% ... Training loss: 0.059 ... Validation loss: 0.1530.061062767708889666\n",
            "0.1761375199634599\n",
            "Progress: 70.7% ... Training loss: 0.061 ... Validation loss: 0.1760.06002076804132446\n",
            "0.15443458696073384\n",
            "Progress: 70.7% ... Training loss: 0.060 ... Validation loss: 0.1540.06847046093423206\n",
            "0.1863365927854713\n",
            "Progress: 70.7% ... Training loss: 0.068 ... Validation loss: 0.1860.06581956298808495\n",
            "0.13768099065659625\n",
            "Progress: 70.8% ... Training loss: 0.065 ... Validation loss: 0.1370.0624597283831438\n",
            "0.14358545641030293\n",
            "Progress: 70.8% ... Training loss: 0.062 ... Validation loss: 0.1430.07161322592995951\n",
            "0.2104035318890438\n",
            "Progress: 70.8% ... Training loss: 0.071 ... Validation loss: 0.2100.061415263785880164\n",
            "0.15587018564123029\n",
            "Progress: 70.8% ... Training loss: 0.061 ... Validation loss: 0.1550.06262278983083042\n",
            "0.13720344482476965\n",
            "Progress: 70.8% ... Training loss: 0.062 ... Validation loss: 0.1370.06359059686399318\n",
            "0.1414535813409415\n",
            "Progress: 70.9% ... Training loss: 0.063 ... Validation loss: 0.1410.06048039614923093\n",
            "0.14651573136274718\n",
            "Progress: 70.9% ... Training loss: 0.060 ... Validation loss: 0.1460.05919868137080057\n",
            "0.15285594139806957\n",
            "Progress: 70.9% ... Training loss: 0.059 ... Validation loss: 0.1520.06134888870970113\n",
            "0.1664364586874264\n",
            "Progress: 70.9% ... Training loss: 0.061 ... Validation loss: 0.1660.06927823635159208\n",
            "0.14905074463573326\n",
            "Progress: 70.9% ... Training loss: 0.069 ... Validation loss: 0.1490.059349394202721915\n",
            "0.14668159945937728\n",
            "Progress: 71.0% ... Training loss: 0.059 ... Validation loss: 0.1460.06654557933170195\n",
            "0.15194272940858364\n",
            "Progress: 71.0% ... Training loss: 0.066 ... Validation loss: 0.1510.07030325360595636\n",
            "0.15509094363285708\n",
            "Progress: 71.0% ... Training loss: 0.070 ... Validation loss: 0.1550.06536095714653854\n",
            "0.15420385189612426\n",
            "Progress: 71.0% ... Training loss: 0.065 ... Validation loss: 0.1540.061234243686745835\n",
            "0.17247209517861498\n",
            "Progress: 71.0% ... Training loss: 0.061 ... Validation loss: 0.1720.06205109696857816\n",
            "0.19008851522250328\n",
            "Progress: 71.1% ... Training loss: 0.062 ... Validation loss: 0.1900.0595247100498144\n",
            "0.16935146067977963\n",
            "Progress: 71.1% ... Training loss: 0.059 ... Validation loss: 0.1690.062356055657059865\n",
            "0.1852363379983967\n",
            "Progress: 71.1% ... Training loss: 0.062 ... Validation loss: 0.1850.059373660003315364\n",
            "0.15935205143198644\n",
            "Progress: 71.1% ... Training loss: 0.059 ... Validation loss: 0.1590.05854771859546724\n",
            "0.15705945716931619\n",
            "Progress: 71.1% ... Training loss: 0.058 ... Validation loss: 0.1570.06136529279938067\n",
            "0.17579636607939378\n",
            "Progress: 71.2% ... Training loss: 0.061 ... Validation loss: 0.1750.0616286266072651\n",
            "0.1649252610556276\n",
            "Progress: 71.2% ... Training loss: 0.061 ... Validation loss: 0.1640.06450097706445372\n",
            "0.16377702127881685\n",
            "Progress: 71.2% ... Training loss: 0.064 ... Validation loss: 0.1630.06035462230964723\n",
            "0.14631312643243147\n",
            "Progress: 71.2% ... Training loss: 0.060 ... Validation loss: 0.1460.0612118539957148\n",
            "0.14416373097815985\n",
            "Progress: 71.2% ... Training loss: 0.061 ... Validation loss: 0.1440.06260877831783088\n",
            "0.16928404771837352\n",
            "Progress: 71.3% ... Training loss: 0.062 ... Validation loss: 0.1690.05953433204618937\n",
            "0.14961394138410447\n",
            "Progress: 71.3% ... Training loss: 0.059 ... Validation loss: 0.1490.06044117652190838\n",
            "0.15099098230502764\n",
            "Progress: 71.3% ... Training loss: 0.060 ... Validation loss: 0.1500.0599940782658248\n",
            "0.15358867618944247\n",
            "Progress: 71.3% ... Training loss: 0.059 ... Validation loss: 0.1530.0625941932434969\n",
            "0.15330857355342703\n",
            "Progress: 71.3% ... Training loss: 0.062 ... Validation loss: 0.1530.06337542018904734\n",
            "0.14190119679012964\n",
            "Progress: 71.4% ... Training loss: 0.063 ... Validation loss: 0.1410.06115961669358467\n",
            "0.15932525904895045\n",
            "Progress: 71.4% ... Training loss: 0.061 ... Validation loss: 0.1590.060424423616468166\n",
            "0.15469463890418683\n",
            "Progress: 71.4% ... Training loss: 0.060 ... Validation loss: 0.1540.06417885989548198\n",
            "0.17124458222549013\n",
            "Progress: 71.4% ... Training loss: 0.064 ... Validation loss: 0.1710.05925115316528673\n",
            "0.15583445410392313\n",
            "Progress: 71.4% ... Training loss: 0.059 ... Validation loss: 0.1550.06071994466438127\n",
            "0.1548741367155722\n",
            "Progress: 71.5% ... Training loss: 0.060 ... Validation loss: 0.1540.05954939075308796\n",
            "0.1543255311094443\n",
            "Progress: 71.5% ... Training loss: 0.059 ... Validation loss: 0.1540.0623745158084421\n",
            "0.15943045047990034\n",
            "Progress: 71.5% ... Training loss: 0.062 ... Validation loss: 0.1590.06340563520723262\n",
            "0.15669525461392542\n",
            "Progress: 71.5% ... Training loss: 0.063 ... Validation loss: 0.1560.06817042555685116\n",
            "0.15900174499411363\n",
            "Progress: 71.5% ... Training loss: 0.068 ... Validation loss: 0.1590.059620491757770275\n",
            "0.16314818816383084\n",
            "Progress: 71.6% ... Training loss: 0.059 ... Validation loss: 0.1630.06001418550587\n",
            "0.16623439443323218\n",
            "Progress: 71.6% ... Training loss: 0.060 ... Validation loss: 0.1660.06221259393949827\n",
            "0.18661019604904017\n",
            "Progress: 71.6% ... Training loss: 0.062 ... Validation loss: 0.1860.060125800706270334\n",
            "0.16428295712954594\n",
            "Progress: 71.6% ... Training loss: 0.060 ... Validation loss: 0.1640.06805980401120333\n",
            "0.14713768765879098\n",
            "Progress: 71.6% ... Training loss: 0.068 ... Validation loss: 0.1470.06140073509093319\n",
            "0.18181586673118616\n",
            "Progress: 71.7% ... Training loss: 0.061 ... Validation loss: 0.1810.06038667816354829\n",
            "0.17075025938804822\n",
            "Progress: 71.7% ... Training loss: 0.060 ... Validation loss: 0.1700.059966987340236505\n",
            "0.15785100090207302\n",
            "Progress: 71.7% ... Training loss: 0.059 ... Validation loss: 0.1570.059741503061278366\n",
            "0.15939541381625394\n",
            "Progress: 71.7% ... Training loss: 0.059 ... Validation loss: 0.1590.06837619861756951\n",
            "0.15844314426494072\n",
            "Progress: 71.7% ... Training loss: 0.068 ... Validation loss: 0.1580.06382116920520965\n",
            "0.15292906251680344\n",
            "Progress: 71.8% ... Training loss: 0.063 ... Validation loss: 0.1520.06286703052382031\n",
            "0.15264231648262364\n",
            "Progress: 71.8% ... Training loss: 0.062 ... Validation loss: 0.1520.06090918821902459\n",
            "0.16512893550889046\n",
            "Progress: 71.8% ... Training loss: 0.060 ... Validation loss: 0.1650.06073448220764298\n",
            "0.155824328457601\n",
            "Progress: 71.8% ... Training loss: 0.060 ... Validation loss: 0.1550.05972548120003017\n",
            "0.16648256066680306\n",
            "Progress: 71.8% ... Training loss: 0.059 ... Validation loss: 0.1660.062292141285452005\n",
            "0.17267624718435134\n",
            "Progress: 71.9% ... Training loss: 0.062 ... Validation loss: 0.1720.06048013275109654\n",
            "0.15226670955132954\n",
            "Progress: 71.9% ... Training loss: 0.060 ... Validation loss: 0.1520.06032438264030506\n",
            "0.15008799701279374\n",
            "Progress: 71.9% ... Training loss: 0.060 ... Validation loss: 0.1500.06173613548345276\n",
            "0.17710545453027193\n",
            "Progress: 71.9% ... Training loss: 0.061 ... Validation loss: 0.1770.06419208457262142\n",
            "0.18692604300148957\n",
            "Progress: 71.9% ... Training loss: 0.064 ... Validation loss: 0.1860.06196018398946389\n",
            "0.15307780230989654\n",
            "Progress: 72.0% ... Training loss: 0.061 ... Validation loss: 0.1530.05933823581113037\n",
            "0.1462979406374882\n",
            "Progress: 72.0% ... Training loss: 0.059 ... Validation loss: 0.1460.062473151773738064\n",
            "0.1632312827653168\n",
            "Progress: 72.0% ... Training loss: 0.062 ... Validation loss: 0.1630.062040895013683095\n",
            "0.14762968404297405\n",
            "Progress: 72.0% ... Training loss: 0.062 ... Validation loss: 0.1470.06603770735211272\n",
            "0.18809036111800356\n",
            "Progress: 72.0% ... Training loss: 0.066 ... Validation loss: 0.1880.05920040011311947\n",
            "0.14833759714657502\n",
            "Progress: 72.1% ... Training loss: 0.059 ... Validation loss: 0.1480.061049789521599764\n",
            "0.1559061539479392\n",
            "Progress: 72.1% ... Training loss: 0.061 ... Validation loss: 0.1550.060350145914588704\n",
            "0.15531374986182525\n",
            "Progress: 72.1% ... Training loss: 0.060 ... Validation loss: 0.1550.060751845724256896\n",
            "0.14892411602441047\n",
            "Progress: 72.1% ... Training loss: 0.060 ... Validation loss: 0.1480.06923302845183678\n",
            "0.14073756029819298\n",
            "Progress: 72.1% ... Training loss: 0.069 ... Validation loss: 0.1400.061852888819215315\n",
            "0.14956511969571404\n",
            "Progress: 72.2% ... Training loss: 0.061 ... Validation loss: 0.1490.06092196785485013\n",
            "0.14272338965251496\n",
            "Progress: 72.2% ... Training loss: 0.060 ... Validation loss: 0.1420.06329080618356632\n",
            "0.15821598028899075\n",
            "Progress: 72.2% ... Training loss: 0.063 ... Validation loss: 0.1580.06330471556224794\n",
            "0.16604458944931527\n",
            "Progress: 72.2% ... Training loss: 0.063 ... Validation loss: 0.1660.07160662713179215\n",
            "0.14765691087680496\n",
            "Progress: 72.2% ... Training loss: 0.071 ... Validation loss: 0.1470.06106256822403265\n",
            "0.155861348830699\n",
            "Progress: 72.3% ... Training loss: 0.061 ... Validation loss: 0.1550.06023524841610407\n",
            "0.14452465366038592\n",
            "Progress: 72.3% ... Training loss: 0.060 ... Validation loss: 0.1440.06389598024171828\n",
            "0.16854637963524718\n",
            "Progress: 72.3% ... Training loss: 0.063 ... Validation loss: 0.1680.06977607718680376\n",
            "0.1485618422418661\n",
            "Progress: 72.3% ... Training loss: 0.069 ... Validation loss: 0.1480.06637448475780498\n",
            "0.16922699876271813\n",
            "Progress: 72.3% ... Training loss: 0.066 ... Validation loss: 0.1690.0608400306114868\n",
            "0.1496638921232491\n",
            "Progress: 72.4% ... Training loss: 0.060 ... Validation loss: 0.1490.062216291099938585\n",
            "0.14769228236698181\n",
            "Progress: 72.4% ... Training loss: 0.062 ... Validation loss: 0.1470.06685912189440349\n",
            "0.187399808381167\n",
            "Progress: 72.4% ... Training loss: 0.066 ... Validation loss: 0.1870.059324445765185044\n",
            "0.15086868314341428\n",
            "Progress: 72.4% ... Training loss: 0.059 ... Validation loss: 0.1500.0629815487846205\n",
            "0.15448222958394334\n",
            "Progress: 72.4% ... Training loss: 0.062 ... Validation loss: 0.1540.08696043423409962\n",
            "0.17512221453950558\n",
            "Progress: 72.5% ... Training loss: 0.086 ... Validation loss: 0.1750.06090849965695066\n",
            "0.1541788648342826\n",
            "Progress: 72.5% ... Training loss: 0.060 ... Validation loss: 0.1540.059829410981073895\n",
            "0.17659689061948608\n",
            "Progress: 72.5% ... Training loss: 0.059 ... Validation loss: 0.1760.05855074029611605\n",
            "0.1653548778291029\n",
            "Progress: 72.5% ... Training loss: 0.058 ... Validation loss: 0.1650.059791502483713836\n",
            "0.1591548546916099\n",
            "Progress: 72.5% ... Training loss: 0.059 ... Validation loss: 0.1590.06032879994808984\n",
            "0.1767190813410752\n",
            "Progress: 72.6% ... Training loss: 0.060 ... Validation loss: 0.1760.061562612854438085\n",
            "0.150929814215605\n",
            "Progress: 72.6% ... Training loss: 0.061 ... Validation loss: 0.1500.05906492937450787\n",
            "0.1768604529517046\n",
            "Progress: 72.6% ... Training loss: 0.059 ... Validation loss: 0.1760.06118534559120986\n",
            "0.1630450786663166\n",
            "Progress: 72.6% ... Training loss: 0.061 ... Validation loss: 0.1630.06137781203729829\n",
            "0.1610853015834127\n",
            "Progress: 72.6% ... Training loss: 0.061 ... Validation loss: 0.1610.05978314787731673\n",
            "0.17225849315447928\n",
            "Progress: 72.7% ... Training loss: 0.059 ... Validation loss: 0.1720.059771430727582994\n",
            "0.17636760646467492\n",
            "Progress: 72.7% ... Training loss: 0.059 ... Validation loss: 0.1760.06029541764327425\n",
            "0.16720096301771714\n",
            "Progress: 72.7% ... Training loss: 0.060 ... Validation loss: 0.1670.07532872762159755\n",
            "0.14715638867559314\n",
            "Progress: 72.7% ... Training loss: 0.075 ... Validation loss: 0.1470.0668624347318277\n",
            "0.13607536672354506\n",
            "Progress: 72.7% ... Training loss: 0.066 ... Validation loss: 0.1360.05943847581297073\n",
            "0.14949497544918608\n",
            "Progress: 72.8% ... Training loss: 0.059 ... Validation loss: 0.1490.06156686399335283\n",
            "0.1567059811261156\n",
            "Progress: 72.8% ... Training loss: 0.061 ... Validation loss: 0.1560.059604492586737244\n",
            "0.15809998809165443\n",
            "Progress: 72.8% ... Training loss: 0.059 ... Validation loss: 0.1580.05895996189501596\n",
            "0.16775200517599798\n",
            "Progress: 72.8% ... Training loss: 0.058 ... Validation loss: 0.1670.06230214130927412\n",
            "0.18022802157647208\n",
            "Progress: 72.8% ... Training loss: 0.062 ... Validation loss: 0.1800.0634652072796636\n",
            "0.15848717583155358\n",
            "Progress: 72.9% ... Training loss: 0.063 ... Validation loss: 0.1580.06031099425640748\n",
            "0.16224253705914884\n",
            "Progress: 72.9% ... Training loss: 0.060 ... Validation loss: 0.1620.061904314648663666\n",
            "0.1466840227567782\n",
            "Progress: 72.9% ... Training loss: 0.061 ... Validation loss: 0.1460.059784384105358364\n",
            "0.153603512420476\n",
            "Progress: 72.9% ... Training loss: 0.059 ... Validation loss: 0.1530.06204395728470355\n",
            "0.19027931937166384\n",
            "Progress: 72.9% ... Training loss: 0.062 ... Validation loss: 0.1900.06836400594254595\n",
            "0.14235829786651025\n",
            "Progress: 73.0% ... Training loss: 0.068 ... Validation loss: 0.1420.06069650818883244\n",
            "0.16145208280342765\n",
            "Progress: 73.0% ... Training loss: 0.060 ... Validation loss: 0.1610.060743368827568714\n",
            "0.15721483611111373\n",
            "Progress: 73.0% ... Training loss: 0.060 ... Validation loss: 0.1570.058923155049234455\n",
            "0.14909544457001853\n",
            "Progress: 73.0% ... Training loss: 0.058 ... Validation loss: 0.1490.0610246483956897\n",
            "0.17392734123246908\n",
            "Progress: 73.0% ... Training loss: 0.061 ... Validation loss: 0.1730.059987335322611135\n",
            "0.148386666814797\n",
            "Progress: 73.1% ... Training loss: 0.059 ... Validation loss: 0.1480.060751549958595906\n",
            "0.16495996317867823\n",
            "Progress: 73.1% ... Training loss: 0.060 ... Validation loss: 0.1640.06531915363744857\n",
            "0.14807071932551258\n",
            "Progress: 73.1% ... Training loss: 0.065 ... Validation loss: 0.1480.06255360632564394\n",
            "0.17000144023298838\n",
            "Progress: 73.1% ... Training loss: 0.062 ... Validation loss: 0.1700.059550137915904834\n",
            "0.1503289729659342\n",
            "Progress: 73.1% ... Training loss: 0.059 ... Validation loss: 0.1500.061437629745413\n",
            "0.14318908276251657\n",
            "Progress: 73.2% ... Training loss: 0.061 ... Validation loss: 0.1430.06295240762902574\n",
            "0.15001900496742848\n",
            "Progress: 73.2% ... Training loss: 0.062 ... Validation loss: 0.1500.06175448946971731\n",
            "0.15219017232531806\n",
            "Progress: 73.2% ... Training loss: 0.061 ... Validation loss: 0.1520.06013419099500087\n",
            "0.16672978596884447\n",
            "Progress: 73.2% ... Training loss: 0.060 ... Validation loss: 0.1660.05959440422112651\n",
            "0.16034680117009145\n",
            "Progress: 73.2% ... Training loss: 0.059 ... Validation loss: 0.1600.059878137382255676\n",
            "0.14411864087389056\n",
            "Progress: 73.3% ... Training loss: 0.059 ... Validation loss: 0.1440.05886814417417146\n",
            "0.14592007983123842\n",
            "Progress: 73.3% ... Training loss: 0.058 ... Validation loss: 0.1450.05792465067925449\n",
            "0.161662805613009\n",
            "Progress: 73.3% ... Training loss: 0.057 ... Validation loss: 0.1610.061212802316272644\n",
            "0.18162982345195922\n",
            "Progress: 73.3% ... Training loss: 0.061 ... Validation loss: 0.1810.0583237271676091\n",
            "0.15924640149448344\n",
            "Progress: 73.3% ... Training loss: 0.058 ... Validation loss: 0.1590.059959960397671964\n",
            "0.17173509667355\n",
            "Progress: 73.4% ... Training loss: 0.059 ... Validation loss: 0.1710.0649736645610993\n",
            "0.19827589164554754\n",
            "Progress: 73.4% ... Training loss: 0.064 ... Validation loss: 0.1980.05899305049170048\n",
            "0.16206961447760135\n",
            "Progress: 73.4% ... Training loss: 0.058 ... Validation loss: 0.1620.05960004855865066\n",
            "0.1511093786923411\n",
            "Progress: 73.4% ... Training loss: 0.059 ... Validation loss: 0.1510.05929561758560463\n",
            "0.1463166197571691\n",
            "Progress: 73.4% ... Training loss: 0.059 ... Validation loss: 0.1460.059559364740960546\n",
            "0.1612364584746142\n",
            "Progress: 73.5% ... Training loss: 0.059 ... Validation loss: 0.1610.0593010697782411\n",
            "0.155927486651631\n",
            "Progress: 73.5% ... Training loss: 0.059 ... Validation loss: 0.1550.05931424498595289\n",
            "0.160630346568584\n",
            "Progress: 73.5% ... Training loss: 0.059 ... Validation loss: 0.1600.06195466925296614\n",
            "0.17748497368101954\n",
            "Progress: 73.5% ... Training loss: 0.061 ... Validation loss: 0.1770.061891085204778096\n",
            "0.17553430649903395\n",
            "Progress: 73.5% ... Training loss: 0.061 ... Validation loss: 0.1750.06011492955202571\n",
            "0.15685699610700068\n",
            "Progress: 73.6% ... Training loss: 0.060 ... Validation loss: 0.1560.06568614592395328\n",
            "0.1561698446187697\n",
            "Progress: 73.6% ... Training loss: 0.065 ... Validation loss: 0.1560.06021980551450356\n",
            "0.152715148104532\n",
            "Progress: 73.6% ... Training loss: 0.060 ... Validation loss: 0.1520.07858991209505047\n",
            "0.1346711221363813\n",
            "Progress: 73.6% ... Training loss: 0.078 ... Validation loss: 0.1340.059605139361947355\n",
            "0.17364088869001224\n",
            "Progress: 73.6% ... Training loss: 0.059 ... Validation loss: 0.1730.058509759117228936\n",
            "0.1502195341475884\n",
            "Progress: 73.7% ... Training loss: 0.058 ... Validation loss: 0.1500.05914168687076554\n",
            "0.1442933299209985\n",
            "Progress: 73.7% ... Training loss: 0.059 ... Validation loss: 0.1440.06257725132071691\n",
            "0.1452084279131989\n",
            "Progress: 73.7% ... Training loss: 0.062 ... Validation loss: 0.1450.06252447129180731\n",
            "0.16780424962503662\n",
            "Progress: 73.7% ... Training loss: 0.062 ... Validation loss: 0.1670.065840440863267\n",
            "0.14332865845549717\n",
            "Progress: 73.7% ... Training loss: 0.065 ... Validation loss: 0.1430.06768248976295153\n",
            "0.14371951678475708\n",
            "Progress: 73.8% ... Training loss: 0.067 ... Validation loss: 0.1430.06367505733348075\n",
            "0.17697852723549284\n",
            "Progress: 73.8% ... Training loss: 0.063 ... Validation loss: 0.1760.06260053883008874\n",
            "0.18375871442491395\n",
            "Progress: 73.8% ... Training loss: 0.062 ... Validation loss: 0.1830.061546189591979945\n",
            "0.18638531519189333\n",
            "Progress: 73.8% ... Training loss: 0.061 ... Validation loss: 0.1860.061795807000470084\n",
            "0.1586904906493658\n",
            "Progress: 73.8% ... Training loss: 0.061 ... Validation loss: 0.1580.06315570349846032\n",
            "0.1578388144261075\n",
            "Progress: 73.9% ... Training loss: 0.063 ... Validation loss: 0.1570.059755476704700855\n",
            "0.14968210016457217\n",
            "Progress: 73.9% ... Training loss: 0.059 ... Validation loss: 0.1490.05895514036720226\n",
            "0.17278483266724698\n",
            "Progress: 73.9% ... Training loss: 0.058 ... Validation loss: 0.1720.06541584904014927\n",
            "0.15654781236276097\n",
            "Progress: 73.9% ... Training loss: 0.065 ... Validation loss: 0.1560.06251715689893518\n",
            "0.14732298720730605\n",
            "Progress: 73.9% ... Training loss: 0.062 ... Validation loss: 0.1470.06201426494715244\n",
            "0.17510121019614178\n",
            "Progress: 74.0% ... Training loss: 0.062 ... Validation loss: 0.1750.06269093910022384\n",
            "0.158031105761075\n",
            "Progress: 74.0% ... Training loss: 0.062 ... Validation loss: 0.1580.06588442361981245\n",
            "0.19179380897667817\n",
            "Progress: 74.0% ... Training loss: 0.065 ... Validation loss: 0.1910.058528072804879185\n",
            "0.15873074201387144\n",
            "Progress: 74.0% ... Training loss: 0.058 ... Validation loss: 0.1580.06836085317294013\n",
            "0.15063454893307798\n",
            "Progress: 74.0% ... Training loss: 0.068 ... Validation loss: 0.1500.059922655091442394\n",
            "0.14999587234114753\n",
            "Progress: 74.1% ... Training loss: 0.059 ... Validation loss: 0.1490.06414445882942298\n",
            "0.1566233013705928\n",
            "Progress: 74.1% ... Training loss: 0.064 ... Validation loss: 0.1560.06256720386630245\n",
            "0.13615578903283881\n",
            "Progress: 74.1% ... Training loss: 0.062 ... Validation loss: 0.1360.0726984877042311\n",
            "0.13979458898409924\n",
            "Progress: 74.1% ... Training loss: 0.072 ... Validation loss: 0.1390.058548523198037265\n",
            "0.154869423178778\n",
            "Progress: 74.1% ... Training loss: 0.058 ... Validation loss: 0.1540.0698844509028558\n",
            "0.1856947293967427\n",
            "Progress: 74.2% ... Training loss: 0.069 ... Validation loss: 0.1850.0588367713661012\n",
            "0.1608173856721988\n",
            "Progress: 74.2% ... Training loss: 0.058 ... Validation loss: 0.1600.06273507703562357\n",
            "0.1393076257020921\n",
            "Progress: 74.2% ... Training loss: 0.062 ... Validation loss: 0.1390.06014471192113472\n",
            "0.14677272145287962\n",
            "Progress: 74.2% ... Training loss: 0.060 ... Validation loss: 0.1460.0600012412445396\n",
            "0.14888294641150882\n",
            "Progress: 74.2% ... Training loss: 0.060 ... Validation loss: 0.1480.05953427637699883\n",
            "0.151725354772519\n",
            "Progress: 74.3% ... Training loss: 0.059 ... Validation loss: 0.1510.062292540437638204\n",
            "0.14108488299890642\n",
            "Progress: 74.3% ... Training loss: 0.062 ... Validation loss: 0.1410.059606289623774934\n",
            "0.14601641583306516\n",
            "Progress: 74.3% ... Training loss: 0.059 ... Validation loss: 0.1460.06333709325976877\n",
            "0.16027697518849962\n",
            "Progress: 74.3% ... Training loss: 0.063 ... Validation loss: 0.1600.058720173659566846\n",
            "0.15404890541043784\n",
            "Progress: 74.3% ... Training loss: 0.058 ... Validation loss: 0.1540.05947151168490055\n",
            "0.1504334956281173\n",
            "Progress: 74.4% ... Training loss: 0.059 ... Validation loss: 0.1500.05944159157337721\n",
            "0.16296914139810798\n",
            "Progress: 74.4% ... Training loss: 0.059 ... Validation loss: 0.1620.05919514689087375\n",
            "0.14824644516747526\n",
            "Progress: 74.4% ... Training loss: 0.059 ... Validation loss: 0.1480.06901647010142968\n",
            "0.14657223130082148\n",
            "Progress: 74.4% ... Training loss: 0.069 ... Validation loss: 0.1460.0594452334967373\n",
            "0.16091395138592382\n",
            "Progress: 74.4% ... Training loss: 0.059 ... Validation loss: 0.1600.06154855561664606\n",
            "0.16972709649622664\n",
            "Progress: 74.5% ... Training loss: 0.061 ... Validation loss: 0.1690.09221313323974134\n",
            "0.15954374435462226\n",
            "Progress: 74.5% ... Training loss: 0.092 ... Validation loss: 0.1590.06074376049439298\n",
            "0.15586438361801325\n",
            "Progress: 74.5% ... Training loss: 0.060 ... Validation loss: 0.1550.05969778775404147\n",
            "0.14833080257124237\n",
            "Progress: 74.5% ... Training loss: 0.059 ... Validation loss: 0.1480.06052898620847049\n",
            "0.14370100166030356\n",
            "Progress: 74.5% ... Training loss: 0.060 ... Validation loss: 0.1430.06730928262569914\n",
            "0.18578959865202904\n",
            "Progress: 74.6% ... Training loss: 0.067 ... Validation loss: 0.1850.06590499669082857\n",
            "0.16698439348642943\n",
            "Progress: 74.6% ... Training loss: 0.065 ... Validation loss: 0.1660.06047842028747144\n",
            "0.14823605868967554\n",
            "Progress: 74.6% ... Training loss: 0.060 ... Validation loss: 0.1480.06255351662437095\n",
            "0.15031226500417214\n",
            "Progress: 74.6% ... Training loss: 0.062 ... Validation loss: 0.1500.05957746744503979\n",
            "0.16451186760248301\n",
            "Progress: 74.6% ... Training loss: 0.059 ... Validation loss: 0.1640.0657931070558641\n",
            "0.1499293108373567\n",
            "Progress: 74.7% ... Training loss: 0.065 ... Validation loss: 0.1490.059547006550788865\n",
            "0.13852098363460694\n",
            "Progress: 74.7% ... Training loss: 0.059 ... Validation loss: 0.1380.06646620418845064\n",
            "0.1788147444131966\n",
            "Progress: 74.7% ... Training loss: 0.066 ... Validation loss: 0.1780.06049230843770436\n",
            "0.14481816535301767\n",
            "Progress: 74.7% ... Training loss: 0.060 ... Validation loss: 0.1440.058860723647403246\n",
            "0.1543051931664454\n",
            "Progress: 74.7% ... Training loss: 0.058 ... Validation loss: 0.1540.05831475651536974\n",
            "0.15041490945963912\n",
            "Progress: 74.8% ... Training loss: 0.058 ... Validation loss: 0.1500.05957277621155043\n",
            "0.14853456297411988\n",
            "Progress: 74.8% ... Training loss: 0.059 ... Validation loss: 0.1480.057946931166967065\n",
            "0.14402465351719168\n",
            "Progress: 74.8% ... Training loss: 0.057 ... Validation loss: 0.1440.06262531958174317\n",
            "0.1598703509318005\n",
            "Progress: 74.8% ... Training loss: 0.062 ... Validation loss: 0.1590.058752130344663436\n",
            "0.14782177302076696\n",
            "Progress: 74.8% ... Training loss: 0.058 ... Validation loss: 0.1470.058327328748383034\n",
            "0.15251608440216746\n",
            "Progress: 74.9% ... Training loss: 0.058 ... Validation loss: 0.1520.06322918545647245\n",
            "0.17329049576360195\n",
            "Progress: 74.9% ... Training loss: 0.063 ... Validation loss: 0.1730.05916659240146836\n",
            "0.13899698693419202\n",
            "Progress: 74.9% ... Training loss: 0.059 ... Validation loss: 0.1380.059827195399624056\n",
            "0.14455282980474798\n",
            "Progress: 74.9% ... Training loss: 0.059 ... Validation loss: 0.1440.06012403887179448\n",
            "0.16250050363279187\n",
            "Progress: 74.9% ... Training loss: 0.060 ... Validation loss: 0.1620.06106862622806738\n",
            "0.15224380360120415\n",
            "Progress: 75.0% ... Training loss: 0.061 ... Validation loss: 0.1520.062132085014273135\n",
            "0.13841365041551107\n",
            "Progress: 75.0% ... Training loss: 0.062 ... Validation loss: 0.1380.06259488822698765\n",
            "0.15772162884745985\n",
            "Progress: 75.0% ... Training loss: 0.062 ... Validation loss: 0.1570.06060171064242446\n",
            "0.1519739836023014\n",
            "Progress: 75.0% ... Training loss: 0.060 ... Validation loss: 0.1510.05980304675119957\n",
            "0.14426472622236441\n",
            "Progress: 75.0% ... Training loss: 0.059 ... Validation loss: 0.1440.05890241698964482\n",
            "0.14812738804501205\n",
            "Progress: 75.1% ... Training loss: 0.058 ... Validation loss: 0.1480.06758752561817924\n",
            "0.19170858472305558\n",
            "Progress: 75.1% ... Training loss: 0.067 ... Validation loss: 0.1910.058430347176691655\n",
            "0.14442320691516752\n",
            "Progress: 75.1% ... Training loss: 0.058 ... Validation loss: 0.1440.06501997088400452\n",
            "0.14285575774871337\n",
            "Progress: 75.1% ... Training loss: 0.065 ... Validation loss: 0.1420.0604088183808292\n",
            "0.17074354099207054\n",
            "Progress: 75.1% ... Training loss: 0.060 ... Validation loss: 0.1700.060434330694375586\n",
            "0.14093077753472347\n",
            "Progress: 75.2% ... Training loss: 0.060 ... Validation loss: 0.1400.060285584887598824\n",
            "0.1579530046505178\n",
            "Progress: 75.2% ... Training loss: 0.060 ... Validation loss: 0.1570.08731728539434255\n",
            "0.19462613673637125\n",
            "Progress: 75.2% ... Training loss: 0.087 ... Validation loss: 0.1940.06238517163449595\n",
            "0.1613282354113581\n",
            "Progress: 75.2% ... Training loss: 0.062 ... Validation loss: 0.1610.05780821886722883\n",
            "0.146846537253486\n",
            "Progress: 75.2% ... Training loss: 0.057 ... Validation loss: 0.1460.05972297085852353\n",
            "0.15186034672677226\n",
            "Progress: 75.3% ... Training loss: 0.059 ... Validation loss: 0.1510.06075680177873111\n",
            "0.14453416279217426\n",
            "Progress: 75.3% ... Training loss: 0.060 ... Validation loss: 0.1440.05932113686865157\n",
            "0.1505999082039744\n",
            "Progress: 75.3% ... Training loss: 0.059 ... Validation loss: 0.1500.06275987518914414\n",
            "0.14582609737705554\n",
            "Progress: 75.3% ... Training loss: 0.062 ... Validation loss: 0.1450.058913274852098185\n",
            "0.1490582309356017\n",
            "Progress: 75.3% ... Training loss: 0.058 ... Validation loss: 0.1490.06684759328892022\n",
            "0.1397756110558008\n",
            "Progress: 75.4% ... Training loss: 0.066 ... Validation loss: 0.1390.06744778266361143\n",
            "0.13956980462504603\n",
            "Progress: 75.4% ... Training loss: 0.067 ... Validation loss: 0.1390.05937291598363358\n",
            "0.14798082418673722\n",
            "Progress: 75.4% ... Training loss: 0.059 ... Validation loss: 0.1470.06053147780739889\n",
            "0.13743506471725828\n",
            "Progress: 75.4% ... Training loss: 0.060 ... Validation loss: 0.1370.05864841178663381\n",
            "0.14732999800513438\n",
            "Progress: 75.4% ... Training loss: 0.058 ... Validation loss: 0.1470.05904155767073393\n",
            "0.15123836611268363\n",
            "Progress: 75.5% ... Training loss: 0.059 ... Validation loss: 0.1510.060785224846615256\n",
            "0.1588894333011986\n",
            "Progress: 75.5% ... Training loss: 0.060 ... Validation loss: 0.1580.06433759842554516\n",
            "0.15481723754795815\n",
            "Progress: 75.5% ... Training loss: 0.064 ... Validation loss: 0.1540.06000598993475108\n",
            "0.15510434538219067\n",
            "Progress: 75.5% ... Training loss: 0.060 ... Validation loss: 0.1550.058868059772253435\n",
            "0.1540103434861146\n",
            "Progress: 75.5% ... Training loss: 0.058 ... Validation loss: 0.1540.06040825190804892\n",
            "0.1422183417135797\n",
            "Progress: 75.6% ... Training loss: 0.060 ... Validation loss: 0.1420.058547435206580574\n",
            "0.14978405711178538\n",
            "Progress: 75.6% ... Training loss: 0.058 ... Validation loss: 0.1490.06021608134478808\n",
            "0.13808975420066297\n",
            "Progress: 75.6% ... Training loss: 0.060 ... Validation loss: 0.1380.05953250572768589\n",
            "0.13734281542937235\n",
            "Progress: 75.6% ... Training loss: 0.059 ... Validation loss: 0.1370.0611908099202341\n",
            "0.14524722411460309\n",
            "Progress: 75.6% ... Training loss: 0.061 ... Validation loss: 0.1450.060423334846042225\n",
            "0.16365374463793655\n",
            "Progress: 75.7% ... Training loss: 0.060 ... Validation loss: 0.1630.05902884097248414\n",
            "0.15707990240759484\n",
            "Progress: 75.7% ... Training loss: 0.059 ... Validation loss: 0.1570.05872017299344947\n",
            "0.1607191316802767\n",
            "Progress: 75.7% ... Training loss: 0.058 ... Validation loss: 0.1600.0586503974120268\n",
            "0.17055216253520922\n",
            "Progress: 75.7% ... Training loss: 0.058 ... Validation loss: 0.1700.06369584684553925\n",
            "0.20431636190768238\n",
            "Progress: 75.7% ... Training loss: 0.063 ... Validation loss: 0.2040.05994719404243462\n",
            "0.18267666661442092\n",
            "Progress: 75.8% ... Training loss: 0.059 ... Validation loss: 0.1820.06101553125850654\n",
            "0.14307133767221086\n",
            "Progress: 75.8% ... Training loss: 0.061 ... Validation loss: 0.1430.059067718758524734\n",
            "0.15616225925918356\n",
            "Progress: 75.8% ... Training loss: 0.059 ... Validation loss: 0.1560.060778743531304975\n",
            "0.16767093880287226\n",
            "Progress: 75.8% ... Training loss: 0.060 ... Validation loss: 0.1670.062169144655855206\n",
            "0.15812560546852833\n",
            "Progress: 75.8% ... Training loss: 0.062 ... Validation loss: 0.1580.06175426456593089\n",
            "0.1553263426636099\n",
            "Progress: 75.9% ... Training loss: 0.061 ... Validation loss: 0.1550.062361447608540156\n",
            "0.17839482313945246\n",
            "Progress: 75.9% ... Training loss: 0.062 ... Validation loss: 0.1780.06273624693048808\n",
            "0.14241771311415738\n",
            "Progress: 75.9% ... Training loss: 0.062 ... Validation loss: 0.1420.06050214229121896\n",
            "0.15239527175524106\n",
            "Progress: 75.9% ... Training loss: 0.060 ... Validation loss: 0.1520.06824431061638297\n",
            "0.1374482185610841\n",
            "Progress: 75.9% ... Training loss: 0.068 ... Validation loss: 0.1370.05815002179189357\n",
            "0.15432111972730914\n",
            "Progress: 76.0% ... Training loss: 0.058 ... Validation loss: 0.1540.06089669756730331\n",
            "0.16285479857604787\n",
            "Progress: 76.0% ... Training loss: 0.060 ... Validation loss: 0.1620.06960441486084958\n",
            "0.1438116661052489\n",
            "Progress: 76.0% ... Training loss: 0.069 ... Validation loss: 0.1430.06100369608662258\n",
            "0.1611009048759067\n",
            "Progress: 76.0% ... Training loss: 0.061 ... Validation loss: 0.1610.058366938073997614\n",
            "0.153805778729486\n",
            "Progress: 76.0% ... Training loss: 0.058 ... Validation loss: 0.1530.06375001281982945\n",
            "0.14444310878551392\n",
            "Progress: 76.1% ... Training loss: 0.063 ... Validation loss: 0.1440.06082870029452209\n",
            "0.16220739671204712\n",
            "Progress: 76.1% ... Training loss: 0.060 ... Validation loss: 0.1620.06323242394928369\n",
            "0.1420622998321456\n",
            "Progress: 76.1% ... Training loss: 0.063 ... Validation loss: 0.1420.05902732306557066\n",
            "0.16516937522176192\n",
            "Progress: 76.1% ... Training loss: 0.059 ... Validation loss: 0.1650.05901957221824434\n",
            "0.15343495825988707\n",
            "Progress: 76.1% ... Training loss: 0.059 ... Validation loss: 0.1530.062139661455449545\n",
            "0.16973769191245022\n",
            "Progress: 76.2% ... Training loss: 0.062 ... Validation loss: 0.1690.061452551297186464\n",
            "0.17552180783617946\n",
            "Progress: 76.2% ... Training loss: 0.061 ... Validation loss: 0.1750.07957663329408966\n",
            "0.14258108153592855\n",
            "Progress: 76.2% ... Training loss: 0.079 ... Validation loss: 0.1420.07110485579106798\n",
            "0.15003486868121085\n",
            "Progress: 76.2% ... Training loss: 0.071 ... Validation loss: 0.1500.05934560754632203\n",
            "0.14834080430275914\n",
            "Progress: 76.2% ... Training loss: 0.059 ... Validation loss: 0.1480.06443433639210468\n",
            "0.14266113191073326\n",
            "Progress: 76.3% ... Training loss: 0.064 ... Validation loss: 0.1420.06241406010577529\n",
            "0.16652744868123023\n",
            "Progress: 76.3% ... Training loss: 0.062 ... Validation loss: 0.1660.06097651379692604\n",
            "0.15764108933472978\n",
            "Progress: 76.3% ... Training loss: 0.060 ... Validation loss: 0.1570.06849335492942783\n",
            "0.17107816571981616\n",
            "Progress: 76.3% ... Training loss: 0.068 ... Validation loss: 0.1710.06062070327614201\n",
            "0.14386016739272328\n",
            "Progress: 76.3% ... Training loss: 0.060 ... Validation loss: 0.1430.059016706269479344\n",
            "0.15321718299919848\n",
            "Progress: 76.4% ... Training loss: 0.059 ... Validation loss: 0.1530.06468734619329874\n",
            "0.18622239357946274\n",
            "Progress: 76.4% ... Training loss: 0.064 ... Validation loss: 0.1860.059500357400894044\n",
            "0.14283598118826615\n",
            "Progress: 76.4% ... Training loss: 0.059 ... Validation loss: 0.1420.0623970937884521\n",
            "0.16174767624814665\n",
            "Progress: 76.4% ... Training loss: 0.062 ... Validation loss: 0.1610.06003747261234427\n",
            "0.1579382825409828\n",
            "Progress: 76.4% ... Training loss: 0.060 ... Validation loss: 0.1570.06082433559621825\n",
            "0.1381917287028532\n",
            "Progress: 76.5% ... Training loss: 0.060 ... Validation loss: 0.1380.059362704951740765\n",
            "0.13916895028416873\n",
            "Progress: 76.5% ... Training loss: 0.059 ... Validation loss: 0.1390.05925263859409529\n",
            "0.14002543248514268\n",
            "Progress: 76.5% ... Training loss: 0.059 ... Validation loss: 0.1400.062413953261006086\n",
            "0.15256207609815395\n",
            "Progress: 76.5% ... Training loss: 0.062 ... Validation loss: 0.1520.06005590393914747\n",
            "0.1497223781915195\n",
            "Progress: 76.5% ... Training loss: 0.060 ... Validation loss: 0.1490.10044869133611702\n",
            "0.1659942211495491\n",
            "Progress: 76.6% ... Training loss: 0.100 ... Validation loss: 0.1650.061649344444397104\n",
            "0.14572157242800118\n",
            "Progress: 76.6% ... Training loss: 0.061 ... Validation loss: 0.1450.0613052049999868\n",
            "0.1480895503642679\n",
            "Progress: 76.6% ... Training loss: 0.061 ... Validation loss: 0.1480.05925563438319568\n",
            "0.14373561325269305\n",
            "Progress: 76.6% ... Training loss: 0.059 ... Validation loss: 0.1430.060190455385667835\n",
            "0.14513458785935315\n",
            "Progress: 76.6% ... Training loss: 0.060 ... Validation loss: 0.1450.06206934743894747\n",
            "0.15716782482120734\n",
            "Progress: 76.7% ... Training loss: 0.062 ... Validation loss: 0.1570.059808408892956334\n",
            "0.15231772397929258\n",
            "Progress: 76.7% ... Training loss: 0.059 ... Validation loss: 0.1520.07207900829327753\n",
            "0.18939240364439547\n",
            "Progress: 76.7% ... Training loss: 0.072 ... Validation loss: 0.1890.06104260996423483\n",
            "0.14560422674538706\n",
            "Progress: 76.7% ... Training loss: 0.061 ... Validation loss: 0.1450.05877382933696824\n",
            "0.167557509078123\n",
            "Progress: 76.7% ... Training loss: 0.058 ... Validation loss: 0.1670.05805694169733965\n",
            "0.14414795810879594\n",
            "Progress: 76.8% ... Training loss: 0.058 ... Validation loss: 0.1440.058326407645478834\n",
            "0.15407815104309883\n",
            "Progress: 76.8% ... Training loss: 0.058 ... Validation loss: 0.1540.05827703837607998\n",
            "0.1482087838564817\n",
            "Progress: 76.8% ... Training loss: 0.058 ... Validation loss: 0.1480.06477505674610684\n",
            "0.18320723839601524\n",
            "Progress: 76.8% ... Training loss: 0.064 ... Validation loss: 0.1830.058837175655423145\n",
            "0.16071880858852008\n",
            "Progress: 76.8% ... Training loss: 0.058 ... Validation loss: 0.1600.05949745540061354\n",
            "0.14212998116184133\n",
            "Progress: 76.9% ... Training loss: 0.059 ... Validation loss: 0.1420.058094688290535906\n",
            "0.15093589504991525\n",
            "Progress: 76.9% ... Training loss: 0.058 ... Validation loss: 0.1500.06027295840154546\n",
            "0.1534839781965953\n",
            "Progress: 76.9% ... Training loss: 0.060 ... Validation loss: 0.1530.07076582063329542\n",
            "0.17802454710800322\n",
            "Progress: 76.9% ... Training loss: 0.070 ... Validation loss: 0.1780.05893941771168654\n",
            "0.15715609959339336\n",
            "Progress: 76.9% ... Training loss: 0.058 ... Validation loss: 0.1570.06124237932262052\n",
            "0.1509226367471626\n",
            "Progress: 77.0% ... Training loss: 0.061 ... Validation loss: 0.1500.05838044264726515\n",
            "0.1591635853757069\n",
            "Progress: 77.0% ... Training loss: 0.058 ... Validation loss: 0.1590.059556012357200906\n",
            "0.15775025830142747\n",
            "Progress: 77.0% ... Training loss: 0.059 ... Validation loss: 0.1570.06249309673555\n",
            "0.16201650221196187\n",
            "Progress: 77.0% ... Training loss: 0.062 ... Validation loss: 0.1620.06344701852622633\n",
            "0.15150279446533574\n",
            "Progress: 77.0% ... Training loss: 0.063 ... Validation loss: 0.1510.05913655978135824\n",
            "0.14547635938981188\n",
            "Progress: 77.1% ... Training loss: 0.059 ... Validation loss: 0.1450.0673075085537756\n",
            "0.14406874771878128\n",
            "Progress: 77.1% ... Training loss: 0.067 ... Validation loss: 0.1440.05750272650571838\n",
            "0.1653210698941499\n",
            "Progress: 77.1% ... Training loss: 0.057 ... Validation loss: 0.1650.061435721257511997\n",
            "0.18182791466067375\n",
            "Progress: 77.1% ... Training loss: 0.061 ... Validation loss: 0.1810.06606252025088626\n",
            "0.15577351288358385\n",
            "Progress: 77.1% ... Training loss: 0.066 ... Validation loss: 0.1550.05888741028825313\n",
            "0.1516903569418941\n",
            "Progress: 77.2% ... Training loss: 0.058 ... Validation loss: 0.1510.06315480572836374\n",
            "0.14698961860999893\n",
            "Progress: 77.2% ... Training loss: 0.063 ... Validation loss: 0.1460.059570737775075516\n",
            "0.14733582598140413\n",
            "Progress: 77.2% ... Training loss: 0.059 ... Validation loss: 0.1470.059140051809508175\n",
            "0.1674170048033337\n",
            "Progress: 77.2% ... Training loss: 0.059 ... Validation loss: 0.1670.06755723055045454\n",
            "0.15203099587811705\n",
            "Progress: 77.2% ... Training loss: 0.067 ... Validation loss: 0.1520.05842792576303594\n",
            "0.15035371583505452\n",
            "Progress: 77.3% ... Training loss: 0.058 ... Validation loss: 0.1500.05994176303312309\n",
            "0.16998865154819673\n",
            "Progress: 77.3% ... Training loss: 0.059 ... Validation loss: 0.1690.062274539578137805\n",
            "0.18208397618824762\n",
            "Progress: 77.3% ... Training loss: 0.062 ... Validation loss: 0.1820.07650122273524937\n",
            "0.20649119478663674\n",
            "Progress: 77.3% ... Training loss: 0.076 ... Validation loss: 0.2060.059669452282096504\n",
            "0.15224098139793546\n",
            "Progress: 77.3% ... Training loss: 0.059 ... Validation loss: 0.1520.05838822520905639\n",
            "0.15768168094690185\n",
            "Progress: 77.4% ... Training loss: 0.058 ... Validation loss: 0.1570.06991484435730896\n",
            "0.15605114013000415\n",
            "Progress: 77.4% ... Training loss: 0.069 ... Validation loss: 0.1560.05954967367338648\n",
            "0.1713901334815356\n",
            "Progress: 77.4% ... Training loss: 0.059 ... Validation loss: 0.1710.06567882333532576\n",
            "0.1589460660608913\n",
            "Progress: 77.4% ... Training loss: 0.065 ... Validation loss: 0.1580.06139111144561858\n",
            "0.1501570179927574\n",
            "Progress: 77.4% ... Training loss: 0.061 ... Validation loss: 0.1500.0592301948576349\n",
            "0.15101126887838776\n",
            "Progress: 77.5% ... Training loss: 0.059 ... Validation loss: 0.1510.05920959148348832\n",
            "0.16523072501226332\n",
            "Progress: 77.5% ... Training loss: 0.059 ... Validation loss: 0.1650.06081500654677553\n",
            "0.14894689019931476\n",
            "Progress: 77.5% ... Training loss: 0.060 ... Validation loss: 0.1480.057797592182644505\n",
            "0.15639679463096287\n",
            "Progress: 77.5% ... Training loss: 0.057 ... Validation loss: 0.1560.061949590887475654\n",
            "0.1497944249647874\n",
            "Progress: 77.5% ... Training loss: 0.061 ... Validation loss: 0.1490.06095977406514057\n",
            "0.15394862262679426\n",
            "Progress: 77.6% ... Training loss: 0.060 ... Validation loss: 0.1530.061154402180737165\n",
            "0.1597418300108495\n",
            "Progress: 77.6% ... Training loss: 0.061 ... Validation loss: 0.1590.06102429297642327\n",
            "0.17947554929671355\n",
            "Progress: 77.6% ... Training loss: 0.061 ... Validation loss: 0.1790.0590614395202211\n",
            "0.16141496508694228\n",
            "Progress: 77.6% ... Training loss: 0.059 ... Validation loss: 0.1610.05917940290031553\n",
            "0.15574927489982257\n",
            "Progress: 77.6% ... Training loss: 0.059 ... Validation loss: 0.1550.06314096473843636\n",
            "0.14964696782493436\n",
            "Progress: 77.7% ... Training loss: 0.063 ... Validation loss: 0.1490.059988761332172465\n",
            "0.16425776634018427\n",
            "Progress: 77.7% ... Training loss: 0.059 ... Validation loss: 0.1640.05764784424258754\n",
            "0.16642611188087691\n",
            "Progress: 77.7% ... Training loss: 0.057 ... Validation loss: 0.1660.06037637858955175\n",
            "0.1521625541766638\n",
            "Progress: 77.7% ... Training loss: 0.060 ... Validation loss: 0.1520.05838380503902245\n",
            "0.16088971676070893\n",
            "Progress: 77.7% ... Training loss: 0.058 ... Validation loss: 0.1600.05937355778012777\n",
            "0.1831820516164468\n",
            "Progress: 77.8% ... Training loss: 0.059 ... Validation loss: 0.1830.06261166138816142\n",
            "0.17996687950142307\n",
            "Progress: 77.8% ... Training loss: 0.062 ... Validation loss: 0.1790.06936267787849068\n",
            "0.17650787159298195\n",
            "Progress: 77.8% ... Training loss: 0.069 ... Validation loss: 0.1760.058414325097900406\n",
            "0.15466989242996726\n",
            "Progress: 77.8% ... Training loss: 0.058 ... Validation loss: 0.1540.06025952746318701\n",
            "0.15212692206117256\n",
            "Progress: 77.8% ... Training loss: 0.060 ... Validation loss: 0.1520.0577845828046849\n",
            "0.16547537978859853\n",
            "Progress: 77.9% ... Training loss: 0.057 ... Validation loss: 0.1650.0707584492066148\n",
            "0.147280718833619\n",
            "Progress: 77.9% ... Training loss: 0.070 ... Validation loss: 0.1470.05967370066453479\n",
            "0.16284707282816424\n",
            "Progress: 77.9% ... Training loss: 0.059 ... Validation loss: 0.1620.058752218395295104\n",
            "0.14777019795872076\n",
            "Progress: 77.9% ... Training loss: 0.058 ... Validation loss: 0.1470.06017242851339496\n",
            "0.1749013531337216\n",
            "Progress: 77.9% ... Training loss: 0.060 ... Validation loss: 0.1740.06227350729344451\n",
            "0.19267663478463873\n",
            "Progress: 78.0% ... Training loss: 0.062 ... Validation loss: 0.1920.05896099734144697\n",
            "0.15224466005382362\n",
            "Progress: 78.0% ... Training loss: 0.058 ... Validation loss: 0.1520.06407839059830224\n",
            "0.13519353422106792\n",
            "Progress: 78.0% ... Training loss: 0.064 ... Validation loss: 0.1350.05867898484787271\n",
            "0.16151137191595805\n",
            "Progress: 78.0% ... Training loss: 0.058 ... Validation loss: 0.1610.059951780160779115\n",
            "0.17118011246709078\n",
            "Progress: 78.0% ... Training loss: 0.059 ... Validation loss: 0.1710.07039268461248649\n",
            "0.14935306221612973\n",
            "Progress: 78.1% ... Training loss: 0.070 ... Validation loss: 0.1490.05735379756418262\n",
            "0.15445083157210007\n",
            "Progress: 78.1% ... Training loss: 0.057 ... Validation loss: 0.1540.05814463542637393\n",
            "0.1566419408562312\n",
            "Progress: 78.1% ... Training loss: 0.058 ... Validation loss: 0.1560.05807964709701348\n",
            "0.15000004754847301\n",
            "Progress: 78.1% ... Training loss: 0.058 ... Validation loss: 0.1500.06072966793008319\n",
            "0.15656335573732005\n",
            "Progress: 78.1% ... Training loss: 0.060 ... Validation loss: 0.1560.059800470102304264\n",
            "0.14233259439868384\n",
            "Progress: 78.2% ... Training loss: 0.059 ... Validation loss: 0.1420.0595507506075791\n",
            "0.15994094309765589\n",
            "Progress: 78.2% ... Training loss: 0.059 ... Validation loss: 0.1590.058536838692993816\n",
            "0.1628190504347117\n",
            "Progress: 78.2% ... Training loss: 0.058 ... Validation loss: 0.1620.06385518925275135\n",
            "0.1548355701458109\n",
            "Progress: 78.2% ... Training loss: 0.063 ... Validation loss: 0.1540.0599169206604457\n",
            "0.16380397646661243\n",
            "Progress: 78.2% ... Training loss: 0.059 ... Validation loss: 0.1630.06226211767318055\n",
            "0.17431343172437014\n",
            "Progress: 78.3% ... Training loss: 0.062 ... Validation loss: 0.1740.061010103970254304\n",
            "0.15814942869471205\n",
            "Progress: 78.3% ... Training loss: 0.061 ... Validation loss: 0.1580.06339359575710916\n",
            "0.18508801709316483\n",
            "Progress: 78.3% ... Training loss: 0.063 ... Validation loss: 0.1850.05942130146907178\n",
            "0.15716258213356554\n",
            "Progress: 78.3% ... Training loss: 0.059 ... Validation loss: 0.1570.05931457446460267\n",
            "0.17412540573636484\n",
            "Progress: 78.3% ... Training loss: 0.059 ... Validation loss: 0.1740.0600457305687497\n",
            "0.14127632031001103\n",
            "Progress: 78.4% ... Training loss: 0.060 ... Validation loss: 0.1410.0666530192926129\n",
            "0.14054210670991152\n",
            "Progress: 78.4% ... Training loss: 0.066 ... Validation loss: 0.1400.06000480598033655\n",
            "0.14524496878669726\n",
            "Progress: 78.4% ... Training loss: 0.060 ... Validation loss: 0.1450.05817706858099066\n",
            "0.1418462643612434\n",
            "Progress: 78.4% ... Training loss: 0.058 ... Validation loss: 0.1410.058003850840758694\n",
            "0.14324600528591575\n",
            "Progress: 78.4% ... Training loss: 0.058 ... Validation loss: 0.1430.06378987957399691\n",
            "0.1455537629101888\n",
            "Progress: 78.5% ... Training loss: 0.063 ... Validation loss: 0.1450.05827962533083946\n",
            "0.13859899045002524\n",
            "Progress: 78.5% ... Training loss: 0.058 ... Validation loss: 0.1380.06077415499139647\n",
            "0.14051647122267097\n",
            "Progress: 78.5% ... Training loss: 0.060 ... Validation loss: 0.1400.058579583936597235\n",
            "0.13964573521792822\n",
            "Progress: 78.5% ... Training loss: 0.058 ... Validation loss: 0.1390.05865290260089843\n",
            "0.14941211673573812\n",
            "Progress: 78.5% ... Training loss: 0.058 ... Validation loss: 0.1490.06043165385105862\n",
            "0.14227759089784564\n",
            "Progress: 78.6% ... Training loss: 0.060 ... Validation loss: 0.1420.06341352451527382\n",
            "0.15310024024258\n",
            "Progress: 78.6% ... Training loss: 0.063 ... Validation loss: 0.1530.05833048786693469\n",
            "0.14214024396783773\n",
            "Progress: 78.6% ... Training loss: 0.058 ... Validation loss: 0.1420.06234482907447316\n",
            "0.1445203153711159\n",
            "Progress: 78.6% ... Training loss: 0.062 ... Validation loss: 0.1440.05966071155792654\n",
            "0.15201682769583738\n",
            "Progress: 78.6% ... Training loss: 0.059 ... Validation loss: 0.1520.05777868720067369\n",
            "0.14160489984024127\n",
            "Progress: 78.7% ... Training loss: 0.057 ... Validation loss: 0.1410.057841369516965405\n",
            "0.1415254209088559\n",
            "Progress: 78.7% ... Training loss: 0.057 ... Validation loss: 0.1410.05897104135535119\n",
            "0.14894179679569955\n",
            "Progress: 78.7% ... Training loss: 0.058 ... Validation loss: 0.1480.07487643734440426\n",
            "0.14479656593800622\n",
            "Progress: 78.7% ... Training loss: 0.074 ... Validation loss: 0.1440.059737936864585686\n",
            "0.15166360040976573\n",
            "Progress: 78.7% ... Training loss: 0.059 ... Validation loss: 0.1510.06959499086106033\n",
            "0.14638597405549592\n",
            "Progress: 78.8% ... Training loss: 0.069 ... Validation loss: 0.1460.05747935656060744\n",
            "0.14729417596733377\n",
            "Progress: 78.8% ... Training loss: 0.057 ... Validation loss: 0.1470.05816754638569587\n",
            "0.14029478915075846\n",
            "Progress: 78.8% ... Training loss: 0.058 ... Validation loss: 0.1400.05909294523350704\n",
            "0.15027843054610185\n",
            "Progress: 78.8% ... Training loss: 0.059 ... Validation loss: 0.1500.06257174786313249\n",
            "0.15655073239558417\n",
            "Progress: 78.8% ... Training loss: 0.062 ... Validation loss: 0.1560.05895295390350136\n",
            "0.14863897953887265\n",
            "Progress: 78.9% ... Training loss: 0.058 ... Validation loss: 0.1480.06008889427071572\n",
            "0.1578359749060034\n",
            "Progress: 78.9% ... Training loss: 0.060 ... Validation loss: 0.1570.05779074399830427\n",
            "0.14673753098067685\n",
            "Progress: 78.9% ... Training loss: 0.057 ... Validation loss: 0.1460.05745871310683721\n",
            "0.15539595891398658\n",
            "Progress: 78.9% ... Training loss: 0.057 ... Validation loss: 0.1550.06037447379773248\n",
            "0.1612080027342132\n",
            "Progress: 78.9% ... Training loss: 0.060 ... Validation loss: 0.1610.06301764247273324\n",
            "0.1496902574615487\n",
            "Progress: 79.0% ... Training loss: 0.063 ... Validation loss: 0.1490.05946303294648083\n",
            "0.1480769317757887\n",
            "Progress: 79.0% ... Training loss: 0.059 ... Validation loss: 0.1480.07506836275476692\n",
            "0.14979634607980818\n",
            "Progress: 79.0% ... Training loss: 0.075 ... Validation loss: 0.1490.06112554573149887\n",
            "0.14395627114682266\n",
            "Progress: 79.0% ... Training loss: 0.061 ... Validation loss: 0.1430.060384412499685944\n",
            "0.15150973458796857\n",
            "Progress: 79.0% ... Training loss: 0.060 ... Validation loss: 0.1510.05941591959367537\n",
            "0.15641499250422508\n",
            "Progress: 79.1% ... Training loss: 0.059 ... Validation loss: 0.1560.059071484918099285\n",
            "0.16457689261320285\n",
            "Progress: 79.1% ... Training loss: 0.059 ... Validation loss: 0.1640.060105774450232054\n",
            "0.14549836813632908\n",
            "Progress: 79.1% ... Training loss: 0.060 ... Validation loss: 0.1450.060171253001546715\n",
            "0.15522288528234063\n",
            "Progress: 79.1% ... Training loss: 0.060 ... Validation loss: 0.1550.059132944042518704\n",
            "0.16833955288807007\n",
            "Progress: 79.1% ... Training loss: 0.059 ... Validation loss: 0.1680.057748141981470186\n",
            "0.14952392803597217\n",
            "Progress: 79.2% ... Training loss: 0.057 ... Validation loss: 0.1490.058280574503586646\n",
            "0.15040474893059033\n",
            "Progress: 79.2% ... Training loss: 0.058 ... Validation loss: 0.1500.0598593475595161\n",
            "0.16951955508686695\n",
            "Progress: 79.2% ... Training loss: 0.059 ... Validation loss: 0.1690.05853272114455427\n",
            "0.16320853729561866\n",
            "Progress: 79.2% ... Training loss: 0.058 ... Validation loss: 0.1630.058454737480076194\n",
            "0.16963392817503803\n",
            "Progress: 79.2% ... Training loss: 0.058 ... Validation loss: 0.1690.05730368074401277\n",
            "0.14648185231717156\n",
            "Progress: 79.3% ... Training loss: 0.057 ... Validation loss: 0.1460.059193985429142976\n",
            "0.14573553867445432\n",
            "Progress: 79.3% ... Training loss: 0.059 ... Validation loss: 0.1450.05859485643996466\n",
            "0.15097655480961353\n",
            "Progress: 79.3% ... Training loss: 0.058 ... Validation loss: 0.1500.05770598016134807\n",
            "0.15435915641253187\n",
            "Progress: 79.3% ... Training loss: 0.057 ... Validation loss: 0.1540.06838017120773239\n",
            "0.16693675363585103\n",
            "Progress: 79.3% ... Training loss: 0.068 ... Validation loss: 0.1660.05763181631638774\n",
            "0.1551595289910547\n",
            "Progress: 79.4% ... Training loss: 0.057 ... Validation loss: 0.1550.05926882207485587\n",
            "0.14768409790524245\n",
            "Progress: 79.4% ... Training loss: 0.059 ... Validation loss: 0.1470.0594151895233492\n",
            "0.14775773052365102\n",
            "Progress: 79.4% ... Training loss: 0.059 ... Validation loss: 0.1470.05777603988317065\n",
            "0.15554360061760625\n",
            "Progress: 79.4% ... Training loss: 0.057 ... Validation loss: 0.1550.0586999044202007\n",
            "0.17226953285266136\n",
            "Progress: 79.4% ... Training loss: 0.058 ... Validation loss: 0.1720.05847035214364083\n",
            "0.17152127017176982\n",
            "Progress: 79.5% ... Training loss: 0.058 ... Validation loss: 0.1710.06705923292683409\n",
            "0.17206252645978845\n",
            "Progress: 79.5% ... Training loss: 0.067 ... Validation loss: 0.1720.05913721321190938\n",
            "0.15201211092567096\n",
            "Progress: 79.5% ... Training loss: 0.059 ... Validation loss: 0.1520.05815757806024356\n",
            "0.1518532506675389\n",
            "Progress: 79.5% ... Training loss: 0.058 ... Validation loss: 0.1510.06084358020066273\n",
            "0.16646930540241897\n",
            "Progress: 79.5% ... Training loss: 0.060 ... Validation loss: 0.1660.05775529885491628\n",
            "0.16171234559857056\n",
            "Progress: 79.6% ... Training loss: 0.057 ... Validation loss: 0.1610.06167111258331242\n",
            "0.14508762060964842\n",
            "Progress: 79.6% ... Training loss: 0.061 ... Validation loss: 0.1450.06347639741466282\n",
            "0.13905002851661613\n",
            "Progress: 79.6% ... Training loss: 0.063 ... Validation loss: 0.1390.05868514657507194\n",
            "0.16839334779058576\n",
            "Progress: 79.6% ... Training loss: 0.058 ... Validation loss: 0.1680.05796765551178147\n",
            "0.1613614112657522\n",
            "Progress: 79.6% ... Training loss: 0.057 ... Validation loss: 0.1610.06801555715955128\n",
            "0.14794245799468841\n",
            "Progress: 79.7% ... Training loss: 0.068 ... Validation loss: 0.1470.06057042967291814\n",
            "0.14963432612565866\n",
            "Progress: 79.7% ... Training loss: 0.060 ... Validation loss: 0.1490.05934874650931246\n",
            "0.16154954606993902\n",
            "Progress: 79.7% ... Training loss: 0.059 ... Validation loss: 0.1610.05789482516307256\n",
            "0.1605245024119335\n",
            "Progress: 79.7% ... Training loss: 0.057 ... Validation loss: 0.1600.05766858277400503\n",
            "0.15722368838941045\n",
            "Progress: 79.7% ... Training loss: 0.057 ... Validation loss: 0.1570.05755694592433215\n",
            "0.15225929849944997\n",
            "Progress: 79.8% ... Training loss: 0.057 ... Validation loss: 0.1520.057866947857590854\n",
            "0.1602382513289126\n",
            "Progress: 79.8% ... Training loss: 0.057 ... Validation loss: 0.1600.06029077108883229\n",
            "0.14338105671812468\n",
            "Progress: 79.8% ... Training loss: 0.060 ... Validation loss: 0.1430.05825607023760414\n",
            "0.15650148147646212\n",
            "Progress: 79.8% ... Training loss: 0.058 ... Validation loss: 0.1560.05831715544462653\n",
            "0.1498702710258219\n",
            "Progress: 79.8% ... Training loss: 0.058 ... Validation loss: 0.1490.05855655005571375\n",
            "0.14541427880916682\n",
            "Progress: 79.9% ... Training loss: 0.058 ... Validation loss: 0.1450.05971687153625523\n",
            "0.17618409443974908\n",
            "Progress: 79.9% ... Training loss: 0.059 ... Validation loss: 0.1760.0581365199488442\n",
            "0.16477463655989\n",
            "Progress: 79.9% ... Training loss: 0.058 ... Validation loss: 0.1640.05803059909207189\n",
            "0.1775787912858051\n",
            "Progress: 79.9% ... Training loss: 0.058 ... Validation loss: 0.1770.06007418202489568\n",
            "0.15895714060693727\n",
            "Progress: 79.9% ... Training loss: 0.060 ... Validation loss: 0.1580.06439282541420135\n",
            "0.14938182867687252\n",
            "Progress: 80.0% ... Training loss: 0.064 ... Validation loss: 0.1490.05753271106096269\n",
            "0.15768149434223672\n",
            "Progress: 80.0% ... Training loss: 0.057 ... Validation loss: 0.1570.05994545489725274\n",
            "0.17524883948812767\n",
            "Progress: 80.0% ... Training loss: 0.059 ... Validation loss: 0.1750.05928005123376484\n",
            "0.1766018923568543\n",
            "Progress: 80.0% ... Training loss: 0.059 ... Validation loss: 0.1760.05893034380446917\n",
            "0.16581467575038875\n",
            "Progress: 80.0% ... Training loss: 0.058 ... Validation loss: 0.1650.06480029282145099\n",
            "0.19721111884853013\n",
            "Progress: 80.1% ... Training loss: 0.064 ... Validation loss: 0.1970.05869875084720482\n",
            "0.1663632066756526\n",
            "Progress: 80.1% ... Training loss: 0.058 ... Validation loss: 0.1660.06293783530351867\n",
            "0.17360887982580336\n",
            "Progress: 80.1% ... Training loss: 0.062 ... Validation loss: 0.1730.06017209435537483\n",
            "0.15094807209920325\n",
            "Progress: 80.1% ... Training loss: 0.060 ... Validation loss: 0.1500.059587372269469134\n",
            "0.14399958037216692\n",
            "Progress: 80.1% ... Training loss: 0.059 ... Validation loss: 0.1430.06169588010688071\n",
            "0.17089644012326421\n",
            "Progress: 80.2% ... Training loss: 0.061 ... Validation loss: 0.1700.058484627311632904\n",
            "0.1490353922567113\n",
            "Progress: 80.2% ... Training loss: 0.058 ... Validation loss: 0.1490.06467900673757657\n",
            "0.15569262113283777\n",
            "Progress: 80.2% ... Training loss: 0.064 ... Validation loss: 0.1550.059877255515083556\n",
            "0.14671605953584976\n",
            "Progress: 80.2% ... Training loss: 0.059 ... Validation loss: 0.1460.0593923253478859\n",
            "0.17462909388851702\n",
            "Progress: 80.2% ... Training loss: 0.059 ... Validation loss: 0.1740.06165271688430541\n",
            "0.1639842826642463\n",
            "Progress: 80.3% ... Training loss: 0.061 ... Validation loss: 0.1630.06113950283028565\n",
            "0.17086062349922038\n",
            "Progress: 80.3% ... Training loss: 0.061 ... Validation loss: 0.1700.05824268257059143\n",
            "0.15925132509456222\n",
            "Progress: 80.3% ... Training loss: 0.058 ... Validation loss: 0.1590.06603865458651244\n",
            "0.1594758558125743\n",
            "Progress: 80.3% ... Training loss: 0.066 ... Validation loss: 0.1590.05810001755503646\n",
            "0.17733936096653777\n",
            "Progress: 80.3% ... Training loss: 0.058 ... Validation loss: 0.1770.0614121504660588\n",
            "0.15559721254931042\n",
            "Progress: 80.4% ... Training loss: 0.061 ... Validation loss: 0.1550.057604481745199144\n",
            "0.1517033788930983\n",
            "Progress: 80.4% ... Training loss: 0.057 ... Validation loss: 0.1510.05849119092818655\n",
            "0.16317143739187692\n",
            "Progress: 80.4% ... Training loss: 0.058 ... Validation loss: 0.1630.06053084520606401\n",
            "0.16468422330156998\n",
            "Progress: 80.4% ... Training loss: 0.060 ... Validation loss: 0.1640.061970556163367344\n",
            "0.16982857307785557\n",
            "Progress: 80.4% ... Training loss: 0.061 ... Validation loss: 0.1690.05819040985314679\n",
            "0.14824431329061388\n",
            "Progress: 80.5% ... Training loss: 0.058 ... Validation loss: 0.1480.0590211764739491\n",
            "0.1462759465222902\n",
            "Progress: 80.5% ... Training loss: 0.059 ... Validation loss: 0.1460.057166938616553566\n",
            "0.1541893078317524\n",
            "Progress: 80.5% ... Training loss: 0.057 ... Validation loss: 0.1540.0602298441188777\n",
            "0.14975400634097236\n",
            "Progress: 80.5% ... Training loss: 0.060 ... Validation loss: 0.1490.05747934561691325\n",
            "0.15020603728044105\n",
            "Progress: 80.5% ... Training loss: 0.057 ... Validation loss: 0.1500.062329737419008185\n",
            "0.15047818049342304\n",
            "Progress: 80.6% ... Training loss: 0.062 ... Validation loss: 0.1500.058331658178532184\n",
            "0.15825304372850194\n",
            "Progress: 80.6% ... Training loss: 0.058 ... Validation loss: 0.1580.057210292765647826\n",
            "0.16135329571693188\n",
            "Progress: 80.6% ... Training loss: 0.057 ... Validation loss: 0.1610.06017788110890305\n",
            "0.1724337884793802\n",
            "Progress: 80.6% ... Training loss: 0.060 ... Validation loss: 0.1720.05824859245464574\n",
            "0.14852834917947932\n",
            "Progress: 80.6% ... Training loss: 0.058 ... Validation loss: 0.1480.06000546050067161\n",
            "0.1423395053362215\n",
            "Progress: 80.7% ... Training loss: 0.060 ... Validation loss: 0.1420.05762711777056654\n",
            "0.1445223139935015\n",
            "Progress: 80.7% ... Training loss: 0.057 ... Validation loss: 0.1440.0580693612045184\n",
            "0.16210575002542\n",
            "Progress: 80.7% ... Training loss: 0.058 ... Validation loss: 0.1620.05990587231015954\n",
            "0.17118350535001522\n",
            "Progress: 80.7% ... Training loss: 0.059 ... Validation loss: 0.1710.05938070709389727\n",
            "0.16882852828111838\n",
            "Progress: 80.7% ... Training loss: 0.059 ... Validation loss: 0.1680.057987953157062816\n",
            "0.15290501304012827\n",
            "Progress: 80.8% ... Training loss: 0.057 ... Validation loss: 0.1520.0654843584484511\n",
            "0.1875840000141798\n",
            "Progress: 80.8% ... Training loss: 0.065 ... Validation loss: 0.1870.05919802849672184\n",
            "0.15741698635641985\n",
            "Progress: 80.8% ... Training loss: 0.059 ... Validation loss: 0.1570.06120938895347755\n",
            "0.1595327048485863\n",
            "Progress: 80.8% ... Training loss: 0.061 ... Validation loss: 0.1590.059041683965365514\n",
            "0.15901336348054845\n",
            "Progress: 80.8% ... Training loss: 0.059 ... Validation loss: 0.1590.06128083269645395\n",
            "0.1757988552795336\n",
            "Progress: 80.9% ... Training loss: 0.061 ... Validation loss: 0.1750.06062621879151895\n",
            "0.1540485616764657\n",
            "Progress: 80.9% ... Training loss: 0.060 ... Validation loss: 0.1540.05882555891346465\n",
            "0.13916295909752002\n",
            "Progress: 80.9% ... Training loss: 0.058 ... Validation loss: 0.1390.05857959067629432\n",
            "0.149052233167302\n",
            "Progress: 80.9% ... Training loss: 0.058 ... Validation loss: 0.1490.062284245460185815\n",
            "0.1711479338777066\n",
            "Progress: 80.9% ... Training loss: 0.062 ... Validation loss: 0.1710.059487804083760185\n",
            "0.14812729263214744\n",
            "Progress: 81.0% ... Training loss: 0.059 ... Validation loss: 0.1480.0579766334384006\n",
            "0.15224220171861508\n",
            "Progress: 81.0% ... Training loss: 0.057 ... Validation loss: 0.1520.058619737283798885\n",
            "0.14759954693061994\n",
            "Progress: 81.0% ... Training loss: 0.058 ... Validation loss: 0.1470.05983221119005336\n",
            "0.15438795202704614\n",
            "Progress: 81.0% ... Training loss: 0.059 ... Validation loss: 0.1540.056804345447752015\n",
            "0.1536512101143943\n",
            "Progress: 81.0% ... Training loss: 0.056 ... Validation loss: 0.1530.06066535346542645\n",
            "0.13980062120812387\n",
            "Progress: 81.1% ... Training loss: 0.060 ... Validation loss: 0.1390.05877322097509912\n",
            "0.14880508633620299\n",
            "Progress: 81.1% ... Training loss: 0.058 ... Validation loss: 0.1480.059497917599165694\n",
            "0.15268971114341753\n",
            "Progress: 81.1% ... Training loss: 0.059 ... Validation loss: 0.1520.06419270397923614\n",
            "0.1440529157558469\n",
            "Progress: 81.1% ... Training loss: 0.064 ... Validation loss: 0.1440.05873154574973593\n",
            "0.15608331306437698\n",
            "Progress: 81.1% ... Training loss: 0.058 ... Validation loss: 0.1560.06047275854861309\n",
            "0.1546679300902957\n",
            "Progress: 81.2% ... Training loss: 0.060 ... Validation loss: 0.1540.061081120035022404\n",
            "0.18172902016151582\n",
            "Progress: 81.2% ... Training loss: 0.061 ... Validation loss: 0.1810.05756286271175884\n",
            "0.16035426389641294\n",
            "Progress: 81.2% ... Training loss: 0.057 ... Validation loss: 0.1600.05953432327842336\n",
            "0.15486999169903995\n",
            "Progress: 81.2% ... Training loss: 0.059 ... Validation loss: 0.1540.06510878804873489\n",
            "0.1934970771930425\n",
            "Progress: 81.2% ... Training loss: 0.065 ... Validation loss: 0.1930.05999786215562678\n",
            "0.14727469558749012\n",
            "Progress: 81.3% ... Training loss: 0.059 ... Validation loss: 0.1470.05783524115256196\n",
            "0.15485546782971435\n",
            "Progress: 81.3% ... Training loss: 0.057 ... Validation loss: 0.1540.05837338051513798\n",
            "0.1519384878462867\n",
            "Progress: 81.3% ... Training loss: 0.058 ... Validation loss: 0.1510.05963723396953834\n",
            "0.1384005133461352\n",
            "Progress: 81.3% ... Training loss: 0.059 ... Validation loss: 0.1380.05998978290335114\n",
            "0.15117516128708408\n",
            "Progress: 81.3% ... Training loss: 0.059 ... Validation loss: 0.1510.05880267624865449\n",
            "0.14177984324248033\n",
            "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.1410.05729045826139084\n",
            "0.1471780502939323\n",
            "Progress: 81.4% ... Training loss: 0.057 ... Validation loss: 0.1470.058165663708919686\n",
            "0.14587838396086075\n",
            "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.1450.059789566065093946\n",
            "0.14718517795544808\n",
            "Progress: 81.4% ... Training loss: 0.059 ... Validation loss: 0.1470.0575315159601249\n",
            "0.1488543892566061\n",
            "Progress: 81.4% ... Training loss: 0.057 ... Validation loss: 0.1480.05730092607524718\n",
            "0.1392810328672502\n",
            "Progress: 81.5% ... Training loss: 0.057 ... Validation loss: 0.1390.05699266659870455\n",
            "0.14423376437541957\n",
            "Progress: 81.5% ... Training loss: 0.056 ... Validation loss: 0.1440.05874236899804518\n",
            "0.15264827000567888\n",
            "Progress: 81.5% ... Training loss: 0.058 ... Validation loss: 0.1520.058576835210233984\n",
            "0.16324560741471147\n",
            "Progress: 81.5% ... Training loss: 0.058 ... Validation loss: 0.1630.0610549125354451\n",
            "0.1733384932059486\n",
            "Progress: 81.5% ... Training loss: 0.061 ... Validation loss: 0.1730.05782413489788147\n",
            "0.15256020697595926\n",
            "Progress: 81.6% ... Training loss: 0.057 ... Validation loss: 0.1520.06241177696414823\n",
            "0.17714265408888402\n",
            "Progress: 81.6% ... Training loss: 0.062 ... Validation loss: 0.1770.06058840375981088\n",
            "0.163395022265334\n",
            "Progress: 81.6% ... Training loss: 0.060 ... Validation loss: 0.1630.060812073142693344\n",
            "0.15880466139850608\n",
            "Progress: 81.6% ... Training loss: 0.060 ... Validation loss: 0.1580.0568541574652521\n",
            "0.14865289253105773\n",
            "Progress: 81.6% ... Training loss: 0.056 ... Validation loss: 0.1480.059551787926441446\n",
            "0.16912183229855426\n",
            "Progress: 81.7% ... Training loss: 0.059 ... Validation loss: 0.1690.06093670397357467\n",
            "0.1570184296840574\n",
            "Progress: 81.7% ... Training loss: 0.060 ... Validation loss: 0.1570.059569493130807276\n",
            "0.1604601837870265\n",
            "Progress: 81.7% ... Training loss: 0.059 ... Validation loss: 0.1600.057206990240683334\n",
            "0.1427784334012853\n",
            "Progress: 81.7% ... Training loss: 0.057 ... Validation loss: 0.1420.0576932881120905\n",
            "0.14709920386249792\n",
            "Progress: 81.7% ... Training loss: 0.057 ... Validation loss: 0.1470.05783741360145193\n",
            "0.15997149081162235\n",
            "Progress: 81.8% ... Training loss: 0.057 ... Validation loss: 0.1590.060524498682705166\n",
            "0.14094673213725994\n",
            "Progress: 81.8% ... Training loss: 0.060 ... Validation loss: 0.1400.05843047990003065\n",
            "0.14997987363209567\n",
            "Progress: 81.8% ... Training loss: 0.058 ... Validation loss: 0.1490.05812979675566199\n",
            "0.13658841144625178\n",
            "Progress: 81.8% ... Training loss: 0.058 ... Validation loss: 0.1360.05756873974153848\n",
            "0.1347349295317526\n",
            "Progress: 81.8% ... Training loss: 0.057 ... Validation loss: 0.1340.06797414689364886\n",
            "0.16442314480401946\n",
            "Progress: 81.9% ... Training loss: 0.067 ... Validation loss: 0.1640.06184692149353135\n",
            "0.1490400588164173\n",
            "Progress: 81.9% ... Training loss: 0.061 ... Validation loss: 0.1490.06253764495942214\n",
            "0.13269875330336572\n",
            "Progress: 81.9% ... Training loss: 0.062 ... Validation loss: 0.1320.06042442479804988\n",
            "0.1544102099980043\n",
            "Progress: 81.9% ... Training loss: 0.060 ... Validation loss: 0.1540.06019278528029131\n",
            "0.14630678948842438\n",
            "Progress: 81.9% ... Training loss: 0.060 ... Validation loss: 0.1460.05820579559289604\n",
            "0.14596981035139567\n",
            "Progress: 82.0% ... Training loss: 0.058 ... Validation loss: 0.1450.0597495928453951\n",
            "0.1387834624147367\n",
            "Progress: 82.0% ... Training loss: 0.059 ... Validation loss: 0.1380.06312851604403952\n",
            "0.1403096868256798\n",
            "Progress: 82.0% ... Training loss: 0.063 ... Validation loss: 0.1400.05720996672791795\n",
            "0.1440096615544164\n",
            "Progress: 82.0% ... Training loss: 0.057 ... Validation loss: 0.1440.05750952249280966\n",
            "0.1430396456626028\n",
            "Progress: 82.0% ... Training loss: 0.057 ... Validation loss: 0.1430.06855517862949613\n",
            "0.1731812923514207\n",
            "Progress: 82.1% ... Training loss: 0.068 ... Validation loss: 0.1730.058077829581324764\n",
            "0.1363644448653595\n",
            "Progress: 82.1% ... Training loss: 0.058 ... Validation loss: 0.1360.058623185414627\n",
            "0.1424644392169022\n",
            "Progress: 82.1% ... Training loss: 0.058 ... Validation loss: 0.1420.05916829732695824\n",
            "0.14815670790360508\n",
            "Progress: 82.1% ... Training loss: 0.059 ... Validation loss: 0.1480.05859959484617774\n",
            "0.13900787182514793\n",
            "Progress: 82.1% ... Training loss: 0.058 ... Validation loss: 0.1390.05710896572566216\n",
            "0.1461829602610662\n",
            "Progress: 82.2% ... Training loss: 0.057 ... Validation loss: 0.1460.07345032510654417\n",
            "0.14351128532554963\n",
            "Progress: 82.2% ... Training loss: 0.073 ... Validation loss: 0.1430.061499801995991706\n",
            "0.14190829709517755\n",
            "Progress: 82.2% ... Training loss: 0.061 ... Validation loss: 0.1410.057826749862390274\n",
            "0.13770858965904947\n",
            "Progress: 82.2% ... Training loss: 0.057 ... Validation loss: 0.1370.06114059726448505\n",
            "0.13859745808694132\n",
            "Progress: 82.2% ... Training loss: 0.061 ... Validation loss: 0.1380.05902795936575803\n",
            "0.14108001285823363\n",
            "Progress: 82.3% ... Training loss: 0.059 ... Validation loss: 0.1410.06442148229178446\n",
            "0.15371324824850038\n",
            "Progress: 82.3% ... Training loss: 0.064 ... Validation loss: 0.1530.06032331379846066\n",
            "0.16038241035979858\n",
            "Progress: 82.3% ... Training loss: 0.060 ... Validation loss: 0.1600.07685612513121823\n",
            "0.21104387807202984\n",
            "Progress: 82.3% ... Training loss: 0.076 ... Validation loss: 0.2110.06463645261088581\n",
            "0.14580674771554428\n",
            "Progress: 82.3% ... Training loss: 0.064 ... Validation loss: 0.1450.06577753873969933\n",
            "0.1381923852568437\n",
            "Progress: 82.4% ... Training loss: 0.065 ... Validation loss: 0.1380.06246400988387945\n",
            "0.14378130333714323\n",
            "Progress: 82.4% ... Training loss: 0.062 ... Validation loss: 0.1430.059708758564581285\n",
            "0.15363893720014188\n",
            "Progress: 82.4% ... Training loss: 0.059 ... Validation loss: 0.1530.06119584471221288\n",
            "0.14411402851085353\n",
            "Progress: 82.4% ... Training loss: 0.061 ... Validation loss: 0.1440.06250141683769465\n",
            "0.14298358325316807\n",
            "Progress: 82.4% ... Training loss: 0.062 ... Validation loss: 0.1420.05883043297599274\n",
            "0.15092934896898536\n",
            "Progress: 82.5% ... Training loss: 0.058 ... Validation loss: 0.1500.05920901407851112\n",
            "0.13883024637370348\n",
            "Progress: 82.5% ... Training loss: 0.059 ... Validation loss: 0.1380.06036061208124608\n",
            "0.146084225016317\n",
            "Progress: 82.5% ... Training loss: 0.060 ... Validation loss: 0.1460.05755098980239367\n",
            "0.15733553098308745\n",
            "Progress: 82.5% ... Training loss: 0.057 ... Validation loss: 0.1570.058183782050586524\n",
            "0.14719294700296753\n",
            "Progress: 82.5% ... Training loss: 0.058 ... Validation loss: 0.1470.06309467669867738\n",
            "0.1400992943369252\n",
            "Progress: 82.6% ... Training loss: 0.063 ... Validation loss: 0.1400.05787553223662088\n",
            "0.15169262637591094\n",
            "Progress: 82.6% ... Training loss: 0.057 ... Validation loss: 0.1510.059117083061625615\n",
            "0.14885667598840946\n",
            "Progress: 82.6% ... Training loss: 0.059 ... Validation loss: 0.1480.058360670239042116\n",
            "0.139232195970724\n",
            "Progress: 82.6% ... Training loss: 0.058 ... Validation loss: 0.1390.06887573080403822\n",
            "0.19152196096933227\n",
            "Progress: 82.6% ... Training loss: 0.068 ... Validation loss: 0.1910.05772865039095388\n",
            "0.14871433899910672\n",
            "Progress: 82.7% ... Training loss: 0.057 ... Validation loss: 0.1480.061801185179332876\n",
            "0.17941104502070138\n",
            "Progress: 82.7% ... Training loss: 0.061 ... Validation loss: 0.1790.057752352932818767\n",
            "0.14950206591189835\n",
            "Progress: 82.7% ... Training loss: 0.057 ... Validation loss: 0.1490.06151782027172312\n",
            "0.14235021922763966\n",
            "Progress: 82.7% ... Training loss: 0.061 ... Validation loss: 0.1420.05697692914738152\n",
            "0.14949893066466455\n",
            "Progress: 82.7% ... Training loss: 0.056 ... Validation loss: 0.1490.05808247589908543\n",
            "0.15892213726809024\n",
            "Progress: 82.8% ... Training loss: 0.058 ... Validation loss: 0.1580.0688733768607983\n",
            "0.14367797030284674\n",
            "Progress: 82.8% ... Training loss: 0.068 ... Validation loss: 0.1430.057756175077266186\n",
            "0.1590946259921357\n",
            "Progress: 82.8% ... Training loss: 0.057 ... Validation loss: 0.1590.0577791992986179\n",
            "0.14874862699005062\n",
            "Progress: 82.8% ... Training loss: 0.057 ... Validation loss: 0.1480.05918817205668129\n",
            "0.14907791088568315\n",
            "Progress: 82.8% ... Training loss: 0.059 ... Validation loss: 0.1490.059260534702632005\n",
            "0.1595323719820506\n",
            "Progress: 82.9% ... Training loss: 0.059 ... Validation loss: 0.1590.059899466517178715\n",
            "0.14261797683158894\n",
            "Progress: 82.9% ... Training loss: 0.059 ... Validation loss: 0.1420.08818963512394483\n",
            "0.2515751483188935\n",
            "Progress: 82.9% ... Training loss: 0.088 ... Validation loss: 0.2510.06016305950318486\n",
            "0.16531162111134007\n",
            "Progress: 82.9% ... Training loss: 0.060 ... Validation loss: 0.1650.05771467635470718\n",
            "0.14971661416809343\n",
            "Progress: 82.9% ... Training loss: 0.057 ... Validation loss: 0.1490.058033890068801774\n",
            "0.1421890561923399\n",
            "Progress: 83.0% ... Training loss: 0.058 ... Validation loss: 0.1420.061666845729846806\n",
            "0.14184692541600957\n",
            "Progress: 83.0% ... Training loss: 0.061 ... Validation loss: 0.1410.05972708472566192\n",
            "0.14685754908728338\n",
            "Progress: 83.0% ... Training loss: 0.059 ... Validation loss: 0.1460.058543004116066284\n",
            "0.15665307239833356\n",
            "Progress: 83.0% ... Training loss: 0.058 ... Validation loss: 0.1560.05853077329493105\n",
            "0.1456070605760592\n",
            "Progress: 83.0% ... Training loss: 0.058 ... Validation loss: 0.1450.05983261450231029\n",
            "0.17003214004308803\n",
            "Progress: 83.1% ... Training loss: 0.059 ... Validation loss: 0.1700.06720703081625372\n",
            "0.1734306306014606\n",
            "Progress: 83.1% ... Training loss: 0.067 ... Validation loss: 0.1730.05832339505376047\n",
            "0.14806637926484414\n",
            "Progress: 83.1% ... Training loss: 0.058 ... Validation loss: 0.1480.05963435999383924\n",
            "0.1380046936183575\n",
            "Progress: 83.1% ... Training loss: 0.059 ... Validation loss: 0.1380.05730907946578358\n",
            "0.14997897103553615\n",
            "Progress: 83.1% ... Training loss: 0.057 ... Validation loss: 0.1490.06035390038553687\n",
            "0.14702499914478187\n",
            "Progress: 83.2% ... Training loss: 0.060 ... Validation loss: 0.1470.05999293408093478\n",
            "0.17303041242223863\n",
            "Progress: 83.2% ... Training loss: 0.059 ... Validation loss: 0.1730.05917588000959764\n",
            "0.17101378575067888\n",
            "Progress: 83.2% ... Training loss: 0.059 ... Validation loss: 0.1710.05788283112311761\n",
            "0.15388586106474916\n",
            "Progress: 83.2% ... Training loss: 0.057 ... Validation loss: 0.1530.0629123988974353\n",
            "0.1698862183899263\n",
            "Progress: 83.2% ... Training loss: 0.062 ... Validation loss: 0.1690.059505855756095315\n",
            "0.14852262450364095\n",
            "Progress: 83.3% ... Training loss: 0.059 ... Validation loss: 0.1480.057952038548468074\n",
            "0.14460462762185322\n",
            "Progress: 83.3% ... Training loss: 0.057 ... Validation loss: 0.1440.057302933033149796\n",
            "0.1466881120103774\n",
            "Progress: 83.3% ... Training loss: 0.057 ... Validation loss: 0.1460.05933812169848857\n",
            "0.1434052181525414\n",
            "Progress: 83.3% ... Training loss: 0.059 ... Validation loss: 0.1430.05772125897053446\n",
            "0.14955066043354684\n",
            "Progress: 83.3% ... Training loss: 0.057 ... Validation loss: 0.1490.05835279284658483\n",
            "0.15179907267547862\n",
            "Progress: 83.4% ... Training loss: 0.058 ... Validation loss: 0.1510.05951149107074785\n",
            "0.15839447578869964\n",
            "Progress: 83.4% ... Training loss: 0.059 ... Validation loss: 0.1580.05774316015669337\n",
            "0.14041047445529206\n",
            "Progress: 83.4% ... Training loss: 0.057 ... Validation loss: 0.1400.05863676512049466\n",
            "0.14812582530901297\n",
            "Progress: 83.4% ... Training loss: 0.058 ... Validation loss: 0.1480.06176723262301376\n",
            "0.14600222596519385\n",
            "Progress: 83.4% ... Training loss: 0.061 ... Validation loss: 0.1460.06131557220905974\n",
            "0.1540360176688496\n",
            "Progress: 83.5% ... Training loss: 0.061 ... Validation loss: 0.1540.05826974185097974\n",
            "0.1478732683279037\n",
            "Progress: 83.5% ... Training loss: 0.058 ... Validation loss: 0.1470.05796964050915872\n",
            "0.14141285780754187\n",
            "Progress: 83.5% ... Training loss: 0.057 ... Validation loss: 0.1410.05854392992659919\n",
            "0.1524138068032233\n",
            "Progress: 83.5% ... Training loss: 0.058 ... Validation loss: 0.1520.0578792052969648\n",
            "0.13635762797668868\n",
            "Progress: 83.5% ... Training loss: 0.057 ... Validation loss: 0.1360.05708682574729836\n",
            "0.13831121474118316\n",
            "Progress: 83.6% ... Training loss: 0.057 ... Validation loss: 0.1380.058201713373793505\n",
            "0.15821970553579792\n",
            "Progress: 83.6% ... Training loss: 0.058 ... Validation loss: 0.1580.06185945745309771\n",
            "0.14982077892975376\n",
            "Progress: 83.6% ... Training loss: 0.061 ... Validation loss: 0.1490.05882276996329794\n",
            "0.1412543002413387\n",
            "Progress: 83.6% ... Training loss: 0.058 ... Validation loss: 0.1410.06692579664205166\n",
            "0.14747333524842776\n",
            "Progress: 83.6% ... Training loss: 0.066 ... Validation loss: 0.1470.05784981847140599\n",
            "0.15353401574481496\n",
            "Progress: 83.7% ... Training loss: 0.057 ... Validation loss: 0.1530.060259403074042026\n",
            "0.15276749274771145\n",
            "Progress: 83.7% ... Training loss: 0.060 ... Validation loss: 0.1520.06033807211336098\n",
            "0.14341740359102334\n",
            "Progress: 83.7% ... Training loss: 0.060 ... Validation loss: 0.1430.057824312283667266\n",
            "0.14544425521758023\n",
            "Progress: 83.7% ... Training loss: 0.057 ... Validation loss: 0.1450.05782122565559151\n",
            "0.13982354859564003\n",
            "Progress: 83.7% ... Training loss: 0.057 ... Validation loss: 0.1390.05748575223623079\n",
            "0.1477253138523205\n",
            "Progress: 83.8% ... Training loss: 0.057 ... Validation loss: 0.1470.05839040903669106\n",
            "0.1369108708083174\n",
            "Progress: 83.8% ... Training loss: 0.058 ... Validation loss: 0.1360.058021921076783244\n",
            "0.13388379386423582\n",
            "Progress: 83.8% ... Training loss: 0.058 ... Validation loss: 0.1330.05763220416959781\n",
            "0.13534334897307632\n",
            "Progress: 83.8% ... Training loss: 0.057 ... Validation loss: 0.1350.05672853248169462\n",
            "0.13987666991024483\n",
            "Progress: 83.8% ... Training loss: 0.056 ... Validation loss: 0.1390.06637577420991748\n",
            "0.1517151539387514\n",
            "Progress: 83.9% ... Training loss: 0.066 ... Validation loss: 0.1510.05896547423622628\n",
            "0.13769923407029466\n",
            "Progress: 83.9% ... Training loss: 0.058 ... Validation loss: 0.1370.05971576811360647\n",
            "0.14225093905285285\n",
            "Progress: 83.9% ... Training loss: 0.059 ... Validation loss: 0.1420.05739164760539401\n",
            "0.13766894309635072\n",
            "Progress: 83.9% ... Training loss: 0.057 ... Validation loss: 0.1370.06301455680072056\n",
            "0.1422777262065289\n",
            "Progress: 83.9% ... Training loss: 0.063 ... Validation loss: 0.1420.0613976565050082\n",
            "0.15018910320176518\n",
            "Progress: 84.0% ... Training loss: 0.061 ... Validation loss: 0.1500.05708783028695735\n",
            "0.14163516391323633\n",
            "Progress: 84.0% ... Training loss: 0.057 ... Validation loss: 0.1410.05803733905298812\n",
            "0.14558057251725054\n",
            "Progress: 84.0% ... Training loss: 0.058 ... Validation loss: 0.1450.058747400610265295\n",
            "0.1332085826146616\n",
            "Progress: 84.0% ... Training loss: 0.058 ... Validation loss: 0.1330.060440752485808824\n",
            "0.13966430645068886\n",
            "Progress: 84.0% ... Training loss: 0.060 ... Validation loss: 0.1390.05867639814715823\n",
            "0.1496358927433194\n",
            "Progress: 84.1% ... Training loss: 0.058 ... Validation loss: 0.1490.05782158067334642\n",
            "0.14208393005660958\n",
            "Progress: 84.1% ... Training loss: 0.057 ... Validation loss: 0.1420.0590614834078489\n",
            "0.13845071849132543\n",
            "Progress: 84.1% ... Training loss: 0.059 ... Validation loss: 0.1380.057226483862184346\n",
            "0.14519383188548113\n",
            "Progress: 84.1% ... Training loss: 0.057 ... Validation loss: 0.1450.05939055544189986\n",
            "0.13224974141635684\n",
            "Progress: 84.1% ... Training loss: 0.059 ... Validation loss: 0.1320.0651229702216369\n",
            "0.1389581427786705\n",
            "Progress: 84.2% ... Training loss: 0.065 ... Validation loss: 0.1380.061287453626791555\n",
            "0.15014206254123763\n",
            "Progress: 84.2% ... Training loss: 0.061 ... Validation loss: 0.1500.06007665970392873\n",
            "0.15548796454877778\n",
            "Progress: 84.2% ... Training loss: 0.060 ... Validation loss: 0.1550.05774046913493205\n",
            "0.1567430032160572\n",
            "Progress: 84.2% ... Training loss: 0.057 ... Validation loss: 0.1560.06296762858434887\n",
            "0.13766633846435672\n",
            "Progress: 84.2% ... Training loss: 0.062 ... Validation loss: 0.1370.058374104797679294\n",
            "0.15537280464432762\n",
            "Progress: 84.3% ... Training loss: 0.058 ... Validation loss: 0.1550.05776552338859478\n",
            "0.1480795397549671\n",
            "Progress: 84.3% ... Training loss: 0.057 ... Validation loss: 0.1480.05837477242696038\n",
            "0.1423638362783917\n",
            "Progress: 84.3% ... Training loss: 0.058 ... Validation loss: 0.1420.05745114452632614\n",
            "0.14730766189456992\n",
            "Progress: 84.3% ... Training loss: 0.057 ... Validation loss: 0.1470.05851485907561024\n",
            "0.1484631477366324\n",
            "Progress: 84.3% ... Training loss: 0.058 ... Validation loss: 0.1480.057460880603775515\n",
            "0.14737700260779\n",
            "Progress: 84.4% ... Training loss: 0.057 ... Validation loss: 0.1470.059450245610818776\n",
            "0.14423152849782273\n",
            "Progress: 84.4% ... Training loss: 0.059 ... Validation loss: 0.1440.05930729877872355\n",
            "0.1483111692313722\n",
            "Progress: 84.4% ... Training loss: 0.059 ... Validation loss: 0.1480.0576148915860666\n",
            "0.15310283306514047\n",
            "Progress: 84.4% ... Training loss: 0.057 ... Validation loss: 0.1530.05755847730907232\n",
            "0.15394177247820048\n",
            "Progress: 84.4% ... Training loss: 0.057 ... Validation loss: 0.1530.0573479536504258\n",
            "0.15195576292515992\n",
            "Progress: 84.5% ... Training loss: 0.057 ... Validation loss: 0.1510.05907377083765111\n",
            "0.1415539527836722\n",
            "Progress: 84.5% ... Training loss: 0.059 ... Validation loss: 0.1410.05674411566748526\n",
            "0.14713406105098767\n",
            "Progress: 84.5% ... Training loss: 0.056 ... Validation loss: 0.1470.058472101657267475\n",
            "0.14259163311063322\n",
            "Progress: 84.5% ... Training loss: 0.058 ... Validation loss: 0.1420.06251481752568845\n",
            "0.1806069365767469\n",
            "Progress: 84.5% ... Training loss: 0.062 ... Validation loss: 0.1800.06338980779785072\n",
            "0.17994309163958774\n",
            "Progress: 84.6% ... Training loss: 0.063 ... Validation loss: 0.1790.058865163609798085\n",
            "0.14899835822256283\n",
            "Progress: 84.6% ... Training loss: 0.058 ... Validation loss: 0.1480.05988209105469037\n",
            "0.1738768444204749\n",
            "Progress: 84.6% ... Training loss: 0.059 ... Validation loss: 0.1730.05960323475490395\n",
            "0.15904734877074847\n",
            "Progress: 84.6% ... Training loss: 0.059 ... Validation loss: 0.1590.05906288756451966\n",
            "0.1776927256216745\n",
            "Progress: 84.6% ... Training loss: 0.059 ... Validation loss: 0.1770.060290937722506294\n",
            "0.19962074452009398\n",
            "Progress: 84.7% ... Training loss: 0.060 ... Validation loss: 0.1990.06156472481634936\n",
            "0.17451149522278364\n",
            "Progress: 84.7% ... Training loss: 0.061 ... Validation loss: 0.1740.05761464179992497\n",
            "0.1433154587178191\n",
            "Progress: 84.7% ... Training loss: 0.057 ... Validation loss: 0.1430.0598778858644544\n",
            "0.14368961358217544\n",
            "Progress: 84.7% ... Training loss: 0.059 ... Validation loss: 0.1430.05711073151014437\n",
            "0.14668808602975805\n",
            "Progress: 84.7% ... Training loss: 0.057 ... Validation loss: 0.1460.05716757200362782\n",
            "0.15619908261213367\n",
            "Progress: 84.8% ... Training loss: 0.057 ... Validation loss: 0.1560.06372459380705396\n",
            "0.18332492082803836\n",
            "Progress: 84.8% ... Training loss: 0.063 ... Validation loss: 0.1830.05842438778038293\n",
            "0.14544233527024797\n",
            "Progress: 84.8% ... Training loss: 0.058 ... Validation loss: 0.1450.05955988873682342\n",
            "0.14959890598611125\n",
            "Progress: 84.8% ... Training loss: 0.059 ... Validation loss: 0.1490.05665730792642356\n",
            "0.1530044828818771\n",
            "Progress: 84.8% ... Training loss: 0.056 ... Validation loss: 0.1530.05760848272433548\n",
            "0.15452776557333606\n",
            "Progress: 84.9% ... Training loss: 0.057 ... Validation loss: 0.1540.05742737306776522\n",
            "0.1436214581056913\n",
            "Progress: 84.9% ... Training loss: 0.057 ... Validation loss: 0.1430.06182858382495337\n",
            "0.1386873961044465\n",
            "Progress: 84.9% ... Training loss: 0.061 ... Validation loss: 0.1380.06093062300909077\n",
            "0.14645378144695545\n",
            "Progress: 84.9% ... Training loss: 0.060 ... Validation loss: 0.1460.05707659880165288\n",
            "0.14616712309417143\n",
            "Progress: 84.9% ... Training loss: 0.057 ... Validation loss: 0.1460.05990813579658337\n",
            "0.15702678734394895\n",
            "Progress: 85.0% ... Training loss: 0.059 ... Validation loss: 0.1570.06035566919278451\n",
            "0.15125141902003614\n",
            "Progress: 85.0% ... Training loss: 0.060 ... Validation loss: 0.1510.06527620653998148\n",
            "0.16012351745827683\n",
            "Progress: 85.0% ... Training loss: 0.065 ... Validation loss: 0.1600.05688190680818605\n",
            "0.14692007100879723\n",
            "Progress: 85.0% ... Training loss: 0.056 ... Validation loss: 0.1460.05894241067234661\n",
            "0.14282588761959888\n",
            "Progress: 85.0% ... Training loss: 0.058 ... Validation loss: 0.1420.06562773709635554\n",
            "0.14700501456890502\n",
            "Progress: 85.1% ... Training loss: 0.065 ... Validation loss: 0.1470.06140561017792685\n",
            "0.1561030975889681\n",
            "Progress: 85.1% ... Training loss: 0.061 ... Validation loss: 0.1560.05986524402961599\n",
            "0.14159867556679598\n",
            "Progress: 85.1% ... Training loss: 0.059 ... Validation loss: 0.1410.06159652786831383\n",
            "0.15844941087471323\n",
            "Progress: 85.1% ... Training loss: 0.061 ... Validation loss: 0.1580.05888777387368715\n",
            "0.1419368566099293\n",
            "Progress: 85.1% ... Training loss: 0.058 ... Validation loss: 0.1410.05694699551034058\n",
            "0.14988956408887338\n",
            "Progress: 85.2% ... Training loss: 0.056 ... Validation loss: 0.1490.06129802465551594\n",
            "0.14293800518576413\n",
            "Progress: 85.2% ... Training loss: 0.061 ... Validation loss: 0.1420.06054921742252167\n",
            "0.1502058239394134\n",
            "Progress: 85.2% ... Training loss: 0.060 ... Validation loss: 0.1500.058165938391458026\n",
            "0.1411008874341712\n",
            "Progress: 85.2% ... Training loss: 0.058 ... Validation loss: 0.1410.061092924733864955\n",
            "0.13186755525363264\n",
            "Progress: 85.2% ... Training loss: 0.061 ... Validation loss: 0.1310.05737157027731602\n",
            "0.14587108295579274\n",
            "Progress: 85.3% ... Training loss: 0.057 ... Validation loss: 0.1450.05894198404146176\n",
            "0.14034198311941345\n",
            "Progress: 85.3% ... Training loss: 0.058 ... Validation loss: 0.1400.06826829712554654\n",
            "0.17499141929813591\n",
            "Progress: 85.3% ... Training loss: 0.068 ... Validation loss: 0.1740.06541940364356849\n",
            "0.14676567924278006\n",
            "Progress: 85.3% ... Training loss: 0.065 ... Validation loss: 0.1460.05895552282840726\n",
            "0.14615535697594484\n",
            "Progress: 85.3% ... Training loss: 0.058 ... Validation loss: 0.1460.05801900209268736\n",
            "0.14758801492401122\n",
            "Progress: 85.4% ... Training loss: 0.058 ... Validation loss: 0.1470.06056473872447939\n",
            "0.1357892091491926\n",
            "Progress: 85.4% ... Training loss: 0.060 ... Validation loss: 0.1350.057052665822064454\n",
            "0.14745592436346314\n",
            "Progress: 85.4% ... Training loss: 0.057 ... Validation loss: 0.1470.05735544010608248\n",
            "0.14011809352505822\n",
            "Progress: 85.4% ... Training loss: 0.057 ... Validation loss: 0.1400.08169234669243935\n",
            "0.13978941898150257\n",
            "Progress: 85.4% ... Training loss: 0.081 ... Validation loss: 0.1390.06186971230726926\n",
            "0.1295167555035858\n",
            "Progress: 85.5% ... Training loss: 0.061 ... Validation loss: 0.1290.05703978203896664\n",
            "0.13183544514899304\n",
            "Progress: 85.5% ... Training loss: 0.057 ... Validation loss: 0.1310.057366207494125396\n",
            "0.12868041392859036\n",
            "Progress: 85.5% ... Training loss: 0.057 ... Validation loss: 0.1280.05678070118159815\n",
            "0.139271571774485\n",
            "Progress: 85.5% ... Training loss: 0.056 ... Validation loss: 0.1390.06355474649254325\n",
            "0.1770747210604452\n",
            "Progress: 85.5% ... Training loss: 0.063 ... Validation loss: 0.1770.06103148403920133\n",
            "0.1640172403471294\n",
            "Progress: 85.6% ... Training loss: 0.061 ... Validation loss: 0.1640.05647464555590288\n",
            "0.14126206047496348\n",
            "Progress: 85.6% ... Training loss: 0.056 ... Validation loss: 0.1410.05892004239995994\n",
            "0.14215137483649684\n",
            "Progress: 85.6% ... Training loss: 0.058 ... Validation loss: 0.1420.057840936159672465\n",
            "0.14402378099739094\n",
            "Progress: 85.6% ... Training loss: 0.057 ... Validation loss: 0.1440.07154775510934325\n",
            "0.1339992402154197\n",
            "Progress: 85.6% ... Training loss: 0.071 ... Validation loss: 0.1330.05820209600073668\n",
            "0.15038817803831847\n",
            "Progress: 85.7% ... Training loss: 0.058 ... Validation loss: 0.1500.06027838708225567\n",
            "0.16632113876064306\n",
            "Progress: 85.7% ... Training loss: 0.060 ... Validation loss: 0.1660.06242881588736927\n",
            "0.15663643861486526\n",
            "Progress: 85.7% ... Training loss: 0.062 ... Validation loss: 0.1560.05832568753967746\n",
            "0.13842229191721722\n",
            "Progress: 85.7% ... Training loss: 0.058 ... Validation loss: 0.1380.05751376073542229\n",
            "0.14668215468597445\n",
            "Progress: 85.7% ... Training loss: 0.057 ... Validation loss: 0.1460.05768145351349149\n",
            "0.1361857435603113\n",
            "Progress: 85.8% ... Training loss: 0.057 ... Validation loss: 0.1360.056926727906647615\n",
            "0.1415384732022291\n",
            "Progress: 85.8% ... Training loss: 0.056 ... Validation loss: 0.1410.061512266409387485\n",
            "0.13722447961478249\n",
            "Progress: 85.8% ... Training loss: 0.061 ... Validation loss: 0.1370.05729816675083721\n",
            "0.14412036690252927\n",
            "Progress: 85.8% ... Training loss: 0.057 ... Validation loss: 0.1440.059711596808908154\n",
            "0.1357123442338794\n",
            "Progress: 85.8% ... Training loss: 0.059 ... Validation loss: 0.1350.059344938066405194\n",
            "0.1333346955157396\n",
            "Progress: 85.9% ... Training loss: 0.059 ... Validation loss: 0.1330.05709756838796103\n",
            "0.14004507691448265\n",
            "Progress: 85.9% ... Training loss: 0.057 ... Validation loss: 0.1400.057877336892126145\n",
            "0.14389537724043888\n",
            "Progress: 85.9% ... Training loss: 0.057 ... Validation loss: 0.1430.065366302201883\n",
            "0.15554389543981514\n",
            "Progress: 85.9% ... Training loss: 0.065 ... Validation loss: 0.1550.06968540442112693\n",
            "0.14090851147671565\n",
            "Progress: 85.9% ... Training loss: 0.069 ... Validation loss: 0.1400.05828319119432646\n",
            "0.13950417985533972\n",
            "Progress: 86.0% ... Training loss: 0.058 ... Validation loss: 0.1390.05793659554010619\n",
            "0.13525947209666658\n",
            "Progress: 86.0% ... Training loss: 0.057 ... Validation loss: 0.1350.05629275805161833\n",
            "0.13944350753315862\n",
            "Progress: 86.0% ... Training loss: 0.056 ... Validation loss: 0.1390.06053300922604321\n",
            "0.14421907743205153\n",
            "Progress: 86.0% ... Training loss: 0.060 ... Validation loss: 0.1440.0576803523296464\n",
            "0.13748250360100486\n",
            "Progress: 86.0% ... Training loss: 0.057 ... Validation loss: 0.1370.05926216991916621\n",
            "0.13075750399412867\n",
            "Progress: 86.1% ... Training loss: 0.059 ... Validation loss: 0.1300.05858930489080552\n",
            "0.13783243204668297\n",
            "Progress: 86.1% ... Training loss: 0.058 ... Validation loss: 0.1370.05671799455985865\n",
            "0.1372849554171849\n",
            "Progress: 86.1% ... Training loss: 0.056 ... Validation loss: 0.1370.057937261568110146\n",
            "0.13170461910657486\n",
            "Progress: 86.1% ... Training loss: 0.057 ... Validation loss: 0.1310.05968163029325875\n",
            "0.13082706281675752\n",
            "Progress: 86.1% ... Training loss: 0.059 ... Validation loss: 0.1300.06140489850684104\n",
            "0.12871563048265824\n",
            "Progress: 86.2% ... Training loss: 0.061 ... Validation loss: 0.1280.06590017355667466\n",
            "0.1311270235030337\n",
            "Progress: 86.2% ... Training loss: 0.065 ... Validation loss: 0.1310.0576888943016374\n",
            "0.13344650338769443\n",
            "Progress: 86.2% ... Training loss: 0.057 ... Validation loss: 0.1330.059530351198028714\n",
            "0.14203489827127633\n",
            "Progress: 86.2% ... Training loss: 0.059 ... Validation loss: 0.1420.05740000720475717\n",
            "0.13742952061257704\n",
            "Progress: 86.2% ... Training loss: 0.057 ... Validation loss: 0.1370.060865898377604054\n",
            "0.15021143245673965\n",
            "Progress: 86.3% ... Training loss: 0.060 ... Validation loss: 0.1500.0626846145526322\n",
            "0.1380470091983444\n",
            "Progress: 86.3% ... Training loss: 0.062 ... Validation loss: 0.1380.061608185773177414\n",
            "0.17557836877950422\n",
            "Progress: 86.3% ... Training loss: 0.061 ... Validation loss: 0.1750.06222692441317941\n",
            "0.1400828460689666\n",
            "Progress: 86.3% ... Training loss: 0.062 ... Validation loss: 0.1400.05719408358386602\n",
            "0.1475396003606336\n",
            "Progress: 86.3% ... Training loss: 0.057 ... Validation loss: 0.1470.05846437982096896\n",
            "0.14753010249417364\n",
            "Progress: 86.4% ... Training loss: 0.058 ... Validation loss: 0.1470.06074626472737582\n",
            "0.15081021206836306\n",
            "Progress: 86.4% ... Training loss: 0.060 ... Validation loss: 0.1500.05832156910475723\n",
            "0.1618535873299194\n",
            "Progress: 86.4% ... Training loss: 0.058 ... Validation loss: 0.1610.06119827573316352\n",
            "0.15346402072723803\n",
            "Progress: 86.4% ... Training loss: 0.061 ... Validation loss: 0.1530.058687517645150226\n",
            "0.1549529730489323\n",
            "Progress: 86.4% ... Training loss: 0.058 ... Validation loss: 0.1540.058905156570892875\n",
            "0.14056921012992316\n",
            "Progress: 86.5% ... Training loss: 0.058 ... Validation loss: 0.1400.05922407035631677\n",
            "0.14615369343406648\n",
            "Progress: 86.5% ... Training loss: 0.059 ... Validation loss: 0.1460.06170151213637802\n",
            "0.14374531421711012\n",
            "Progress: 86.5% ... Training loss: 0.061 ... Validation loss: 0.1430.06257583239499961\n",
            "0.1535652115694702\n",
            "Progress: 86.5% ... Training loss: 0.062 ... Validation loss: 0.1530.05760292581941144\n",
            "0.14687380924683374\n",
            "Progress: 86.5% ... Training loss: 0.057 ... Validation loss: 0.1460.05952961331379237\n",
            "0.13951490140816766\n",
            "Progress: 86.6% ... Training loss: 0.059 ... Validation loss: 0.1390.057295921931846246\n",
            "0.15097490676250497\n",
            "Progress: 86.6% ... Training loss: 0.057 ... Validation loss: 0.1500.0578685537248925\n",
            "0.15680423891269296\n",
            "Progress: 86.6% ... Training loss: 0.057 ... Validation loss: 0.1560.07738880649399878\n",
            "0.12919587095154816\n",
            "Progress: 86.6% ... Training loss: 0.077 ... Validation loss: 0.1290.06786631310908268\n",
            "0.20321943856678817\n",
            "Progress: 86.6% ... Training loss: 0.067 ... Validation loss: 0.2030.059728653134202066\n",
            "0.14381499123181327\n",
            "Progress: 86.7% ... Training loss: 0.059 ... Validation loss: 0.1430.057892854131804915\n",
            "0.1465501610678735\n",
            "Progress: 86.7% ... Training loss: 0.057 ... Validation loss: 0.1460.05985897642533382\n",
            "0.15239109485330668\n",
            "Progress: 86.7% ... Training loss: 0.059 ... Validation loss: 0.1520.057462861349416396\n",
            "0.14767145168625223\n",
            "Progress: 86.7% ... Training loss: 0.057 ... Validation loss: 0.1470.05705399485072189\n",
            "0.13897708287165433\n",
            "Progress: 86.7% ... Training loss: 0.057 ... Validation loss: 0.1380.06318779960025392\n",
            "0.1338453609048423\n",
            "Progress: 86.8% ... Training loss: 0.063 ... Validation loss: 0.1330.05945935858307095\n",
            "0.13019979388563896\n",
            "Progress: 86.8% ... Training loss: 0.059 ... Validation loss: 0.1300.0585794399294475\n",
            "0.1331264203048721\n",
            "Progress: 86.8% ... Training loss: 0.058 ... Validation loss: 0.1330.0590377061562039\n",
            "0.13674082646043528\n",
            "Progress: 86.8% ... Training loss: 0.059 ... Validation loss: 0.1360.057758150165749186\n",
            "0.14219489840061889\n",
            "Progress: 86.8% ... Training loss: 0.057 ... Validation loss: 0.1420.06145192196427887\n",
            "0.1405759989964731\n",
            "Progress: 86.9% ... Training loss: 0.061 ... Validation loss: 0.1400.0616839286170565\n",
            "0.14515488090245796\n",
            "Progress: 86.9% ... Training loss: 0.061 ... Validation loss: 0.1450.058951609704269985\n",
            "0.1540308398580335\n",
            "Progress: 86.9% ... Training loss: 0.058 ... Validation loss: 0.1540.060061219286964944\n",
            "0.1415870893019891\n",
            "Progress: 86.9% ... Training loss: 0.060 ... Validation loss: 0.1410.05663522531830301\n",
            "0.14466093291614832\n",
            "Progress: 86.9% ... Training loss: 0.056 ... Validation loss: 0.1440.062281234512386305\n",
            "0.13931902788519573\n",
            "Progress: 87.0% ... Training loss: 0.062 ... Validation loss: 0.1390.056604451246810225\n",
            "0.15146225314668676\n",
            "Progress: 87.0% ... Training loss: 0.056 ... Validation loss: 0.1510.059683321901869354\n",
            "0.17434308951060903\n",
            "Progress: 87.0% ... Training loss: 0.059 ... Validation loss: 0.1740.05640294122922279\n",
            "0.14769949834583956\n",
            "Progress: 87.0% ... Training loss: 0.056 ... Validation loss: 0.1470.05640303825808977\n",
            "0.15652343967968413\n",
            "Progress: 87.0% ... Training loss: 0.056 ... Validation loss: 0.1560.056996830503020034\n",
            "0.14012910472788004\n",
            "Progress: 87.1% ... Training loss: 0.056 ... Validation loss: 0.1400.05626644581467117\n",
            "0.149916884151163\n",
            "Progress: 87.1% ... Training loss: 0.056 ... Validation loss: 0.1490.05723475806868346\n",
            "0.15897815301398147\n",
            "Progress: 87.1% ... Training loss: 0.057 ... Validation loss: 0.1580.05713278749404358\n",
            "0.15210774786790687\n",
            "Progress: 87.1% ... Training loss: 0.057 ... Validation loss: 0.1520.057645610017979285\n",
            "0.1467989124500547\n",
            "Progress: 87.1% ... Training loss: 0.057 ... Validation loss: 0.1460.057802818866956034\n",
            "0.15034570126156271\n",
            "Progress: 87.2% ... Training loss: 0.057 ... Validation loss: 0.1500.05648095360617291\n",
            "0.13386287965325083\n",
            "Progress: 87.2% ... Training loss: 0.056 ... Validation loss: 0.1330.057036399386820556\n",
            "0.1411629133348257\n",
            "Progress: 87.2% ... Training loss: 0.057 ... Validation loss: 0.1410.05853787009171021\n",
            "0.14405001233898945\n",
            "Progress: 87.2% ... Training loss: 0.058 ... Validation loss: 0.1440.05743723245717828\n",
            "0.13829031666782965\n",
            "Progress: 87.2% ... Training loss: 0.057 ... Validation loss: 0.1380.05905009209534637\n",
            "0.1406333494230737\n",
            "Progress: 87.3% ... Training loss: 0.059 ... Validation loss: 0.1400.05819955989124693\n",
            "0.14587657722781064\n",
            "Progress: 87.3% ... Training loss: 0.058 ... Validation loss: 0.1450.06850664727675651\n",
            "0.13108701171278725\n",
            "Progress: 87.3% ... Training loss: 0.068 ... Validation loss: 0.1310.05946800634553897\n",
            "0.14982563008374705\n",
            "Progress: 87.3% ... Training loss: 0.059 ... Validation loss: 0.1490.05877916214571057\n",
            "0.14437425543478477\n",
            "Progress: 87.3% ... Training loss: 0.058 ... Validation loss: 0.1440.05686021418536789\n",
            "0.15005482780481863\n",
            "Progress: 87.4% ... Training loss: 0.056 ... Validation loss: 0.1500.057672488114949357\n",
            "0.13514936869420713\n",
            "Progress: 87.4% ... Training loss: 0.057 ... Validation loss: 0.1350.056972875885236834\n",
            "0.13305201222151486\n",
            "Progress: 87.4% ... Training loss: 0.056 ... Validation loss: 0.1330.058518880659410716\n",
            "0.1458265622723704\n",
            "Progress: 87.4% ... Training loss: 0.058 ... Validation loss: 0.1450.06362078755210517\n",
            "0.1324235936083993\n",
            "Progress: 87.4% ... Training loss: 0.063 ... Validation loss: 0.1320.05958264811982688\n",
            "0.13998909288776026\n",
            "Progress: 87.5% ... Training loss: 0.059 ... Validation loss: 0.1390.061623689564168714\n",
            "0.141539674888733\n",
            "Progress: 87.5% ... Training loss: 0.061 ... Validation loss: 0.1410.06377424938920712\n",
            "0.14527217342708834\n",
            "Progress: 87.5% ... Training loss: 0.063 ... Validation loss: 0.1450.059130974017924054\n",
            "0.1436254024465753\n",
            "Progress: 87.5% ... Training loss: 0.059 ... Validation loss: 0.1430.05764504469403091\n",
            "0.13750286164521108\n",
            "Progress: 87.5% ... Training loss: 0.057 ... Validation loss: 0.1370.05749807169648249\n",
            "0.14432368881456392\n",
            "Progress: 87.6% ... Training loss: 0.057 ... Validation loss: 0.1440.0574188831656959\n",
            "0.14491674799509144\n",
            "Progress: 87.6% ... Training loss: 0.057 ... Validation loss: 0.1440.05743569583020781\n",
            "0.14240864698965297\n",
            "Progress: 87.6% ... Training loss: 0.057 ... Validation loss: 0.1420.0791582223784983\n",
            "0.17690096731081112\n",
            "Progress: 87.6% ... Training loss: 0.079 ... Validation loss: 0.1760.05743332621292154\n",
            "0.15371400680863007\n",
            "Progress: 87.6% ... Training loss: 0.057 ... Validation loss: 0.1530.06565477507354943\n",
            "0.1462320976713831\n",
            "Progress: 87.7% ... Training loss: 0.065 ... Validation loss: 0.1460.06024106729630169\n",
            "0.15058197216695182\n",
            "Progress: 87.7% ... Training loss: 0.060 ... Validation loss: 0.1500.05805937904566195\n",
            "0.16830559034037865\n",
            "Progress: 87.7% ... Training loss: 0.058 ... Validation loss: 0.1680.057479080261399855\n",
            "0.15187136344048086\n",
            "Progress: 87.7% ... Training loss: 0.057 ... Validation loss: 0.1510.06147192742332507\n",
            "0.13555137284911567\n",
            "Progress: 87.7% ... Training loss: 0.061 ... Validation loss: 0.1350.05910370275536798\n",
            "0.1591510594889786\n",
            "Progress: 87.8% ... Training loss: 0.059 ... Validation loss: 0.1590.05764957359359105\n",
            "0.14951081177637954\n",
            "Progress: 87.8% ... Training loss: 0.057 ... Validation loss: 0.1490.05976749355516243\n",
            "0.15264898196961246\n",
            "Progress: 87.8% ... Training loss: 0.059 ... Validation loss: 0.1520.06104897146807383\n",
            "0.161972413786623\n",
            "Progress: 87.8% ... Training loss: 0.061 ... Validation loss: 0.1610.06023876618628479\n",
            "0.14484088474881776\n",
            "Progress: 87.8% ... Training loss: 0.060 ... Validation loss: 0.1440.06457064727561208\n",
            "0.15444316394157584\n",
            "Progress: 87.9% ... Training loss: 0.064 ... Validation loss: 0.1540.059691263228134714\n",
            "0.1467643952549922\n",
            "Progress: 87.9% ... Training loss: 0.059 ... Validation loss: 0.1460.05666890489997329\n",
            "0.15269773039790416\n",
            "Progress: 87.9% ... Training loss: 0.056 ... Validation loss: 0.1520.0595619409025361\n",
            "0.15586460553962808\n",
            "Progress: 87.9% ... Training loss: 0.059 ... Validation loss: 0.1550.05755613111421809\n",
            "0.15177613517439442\n",
            "Progress: 87.9% ... Training loss: 0.057 ... Validation loss: 0.1510.0577908156818506\n",
            "0.14628103062658582\n",
            "Progress: 88.0% ... Training loss: 0.057 ... Validation loss: 0.1460.057042660394542546\n",
            "0.14973920638311605\n",
            "Progress: 88.0% ... Training loss: 0.057 ... Validation loss: 0.1490.057904102231022256\n",
            "0.15667070782421572\n",
            "Progress: 88.0% ... Training loss: 0.057 ... Validation loss: 0.1560.057415284073854884\n",
            "0.163035768409006\n",
            "Progress: 88.0% ... Training loss: 0.057 ... Validation loss: 0.1630.05727225828177864\n",
            "0.16897992576611578\n",
            "Progress: 88.0% ... Training loss: 0.057 ... Validation loss: 0.1680.05790668376112626\n",
            "0.17146415249262295\n",
            "Progress: 88.1% ... Training loss: 0.057 ... Validation loss: 0.1710.0596738161888309\n",
            "0.15264291967340127\n",
            "Progress: 88.1% ... Training loss: 0.059 ... Validation loss: 0.1520.058153252995682546\n",
            "0.13932840794922235\n",
            "Progress: 88.1% ... Training loss: 0.058 ... Validation loss: 0.1390.059801499414944705\n",
            "0.14124583429665075\n",
            "Progress: 88.1% ... Training loss: 0.059 ... Validation loss: 0.1410.06529372875740658\n",
            "0.13766562019091516\n",
            "Progress: 88.1% ... Training loss: 0.065 ... Validation loss: 0.1370.057025605884900445\n",
            "0.14080069871814058\n",
            "Progress: 88.2% ... Training loss: 0.057 ... Validation loss: 0.1400.05799338959037343\n",
            "0.15155368578292416\n",
            "Progress: 88.2% ... Training loss: 0.057 ... Validation loss: 0.1510.076695183076461\n",
            "0.14476806922291438\n",
            "Progress: 88.2% ... Training loss: 0.076 ... Validation loss: 0.1440.05679631036342699\n",
            "0.15033599518482713\n",
            "Progress: 88.2% ... Training loss: 0.056 ... Validation loss: 0.1500.057189567614485494\n",
            "0.16119443327142144\n",
            "Progress: 88.2% ... Training loss: 0.057 ... Validation loss: 0.1610.058664414338408674\n",
            "0.14453630264092723\n",
            "Progress: 88.3% ... Training loss: 0.058 ... Validation loss: 0.1440.06603826247842201\n",
            "0.13992408061789885\n",
            "Progress: 88.3% ... Training loss: 0.066 ... Validation loss: 0.1390.06524701905058966\n",
            "0.13786886082355618\n",
            "Progress: 88.3% ... Training loss: 0.065 ... Validation loss: 0.1370.05689400843365678\n",
            "0.15722868806543924\n",
            "Progress: 88.3% ... Training loss: 0.056 ... Validation loss: 0.1570.06332679931887854\n",
            "0.13984536339271728\n",
            "Progress: 88.3% ... Training loss: 0.063 ... Validation loss: 0.1390.06463267154285804\n",
            "0.139725981633422\n",
            "Progress: 88.4% ... Training loss: 0.064 ... Validation loss: 0.1390.057962494639676614\n",
            "0.16719465720549664\n",
            "Progress: 88.4% ... Training loss: 0.057 ... Validation loss: 0.1670.05714065894660888\n",
            "0.15956231437591828\n",
            "Progress: 88.4% ... Training loss: 0.057 ... Validation loss: 0.1590.05655963674167178\n",
            "0.14899631219607745\n",
            "Progress: 88.4% ... Training loss: 0.056 ... Validation loss: 0.1480.0580777700447596\n",
            "0.1472899375822053\n",
            "Progress: 88.4% ... Training loss: 0.058 ... Validation loss: 0.1470.06313952924656714\n",
            "0.15287515413203848\n",
            "Progress: 88.5% ... Training loss: 0.063 ... Validation loss: 0.1520.056008824506825414\n",
            "0.15086480779189454\n",
            "Progress: 88.5% ... Training loss: 0.056 ... Validation loss: 0.1500.056504945106897866\n",
            "0.1542477022901803\n",
            "Progress: 88.5% ... Training loss: 0.056 ... Validation loss: 0.1540.05686044803089775\n",
            "0.14951564168730483\n",
            "Progress: 88.5% ... Training loss: 0.056 ... Validation loss: 0.1490.05694783061532255\n",
            "0.1470368711042959\n",
            "Progress: 88.5% ... Training loss: 0.056 ... Validation loss: 0.1470.06423215779236933\n",
            "0.17839358858416046\n",
            "Progress: 88.6% ... Training loss: 0.064 ... Validation loss: 0.1780.05902147057419857\n",
            "0.15155939565082152\n",
            "Progress: 88.6% ... Training loss: 0.059 ... Validation loss: 0.1510.05750782725970116\n",
            "0.14609130459657352\n",
            "Progress: 88.6% ... Training loss: 0.057 ... Validation loss: 0.1460.05751956024242167\n",
            "0.14059451352920704\n",
            "Progress: 88.6% ... Training loss: 0.057 ... Validation loss: 0.1400.05850893070953513\n",
            "0.14606483713982782\n",
            "Progress: 88.6% ... Training loss: 0.058 ... Validation loss: 0.1460.06875369990393754\n",
            "0.13987203038125154\n",
            "Progress: 88.7% ... Training loss: 0.068 ... Validation loss: 0.1390.05759505971613011\n",
            "0.135381742544322\n",
            "Progress: 88.7% ... Training loss: 0.057 ... Validation loss: 0.1350.057105840541917086\n",
            "0.13605451234873267\n",
            "Progress: 88.7% ... Training loss: 0.057 ... Validation loss: 0.1360.05891343953274248\n",
            "0.13605714750330355\n",
            "Progress: 88.7% ... Training loss: 0.058 ... Validation loss: 0.1360.056724269596456565\n",
            "0.14704254255699403\n",
            "Progress: 88.7% ... Training loss: 0.056 ... Validation loss: 0.1470.05990059999583226\n",
            "0.1569517089648459\n",
            "Progress: 88.8% ... Training loss: 0.059 ... Validation loss: 0.1560.05918970752996144\n",
            "0.1565935571750007\n",
            "Progress: 88.8% ... Training loss: 0.059 ... Validation loss: 0.1560.056697401480096735\n",
            "0.1456390679383627\n",
            "Progress: 88.8% ... Training loss: 0.056 ... Validation loss: 0.1450.06557026562529678\n",
            "0.13594374094318387\n",
            "Progress: 88.8% ... Training loss: 0.065 ... Validation loss: 0.1350.06138783289621589\n",
            "0.1452906639506137\n",
            "Progress: 88.8% ... Training loss: 0.061 ... Validation loss: 0.1450.05956983781133386\n",
            "0.1310004768150872\n",
            "Progress: 88.9% ... Training loss: 0.059 ... Validation loss: 0.1310.05718764429435455\n",
            "0.13676719083739358\n",
            "Progress: 88.9% ... Training loss: 0.057 ... Validation loss: 0.1360.05730131099106899\n",
            "0.1423165698826689\n",
            "Progress: 88.9% ... Training loss: 0.057 ... Validation loss: 0.1420.05703631301032824\n",
            "0.13427216751284468\n",
            "Progress: 88.9% ... Training loss: 0.057 ... Validation loss: 0.1340.05893393552543073\n",
            "0.1610763607516093\n",
            "Progress: 88.9% ... Training loss: 0.058 ... Validation loss: 0.1610.05996630566444608\n",
            "0.16963945814945008\n",
            "Progress: 89.0% ... Training loss: 0.059 ... Validation loss: 0.1690.057562137013973835\n",
            "0.1513413208197347\n",
            "Progress: 89.0% ... Training loss: 0.057 ... Validation loss: 0.1510.05616673476234646\n",
            "0.16069967138862384\n",
            "Progress: 89.0% ... Training loss: 0.056 ... Validation loss: 0.1600.057008524424499996\n",
            "0.15335369862627649\n",
            "Progress: 89.0% ... Training loss: 0.057 ... Validation loss: 0.1530.05746676853345472\n",
            "0.15184621122212977\n",
            "Progress: 89.0% ... Training loss: 0.057 ... Validation loss: 0.1510.061358357300880934\n",
            "0.1782844884732482\n",
            "Progress: 89.1% ... Training loss: 0.061 ... Validation loss: 0.1780.057247617560789874\n",
            "0.14821073770523185\n",
            "Progress: 89.1% ... Training loss: 0.057 ... Validation loss: 0.1480.05737175921295398\n",
            "0.15948129205045045\n",
            "Progress: 89.1% ... Training loss: 0.057 ... Validation loss: 0.1590.05696977521354121\n",
            "0.1510196130663385\n",
            "Progress: 89.1% ... Training loss: 0.056 ... Validation loss: 0.1510.06035162580491166\n",
            "0.16087636920300086\n",
            "Progress: 89.1% ... Training loss: 0.060 ... Validation loss: 0.1600.058992150058893106\n",
            "0.1625877622581455\n",
            "Progress: 89.2% ... Training loss: 0.058 ... Validation loss: 0.1620.0565117908147239\n",
            "0.15299250215052287\n",
            "Progress: 89.2% ... Training loss: 0.056 ... Validation loss: 0.1520.05819671236098303\n",
            "0.14457597138781225\n",
            "Progress: 89.2% ... Training loss: 0.058 ... Validation loss: 0.1440.05707065402717009\n",
            "0.15971658670222802\n",
            "Progress: 89.2% ... Training loss: 0.057 ... Validation loss: 0.1590.057322092497160713\n",
            "0.1496955945405583\n",
            "Progress: 89.2% ... Training loss: 0.057 ... Validation loss: 0.1490.05794815690653051\n",
            "0.14350663065140423\n",
            "Progress: 89.3% ... Training loss: 0.057 ... Validation loss: 0.1430.07236572199816992\n",
            "0.1366604072614888\n",
            "Progress: 89.3% ... Training loss: 0.072 ... Validation loss: 0.1360.0593223556234088\n",
            "0.14526518220459014\n",
            "Progress: 89.3% ... Training loss: 0.059 ... Validation loss: 0.1450.05888979363059173\n",
            "0.14108518483420943\n",
            "Progress: 89.3% ... Training loss: 0.058 ... Validation loss: 0.1410.05777636753139333\n",
            "0.13667108966899014\n",
            "Progress: 89.3% ... Training loss: 0.057 ... Validation loss: 0.1360.056328809552710135\n",
            "0.13894313213356582\n",
            "Progress: 89.4% ... Training loss: 0.056 ... Validation loss: 0.1380.0626409001580229\n",
            "0.15717645093749508\n",
            "Progress: 89.4% ... Training loss: 0.062 ... Validation loss: 0.1570.0684317177258706\n",
            "0.16822350548879952\n",
            "Progress: 89.4% ... Training loss: 0.068 ... Validation loss: 0.1680.05684946590033015\n",
            "0.14071023991928508\n",
            "Progress: 89.4% ... Training loss: 0.056 ... Validation loss: 0.1400.07520415083686664\n",
            "0.19180109426039155\n",
            "Progress: 89.4% ... Training loss: 0.075 ... Validation loss: 0.1910.05912650513086029\n",
            "0.14951960207506773\n",
            "Progress: 89.5% ... Training loss: 0.059 ... Validation loss: 0.1490.061604880633021365\n",
            "0.17056201918194022\n",
            "Progress: 89.5% ... Training loss: 0.061 ... Validation loss: 0.1700.05756662850659216\n",
            "0.1337873690198613\n",
            "Progress: 89.5% ... Training loss: 0.057 ... Validation loss: 0.1330.06301354572932125\n",
            "0.16904655074621688\n",
            "Progress: 89.5% ... Training loss: 0.063 ... Validation loss: 0.1690.05826117018847663\n",
            "0.14713947332953847\n",
            "Progress: 89.5% ... Training loss: 0.058 ... Validation loss: 0.1470.061261415648008535\n",
            "0.16184624508538323\n",
            "Progress: 89.6% ... Training loss: 0.061 ... Validation loss: 0.1610.06255312242288184\n",
            "0.1371373824605695\n",
            "Progress: 89.6% ... Training loss: 0.062 ... Validation loss: 0.1370.05969808299679877\n",
            "0.14044562337483626\n",
            "Progress: 89.6% ... Training loss: 0.059 ... Validation loss: 0.1400.06800627034017612\n",
            "0.13296907813857117\n",
            "Progress: 89.6% ... Training loss: 0.068 ... Validation loss: 0.1320.0578635834937126\n",
            "0.1374844160148645\n",
            "Progress: 89.6% ... Training loss: 0.057 ... Validation loss: 0.1370.05827715182074525\n",
            "0.13537968985410012\n",
            "Progress: 89.7% ... Training loss: 0.058 ... Validation loss: 0.1350.062486860603133056\n",
            "0.16362841145752854\n",
            "Progress: 89.7% ... Training loss: 0.062 ... Validation loss: 0.1630.058698761238269684\n",
            "0.15761338881118803\n",
            "Progress: 89.7% ... Training loss: 0.058 ... Validation loss: 0.1570.0573848748494402\n",
            "0.13493336624802998\n",
            "Progress: 89.7% ... Training loss: 0.057 ... Validation loss: 0.1340.06446012080236559\n",
            "0.171187986701879\n",
            "Progress: 89.7% ... Training loss: 0.064 ... Validation loss: 0.1710.06076556725372814\n",
            "0.13011454075313497\n",
            "Progress: 89.8% ... Training loss: 0.060 ... Validation loss: 0.1300.05939435359016797\n",
            "0.13474859821082166\n",
            "Progress: 89.8% ... Training loss: 0.059 ... Validation loss: 0.1340.056685733289002074\n",
            "0.13558886760975253\n",
            "Progress: 89.8% ... Training loss: 0.056 ... Validation loss: 0.1350.06414394894519995\n",
            "0.13688160684009493\n",
            "Progress: 89.8% ... Training loss: 0.064 ... Validation loss: 0.1360.05885616703371077\n",
            "0.14437409270362325\n",
            "Progress: 89.8% ... Training loss: 0.058 ... Validation loss: 0.1440.058133539498213864\n",
            "0.13355271187975026\n",
            "Progress: 89.9% ... Training loss: 0.058 ... Validation loss: 0.1330.05955729342453826\n",
            "0.15130665607926816\n",
            "Progress: 89.9% ... Training loss: 0.059 ... Validation loss: 0.1510.05813039778544758\n",
            "0.1328955621546646\n",
            "Progress: 89.9% ... Training loss: 0.058 ... Validation loss: 0.1320.05732044005582693\n",
            "0.13902175191581925\n",
            "Progress: 89.9% ... Training loss: 0.057 ... Validation loss: 0.1390.05699203223937769\n",
            "0.13652282134778954\n",
            "Progress: 89.9% ... Training loss: 0.056 ... Validation loss: 0.1360.05938612061722637\n",
            "0.1470876001249049\n",
            "Progress: 90.0% ... Training loss: 0.059 ... Validation loss: 0.1470.05689382151440388\n",
            "0.13500871531718728\n",
            "Progress: 90.0% ... Training loss: 0.056 ... Validation loss: 0.1350.05781216809427403\n",
            "0.14968634194362349\n",
            "Progress: 90.0% ... Training loss: 0.057 ... Validation loss: 0.1490.05797677578756502\n",
            "0.15291273698238494\n",
            "Progress: 90.0% ... Training loss: 0.057 ... Validation loss: 0.1520.05589895094526233\n",
            "0.13459760752302555\n",
            "Progress: 90.0% ... Training loss: 0.055 ... Validation loss: 0.1340.05842607588806693\n",
            "0.14757665900279932\n",
            "Progress: 90.1% ... Training loss: 0.058 ... Validation loss: 0.1470.055623584824614394\n",
            "0.1385621347900322\n",
            "Progress: 90.1% ... Training loss: 0.055 ... Validation loss: 0.1380.059948966861947235\n",
            "0.13261120796614528\n",
            "Progress: 90.1% ... Training loss: 0.059 ... Validation loss: 0.1320.05968382129859408\n",
            "0.14408263643282576\n",
            "Progress: 90.1% ... Training loss: 0.059 ... Validation loss: 0.1440.05849235852258638\n",
            "0.1398921600950246\n",
            "Progress: 90.1% ... Training loss: 0.058 ... Validation loss: 0.1390.06461641195475729\n",
            "0.15020125970133386\n",
            "Progress: 90.2% ... Training loss: 0.064 ... Validation loss: 0.1500.06852566883761663\n",
            "0.18325067635189538\n",
            "Progress: 90.2% ... Training loss: 0.068 ... Validation loss: 0.1830.05680552232584246\n",
            "0.14109826587231775\n",
            "Progress: 90.2% ... Training loss: 0.056 ... Validation loss: 0.1410.05613788537158996\n",
            "0.13600549096712405\n",
            "Progress: 90.2% ... Training loss: 0.056 ... Validation loss: 0.1360.058881209120606626\n",
            "0.16314501515686158\n",
            "Progress: 90.2% ... Training loss: 0.058 ... Validation loss: 0.1630.06172481180566939\n",
            "0.15199700096733038\n",
            "Progress: 90.3% ... Training loss: 0.061 ... Validation loss: 0.1510.05612094116678322\n",
            "0.14625064048828323\n",
            "Progress: 90.3% ... Training loss: 0.056 ... Validation loss: 0.1460.05846653115415312\n",
            "0.1490245227473219\n",
            "Progress: 90.3% ... Training loss: 0.058 ... Validation loss: 0.1490.05720005992776162\n",
            "0.15017928986165566\n",
            "Progress: 90.3% ... Training loss: 0.057 ... Validation loss: 0.1500.056898799723354904\n",
            "0.14384419272084506\n",
            "Progress: 90.3% ... Training loss: 0.056 ... Validation loss: 0.1430.05750082530761005\n",
            "0.1645228681541001\n",
            "Progress: 90.4% ... Training loss: 0.057 ... Validation loss: 0.1640.05766302380826407\n",
            "0.14586296688419717\n",
            "Progress: 90.4% ... Training loss: 0.057 ... Validation loss: 0.1450.060252796244007464\n",
            "0.1449850250469368\n",
            "Progress: 90.4% ... Training loss: 0.060 ... Validation loss: 0.1440.06621537089353174\n",
            "0.18679312289911773\n",
            "Progress: 90.4% ... Training loss: 0.066 ... Validation loss: 0.1860.0599252451187046\n",
            "0.13757498150209507\n",
            "Progress: 90.4% ... Training loss: 0.059 ... Validation loss: 0.1370.06526821381124241\n",
            "0.17170362053619798\n",
            "Progress: 90.5% ... Training loss: 0.065 ... Validation loss: 0.1710.059051165108441765\n",
            "0.14419741839936298\n",
            "Progress: 90.5% ... Training loss: 0.059 ... Validation loss: 0.1440.05732569745896363\n",
            "0.14824350144946033\n",
            "Progress: 90.5% ... Training loss: 0.057 ... Validation loss: 0.1480.05878857844396054\n",
            "0.13760337122296457\n",
            "Progress: 90.5% ... Training loss: 0.058 ... Validation loss: 0.1370.056862345127832774\n",
            "0.14007914032694282\n",
            "Progress: 90.5% ... Training loss: 0.056 ... Validation loss: 0.1400.05609202223307997\n",
            "0.15033073952030482\n",
            "Progress: 90.6% ... Training loss: 0.056 ... Validation loss: 0.1500.05853234594843272\n",
            "0.14794753075435166\n",
            "Progress: 90.6% ... Training loss: 0.058 ... Validation loss: 0.1470.057123897150869685\n",
            "0.14236802204602794\n",
            "Progress: 90.6% ... Training loss: 0.057 ... Validation loss: 0.1420.058926087551741516\n",
            "0.14531840744386454\n",
            "Progress: 90.6% ... Training loss: 0.058 ... Validation loss: 0.1450.06061614455226447\n",
            "0.16739303732979707\n",
            "Progress: 90.6% ... Training loss: 0.060 ... Validation loss: 0.1670.05608736331584942\n",
            "0.14566459829398928\n",
            "Progress: 90.7% ... Training loss: 0.056 ... Validation loss: 0.1450.05785381585285278\n",
            "0.14781160849383454\n",
            "Progress: 90.7% ... Training loss: 0.057 ... Validation loss: 0.1470.06028915872514776\n",
            "0.17240195598832544\n",
            "Progress: 90.7% ... Training loss: 0.060 ... Validation loss: 0.1720.06207263519127658\n",
            "0.16857277181370775\n",
            "Progress: 90.7% ... Training loss: 0.062 ... Validation loss: 0.1680.060568810146863085\n",
            "0.17617143603130994\n",
            "Progress: 90.7% ... Training loss: 0.060 ... Validation loss: 0.1760.05593950330006574\n",
            "0.14385467146842157\n",
            "Progress: 90.8% ... Training loss: 0.055 ... Validation loss: 0.1430.06478315493111485\n",
            "0.16998718715098163\n",
            "Progress: 90.8% ... Training loss: 0.064 ... Validation loss: 0.1690.06321646657250304\n",
            "0.17860298381250184\n",
            "Progress: 90.8% ... Training loss: 0.063 ... Validation loss: 0.1780.05772940042523211\n",
            "0.14474994546190342\n",
            "Progress: 90.8% ... Training loss: 0.057 ... Validation loss: 0.1440.0633348051788263\n",
            "0.19627083627979422\n",
            "Progress: 90.8% ... Training loss: 0.063 ... Validation loss: 0.1960.056986749328264585\n",
            "0.16249524126977552\n",
            "Progress: 90.9% ... Training loss: 0.056 ... Validation loss: 0.1620.05662380652114374\n",
            "0.14418415445410385\n",
            "Progress: 90.9% ... Training loss: 0.056 ... Validation loss: 0.1440.05661278537725385\n",
            "0.15660381404085474\n",
            "Progress: 90.9% ... Training loss: 0.056 ... Validation loss: 0.1560.05615610289016851\n",
            "0.15023420685646313\n",
            "Progress: 90.9% ... Training loss: 0.056 ... Validation loss: 0.1500.05950956930781039\n",
            "0.17460259713343412\n",
            "Progress: 90.9% ... Training loss: 0.059 ... Validation loss: 0.1740.05849408328772896\n",
            "0.1514208210465434\n",
            "Progress: 91.0% ... Training loss: 0.058 ... Validation loss: 0.1510.05942393933430329\n",
            "0.18385548549628375\n",
            "Progress: 91.0% ... Training loss: 0.059 ... Validation loss: 0.1830.06084983999815197\n",
            "0.14210985034490042\n",
            "Progress: 91.0% ... Training loss: 0.060 ... Validation loss: 0.1420.05835164617525106\n",
            "0.14685349069312342\n",
            "Progress: 91.0% ... Training loss: 0.058 ... Validation loss: 0.1460.07185396315837513\n",
            "0.2125298193730087\n",
            "Progress: 91.0% ... Training loss: 0.071 ... Validation loss: 0.2120.058191613510833864\n",
            "0.15653326609741727\n",
            "Progress: 91.1% ... Training loss: 0.058 ... Validation loss: 0.1560.0639843969565388\n",
            "0.17968895508369134\n",
            "Progress: 91.1% ... Training loss: 0.063 ... Validation loss: 0.1790.06191497196174555\n",
            "0.13394834868511343\n",
            "Progress: 91.1% ... Training loss: 0.061 ... Validation loss: 0.1330.0565952976713259\n",
            "0.15484902100527426\n",
            "Progress: 91.1% ... Training loss: 0.056 ... Validation loss: 0.1540.056876806659293275\n",
            "0.15831021987882668\n",
            "Progress: 91.1% ... Training loss: 0.056 ... Validation loss: 0.1580.057488390209252914\n",
            "0.14238372588543355\n",
            "Progress: 91.2% ... Training loss: 0.057 ... Validation loss: 0.1420.056415874235062335\n",
            "0.14920298211197214\n",
            "Progress: 91.2% ... Training loss: 0.056 ... Validation loss: 0.1490.06205975637792069\n",
            "0.13868952029388884\n",
            "Progress: 91.2% ... Training loss: 0.062 ... Validation loss: 0.1380.08810063505472876\n",
            "0.19515795480266135\n",
            "Progress: 91.2% ... Training loss: 0.088 ... Validation loss: 0.1950.05748771525799712\n",
            "0.1488053211283625\n",
            "Progress: 91.2% ... Training loss: 0.057 ... Validation loss: 0.1480.05824434234864636\n",
            "0.14109995979441897\n",
            "Progress: 91.3% ... Training loss: 0.058 ... Validation loss: 0.1410.057278863740010524\n",
            "0.15119199484346993\n",
            "Progress: 91.3% ... Training loss: 0.057 ... Validation loss: 0.1510.05680233593842855\n",
            "0.14048048542134195\n",
            "Progress: 91.3% ... Training loss: 0.056 ... Validation loss: 0.1400.0623453181131261\n",
            "0.13813854125378505\n",
            "Progress: 91.3% ... Training loss: 0.062 ... Validation loss: 0.1380.056250836826540584\n",
            "0.13780877083134047\n",
            "Progress: 91.3% ... Training loss: 0.056 ... Validation loss: 0.1370.056412652527464774\n",
            "0.1449773749183039\n",
            "Progress: 91.4% ... Training loss: 0.056 ... Validation loss: 0.1440.08090348282257341\n",
            "0.18341098729985097\n",
            "Progress: 91.4% ... Training loss: 0.080 ... Validation loss: 0.1830.06381944087965341\n",
            "0.16199124388752656\n",
            "Progress: 91.4% ... Training loss: 0.063 ... Validation loss: 0.1610.05733124657378608\n",
            "0.1539159835385331\n",
            "Progress: 91.4% ... Training loss: 0.057 ... Validation loss: 0.1530.05784963827034265\n",
            "0.1541130534962467\n",
            "Progress: 91.4% ... Training loss: 0.057 ... Validation loss: 0.1540.057746707756706775\n",
            "0.16071041131483224\n",
            "Progress: 91.5% ... Training loss: 0.057 ... Validation loss: 0.1600.06011945543379182\n",
            "0.1416841171858708\n",
            "Progress: 91.5% ... Training loss: 0.060 ... Validation loss: 0.1410.057107815419624244\n",
            "0.14570011044057138\n",
            "Progress: 91.5% ... Training loss: 0.057 ... Validation loss: 0.1450.06068729379859511\n",
            "0.12993317983225813\n",
            "Progress: 91.5% ... Training loss: 0.060 ... Validation loss: 0.1290.061922642818248566\n",
            "0.16981438236113083\n",
            "Progress: 91.5% ... Training loss: 0.061 ... Validation loss: 0.1690.05636901324307886\n",
            "0.1431903846548743\n",
            "Progress: 91.6% ... Training loss: 0.056 ... Validation loss: 0.1430.05619205196194815\n",
            "0.14379174216006296\n",
            "Progress: 91.6% ... Training loss: 0.056 ... Validation loss: 0.1430.059330338942603894\n",
            "0.15424736438457406\n",
            "Progress: 91.6% ... Training loss: 0.059 ... Validation loss: 0.1540.056915521034873684\n",
            "0.14840830762742804\n",
            "Progress: 91.6% ... Training loss: 0.056 ... Validation loss: 0.1480.05942747414892579\n",
            "0.14880622679111818\n",
            "Progress: 91.6% ... Training loss: 0.059 ... Validation loss: 0.1480.06046490507107616\n",
            "0.1317667560559019\n",
            "Progress: 91.7% ... Training loss: 0.060 ... Validation loss: 0.1310.05727833714692981\n",
            "0.135828391085399\n",
            "Progress: 91.7% ... Training loss: 0.057 ... Validation loss: 0.1350.0656828155547184\n",
            "0.1691767435749844\n",
            "Progress: 91.7% ... Training loss: 0.065 ... Validation loss: 0.1690.05882698379587686\n",
            "0.14524561616377846\n",
            "Progress: 91.7% ... Training loss: 0.058 ... Validation loss: 0.1450.05737388281594046\n",
            "0.14372197912951243\n",
            "Progress: 91.7% ... Training loss: 0.057 ... Validation loss: 0.1430.06106509771717921\n",
            "0.15993542806782257\n",
            "Progress: 91.8% ... Training loss: 0.061 ... Validation loss: 0.1590.060967790798762904\n",
            "0.1326373466565568\n",
            "Progress: 91.8% ... Training loss: 0.060 ... Validation loss: 0.1320.0559101407032223\n",
            "0.13928516402093338\n",
            "Progress: 91.8% ... Training loss: 0.055 ... Validation loss: 0.1390.05672643391607683\n",
            "0.1330456094474822\n",
            "Progress: 91.8% ... Training loss: 0.056 ... Validation loss: 0.1330.060232657051266306\n",
            "0.1307964254301821\n",
            "Progress: 91.8% ... Training loss: 0.060 ... Validation loss: 0.1300.05922735987782489\n",
            "0.13844473502794727\n",
            "Progress: 91.9% ... Training loss: 0.059 ... Validation loss: 0.1380.05590666925037363\n",
            "0.14036629367791897\n",
            "Progress: 91.9% ... Training loss: 0.055 ... Validation loss: 0.1400.05778365831972412\n",
            "0.13286850356264612\n",
            "Progress: 91.9% ... Training loss: 0.057 ... Validation loss: 0.1320.06796460880053411\n",
            "0.16261772824053933\n",
            "Progress: 91.9% ... Training loss: 0.067 ... Validation loss: 0.1620.05642542270297064\n",
            "0.1375604216870437\n",
            "Progress: 91.9% ... Training loss: 0.056 ... Validation loss: 0.1370.057294486762432796\n",
            "0.13958694132957056\n",
            "Progress: 92.0% ... Training loss: 0.057 ... Validation loss: 0.1390.06711245220317529\n",
            "0.13632770820631618\n",
            "Progress: 92.0% ... Training loss: 0.067 ... Validation loss: 0.1360.05898185537696571\n",
            "0.1338314167508089\n",
            "Progress: 92.0% ... Training loss: 0.058 ... Validation loss: 0.1330.056176560344511635\n",
            "0.14666759854106054\n",
            "Progress: 92.0% ... Training loss: 0.056 ... Validation loss: 0.1460.05720703983582268\n",
            "0.14155916133896929\n",
            "Progress: 92.0% ... Training loss: 0.057 ... Validation loss: 0.1410.05698034912111512\n",
            "0.13590474525358878\n",
            "Progress: 92.1% ... Training loss: 0.056 ... Validation loss: 0.1350.05778557827719714\n",
            "0.1489935394415431\n",
            "Progress: 92.1% ... Training loss: 0.057 ... Validation loss: 0.1480.061044393657338214\n",
            "0.1674456923859388\n",
            "Progress: 92.1% ... Training loss: 0.061 ... Validation loss: 0.1670.060720888453553626\n",
            "0.13704546390402886\n",
            "Progress: 92.1% ... Training loss: 0.060 ... Validation loss: 0.1370.06108856833395811\n",
            "0.15009293894668516\n",
            "Progress: 92.1% ... Training loss: 0.061 ... Validation loss: 0.1500.06033762490844465\n",
            "0.13185942357152963\n",
            "Progress: 92.2% ... Training loss: 0.060 ... Validation loss: 0.1310.06895771251893196\n",
            "0.13921570684256615\n",
            "Progress: 92.2% ... Training loss: 0.068 ... Validation loss: 0.1390.05695745807761422\n",
            "0.135854722761583\n",
            "Progress: 92.2% ... Training loss: 0.056 ... Validation loss: 0.1350.05675960210118884\n",
            "0.1466449842819613\n",
            "Progress: 92.2% ... Training loss: 0.056 ... Validation loss: 0.1460.06320305280828205\n",
            "0.14708746009683898\n",
            "Progress: 92.2% ... Training loss: 0.063 ... Validation loss: 0.1470.057419048236566426\n",
            "0.14805264012671623\n",
            "Progress: 92.3% ... Training loss: 0.057 ... Validation loss: 0.1480.05633341220438103\n",
            "0.1482600593351282\n",
            "Progress: 92.3% ... Training loss: 0.056 ... Validation loss: 0.1480.05894519318272074\n",
            "0.1317815531004821\n",
            "Progress: 92.3% ... Training loss: 0.058 ... Validation loss: 0.1310.05571959910057484\n",
            "0.1350634697212348\n",
            "Progress: 92.3% ... Training loss: 0.055 ... Validation loss: 0.1350.05551073454444147\n",
            "0.14933367638922435\n",
            "Progress: 92.3% ... Training loss: 0.055 ... Validation loss: 0.1490.05876143200409993\n",
            "0.1416455479476097\n",
            "Progress: 92.4% ... Training loss: 0.058 ... Validation loss: 0.1410.0563995377075552\n",
            "0.156067566226543\n",
            "Progress: 92.4% ... Training loss: 0.056 ... Validation loss: 0.1560.05881694509336725\n",
            "0.1454334991830986\n",
            "Progress: 92.4% ... Training loss: 0.058 ... Validation loss: 0.1450.05665353451501807\n",
            "0.1560122184286654\n",
            "Progress: 92.4% ... Training loss: 0.056 ... Validation loss: 0.1560.059816877552686505\n",
            "0.13982605446057203\n",
            "Progress: 92.4% ... Training loss: 0.059 ... Validation loss: 0.1390.06720454815294828\n",
            "0.17680199211316058\n",
            "Progress: 92.5% ... Training loss: 0.067 ... Validation loss: 0.1760.05586098775248965\n",
            "0.14341850120636088\n",
            "Progress: 92.5% ... Training loss: 0.055 ... Validation loss: 0.1430.058076866747339286\n",
            "0.13805010526818629\n",
            "Progress: 92.5% ... Training loss: 0.058 ... Validation loss: 0.1380.056677290360090594\n",
            "0.1415116551057579\n",
            "Progress: 92.5% ... Training loss: 0.056 ... Validation loss: 0.1410.06755831729646809\n",
            "0.1374847224173344\n",
            "Progress: 92.5% ... Training loss: 0.067 ... Validation loss: 0.1370.056029570471961936\n",
            "0.14847978833974465\n",
            "Progress: 92.6% ... Training loss: 0.056 ... Validation loss: 0.1480.05619192497930819\n",
            "0.1422909450518957\n",
            "Progress: 92.6% ... Training loss: 0.056 ... Validation loss: 0.1420.05602096400284385\n",
            "0.14394805825904727\n",
            "Progress: 92.6% ... Training loss: 0.056 ... Validation loss: 0.1430.057837463972369274\n",
            "0.1606948270426815\n",
            "Progress: 92.6% ... Training loss: 0.057 ... Validation loss: 0.1600.06572596075917356\n",
            "0.14241264929931724\n",
            "Progress: 92.6% ... Training loss: 0.065 ... Validation loss: 0.1420.05857736020170858\n",
            "0.1671092620590176\n",
            "Progress: 92.7% ... Training loss: 0.058 ... Validation loss: 0.1670.05763014033525534\n",
            "0.13765227489403806\n",
            "Progress: 92.7% ... Training loss: 0.057 ... Validation loss: 0.1370.0581750133004962\n",
            "0.1379389749080198\n",
            "Progress: 92.7% ... Training loss: 0.058 ... Validation loss: 0.1370.05580175475903393\n",
            "0.14916154126228615\n",
            "Progress: 92.7% ... Training loss: 0.055 ... Validation loss: 0.1490.055328408724506055\n",
            "0.14204572270899307\n",
            "Progress: 92.7% ... Training loss: 0.055 ... Validation loss: 0.1420.05928579362528617\n",
            "0.14699259358951364\n",
            "Progress: 92.8% ... Training loss: 0.059 ... Validation loss: 0.1460.05660328485290449\n",
            "0.13534235094426553\n",
            "Progress: 92.8% ... Training loss: 0.056 ... Validation loss: 0.1350.06460870839101249\n",
            "0.16476998974157614\n",
            "Progress: 92.8% ... Training loss: 0.064 ... Validation loss: 0.1640.056136318570774474\n",
            "0.14641665525575676\n",
            "Progress: 92.8% ... Training loss: 0.056 ... Validation loss: 0.1460.05840899271304898\n",
            "0.13082510203033618\n",
            "Progress: 92.8% ... Training loss: 0.058 ... Validation loss: 0.1300.05711369838544663\n",
            "0.15226441237888397\n",
            "Progress: 92.9% ... Training loss: 0.057 ... Validation loss: 0.1520.060943471194826196\n",
            "0.1329286323643818\n",
            "Progress: 92.9% ... Training loss: 0.060 ... Validation loss: 0.1320.05670937904079506\n",
            "0.13554820773384862\n",
            "Progress: 92.9% ... Training loss: 0.056 ... Validation loss: 0.1350.056014200829704486\n",
            "0.14806872247542913\n",
            "Progress: 92.9% ... Training loss: 0.056 ... Validation loss: 0.1480.05662403298163648\n",
            "0.14204714155805204\n",
            "Progress: 92.9% ... Training loss: 0.056 ... Validation loss: 0.1420.05615444864641689\n",
            "0.14813912804776616\n",
            "Progress: 93.0% ... Training loss: 0.056 ... Validation loss: 0.1480.056679749041701914\n",
            "0.13580033294877264\n",
            "Progress: 93.0% ... Training loss: 0.056 ... Validation loss: 0.1350.06461077337178933\n",
            "0.14668020618725816\n",
            "Progress: 93.0% ... Training loss: 0.064 ... Validation loss: 0.1460.05654343505882627\n",
            "0.13824602005638037\n",
            "Progress: 93.0% ... Training loss: 0.056 ... Validation loss: 0.1380.05967411004653844\n",
            "0.15838511544377695\n",
            "Progress: 93.0% ... Training loss: 0.059 ... Validation loss: 0.1580.05871443150656596\n",
            "0.13571359256093338\n",
            "Progress: 93.1% ... Training loss: 0.058 ... Validation loss: 0.1350.056063009324551556\n",
            "0.13760536476908358\n",
            "Progress: 93.1% ... Training loss: 0.056 ... Validation loss: 0.1370.05684630649394195\n",
            "0.13233321237984416\n",
            "Progress: 93.1% ... Training loss: 0.056 ... Validation loss: 0.1320.06269357958217642\n",
            "0.15562111089492034\n",
            "Progress: 93.1% ... Training loss: 0.062 ... Validation loss: 0.1550.05682498471747627\n",
            "0.13525454864471612\n",
            "Progress: 93.1% ... Training loss: 0.056 ... Validation loss: 0.1350.06037145537252963\n",
            "0.13017429907045763\n",
            "Progress: 93.2% ... Training loss: 0.060 ... Validation loss: 0.1300.060306582918814224\n",
            "0.1425751949767974\n",
            "Progress: 93.2% ... Training loss: 0.060 ... Validation loss: 0.1420.05670408629130419\n",
            "0.13605298576427147\n",
            "Progress: 93.2% ... Training loss: 0.056 ... Validation loss: 0.1360.056043993676894714\n",
            "0.13409575946719063\n",
            "Progress: 93.2% ... Training loss: 0.056 ... Validation loss: 0.1340.05611375127702614\n",
            "0.13091113403112836\n",
            "Progress: 93.2% ... Training loss: 0.056 ... Validation loss: 0.1300.05663648996136896\n",
            "0.12789895428303405\n",
            "Progress: 93.3% ... Training loss: 0.056 ... Validation loss: 0.1270.05742796181287357\n",
            "0.13689065729667138\n",
            "Progress: 93.3% ... Training loss: 0.057 ... Validation loss: 0.1360.05879602727199609\n",
            "0.13150815325683102\n",
            "Progress: 93.3% ... Training loss: 0.058 ... Validation loss: 0.1310.057172302498660725\n",
            "0.1335804054662259\n",
            "Progress: 93.3% ... Training loss: 0.057 ... Validation loss: 0.1330.05835904645409366\n",
            "0.13606004507718442\n",
            "Progress: 93.3% ... Training loss: 0.058 ... Validation loss: 0.1360.056314139022097906\n",
            "0.1354555031672285\n",
            "Progress: 93.4% ... Training loss: 0.056 ... Validation loss: 0.1350.05915295968035908\n",
            "0.15612677181887047\n",
            "Progress: 93.4% ... Training loss: 0.059 ... Validation loss: 0.1560.06184146752669359\n",
            "0.13915666559062617\n",
            "Progress: 93.4% ... Training loss: 0.061 ... Validation loss: 0.1390.056141349304957074\n",
            "0.14103422175002692\n",
            "Progress: 93.4% ... Training loss: 0.056 ... Validation loss: 0.1410.059206839269156736\n",
            "0.1547504124443512\n",
            "Progress: 93.4% ... Training loss: 0.059 ... Validation loss: 0.1540.05605336553512163\n",
            "0.144838684204673\n",
            "Progress: 93.5% ... Training loss: 0.056 ... Validation loss: 0.1440.05694752654311182\n",
            "0.14524564148501967\n",
            "Progress: 93.5% ... Training loss: 0.056 ... Validation loss: 0.1450.05598807711604905\n",
            "0.14345006126776208\n",
            "Progress: 93.5% ... Training loss: 0.055 ... Validation loss: 0.1430.05657201384422671\n",
            "0.14204582199224203\n",
            "Progress: 93.5% ... Training loss: 0.056 ... Validation loss: 0.1420.05814063539338248\n",
            "0.14083252667675106\n",
            "Progress: 93.5% ... Training loss: 0.058 ... Validation loss: 0.1400.058008180580692396\n",
            "0.13744702699261094\n",
            "Progress: 93.6% ... Training loss: 0.058 ... Validation loss: 0.1370.06116177495512285\n",
            "0.14240801182763388\n",
            "Progress: 93.6% ... Training loss: 0.061 ... Validation loss: 0.1420.05759353712637549\n",
            "0.14473334253043493\n",
            "Progress: 93.6% ... Training loss: 0.057 ... Validation loss: 0.1440.05786581809934734\n",
            "0.14845742000580367\n",
            "Progress: 93.6% ... Training loss: 0.057 ... Validation loss: 0.1480.057492657293886223\n",
            "0.15236357382642973\n",
            "Progress: 93.6% ... Training loss: 0.057 ... Validation loss: 0.1520.05647458651517455\n",
            "0.14103270380232627\n",
            "Progress: 93.7% ... Training loss: 0.056 ... Validation loss: 0.1410.06850099738592066\n",
            "0.17324810097911833\n",
            "Progress: 93.7% ... Training loss: 0.068 ... Validation loss: 0.1730.059636198314304986\n",
            "0.13548006486879693\n",
            "Progress: 93.7% ... Training loss: 0.059 ... Validation loss: 0.1350.05746779108504026\n",
            "0.13228763363393187\n",
            "Progress: 93.7% ... Training loss: 0.057 ... Validation loss: 0.1320.05596851250395783\n",
            "0.13591924798864483\n",
            "Progress: 93.7% ... Training loss: 0.055 ... Validation loss: 0.1350.057753352410759885\n",
            "0.14991651094451366\n",
            "Progress: 93.8% ... Training loss: 0.057 ... Validation loss: 0.1490.06273512452729653\n",
            "0.15701416571621055\n",
            "Progress: 93.8% ... Training loss: 0.062 ... Validation loss: 0.1570.06009951156992158\n",
            "0.13653309852110726\n",
            "Progress: 93.8% ... Training loss: 0.060 ... Validation loss: 0.1360.06015548848864708\n",
            "0.12645687412755177\n",
            "Progress: 93.8% ... Training loss: 0.060 ... Validation loss: 0.1260.05823838219428963\n",
            "0.1304175692548082\n",
            "Progress: 93.8% ... Training loss: 0.058 ... Validation loss: 0.1300.05598991445019036\n",
            "0.1382297311814065\n",
            "Progress: 93.9% ... Training loss: 0.055 ... Validation loss: 0.1380.05600391961783231\n",
            "0.15086287456390363\n",
            "Progress: 93.9% ... Training loss: 0.056 ... Validation loss: 0.1500.057082748337580494\n",
            "0.14595813921481887\n",
            "Progress: 93.9% ... Training loss: 0.057 ... Validation loss: 0.1450.056323827033923635\n",
            "0.13722102495276686\n",
            "Progress: 93.9% ... Training loss: 0.056 ... Validation loss: 0.1370.0566527933499999\n",
            "0.13579356367328513\n",
            "Progress: 93.9% ... Training loss: 0.056 ... Validation loss: 0.1350.05585976379257756\n",
            "0.13492992988357966\n",
            "Progress: 94.0% ... Training loss: 0.055 ... Validation loss: 0.1340.061578607260608674\n",
            "0.16015053603454688\n",
            "Progress: 94.0% ... Training loss: 0.061 ... Validation loss: 0.1600.06839648972051512\n",
            "0.16380346167570073\n",
            "Progress: 94.0% ... Training loss: 0.068 ... Validation loss: 0.1630.05867004413451617\n",
            "0.13393847048071367\n",
            "Progress: 94.0% ... Training loss: 0.058 ... Validation loss: 0.1330.05846210879260365\n",
            "0.1382303850198527\n",
            "Progress: 94.0% ... Training loss: 0.058 ... Validation loss: 0.1380.061000790819945076\n",
            "0.14160997874762665\n",
            "Progress: 94.1% ... Training loss: 0.061 ... Validation loss: 0.1410.0614715242890095\n",
            "0.16627562580057706\n",
            "Progress: 94.1% ... Training loss: 0.061 ... Validation loss: 0.1660.06015153154566689\n",
            "0.13955131749337776\n",
            "Progress: 94.1% ... Training loss: 0.060 ... Validation loss: 0.1390.06056712466405733\n",
            "0.1784128179680751\n",
            "Progress: 94.1% ... Training loss: 0.060 ... Validation loss: 0.1780.05617414761819925\n",
            "0.1590434152512747\n",
            "Progress: 94.1% ... Training loss: 0.056 ... Validation loss: 0.1590.058773086432330604\n",
            "0.15951842011831746\n",
            "Progress: 94.2% ... Training loss: 0.058 ... Validation loss: 0.1590.057337226020212906\n",
            "0.14887647021775846\n",
            "Progress: 94.2% ... Training loss: 0.057 ... Validation loss: 0.1480.058878719656146807\n",
            "0.16663595091119895\n",
            "Progress: 94.2% ... Training loss: 0.058 ... Validation loss: 0.1660.05558033638903994\n",
            "0.1411847099007882\n",
            "Progress: 94.2% ... Training loss: 0.055 ... Validation loss: 0.1410.05541323540125857\n",
            "0.14884800970271403\n",
            "Progress: 94.2% ... Training loss: 0.055 ... Validation loss: 0.1480.05715999093721115\n",
            "0.14023308167361154\n",
            "Progress: 94.3% ... Training loss: 0.057 ... Validation loss: 0.1400.055910474636720615\n",
            "0.14106684113063925\n",
            "Progress: 94.3% ... Training loss: 0.055 ... Validation loss: 0.1410.055984791246489196\n",
            "0.14357235607425645\n",
            "Progress: 94.3% ... Training loss: 0.055 ... Validation loss: 0.1430.060800279927117756\n",
            "0.14631759053790744\n",
            "Progress: 94.3% ... Training loss: 0.060 ... Validation loss: 0.1460.05515379313757118\n",
            "0.14432108958372217\n",
            "Progress: 94.3% ... Training loss: 0.055 ... Validation loss: 0.1440.05544175951053441\n",
            "0.1448552880435989\n",
            "Progress: 94.4% ... Training loss: 0.055 ... Validation loss: 0.1440.05912595487544091\n",
            "0.16161247680711965\n",
            "Progress: 94.4% ... Training loss: 0.059 ... Validation loss: 0.1610.061180938059423685\n",
            "0.13822013875418604\n",
            "Progress: 94.4% ... Training loss: 0.061 ... Validation loss: 0.1380.05764280451599358\n",
            "0.15955273813246743\n",
            "Progress: 94.4% ... Training loss: 0.057 ... Validation loss: 0.1590.05565868655442824\n",
            "0.1464063706275342\n",
            "Progress: 94.4% ... Training loss: 0.055 ... Validation loss: 0.1460.05723483184610163\n",
            "0.14672289416393505\n",
            "Progress: 94.5% ... Training loss: 0.057 ... Validation loss: 0.1460.058254595900816865\n",
            "0.14830093261489044\n",
            "Progress: 94.5% ... Training loss: 0.058 ... Validation loss: 0.1480.05749499024715361\n",
            "0.14633650144805696\n",
            "Progress: 94.5% ... Training loss: 0.057 ... Validation loss: 0.1460.062047001450458356\n",
            "0.13337877604819312\n",
            "Progress: 94.5% ... Training loss: 0.062 ... Validation loss: 0.1330.05814289458510547\n",
            "0.13630904884796075\n",
            "Progress: 94.5% ... Training loss: 0.058 ... Validation loss: 0.1360.05635401729594557\n",
            "0.14347372553726384\n",
            "Progress: 94.6% ... Training loss: 0.056 ... Validation loss: 0.1430.057772430119204105\n",
            "0.15881272382825345\n",
            "Progress: 94.6% ... Training loss: 0.057 ... Validation loss: 0.1580.05681191479112829\n",
            "0.14224454524644534\n",
            "Progress: 94.6% ... Training loss: 0.056 ... Validation loss: 0.1420.05633507623795307\n",
            "0.14699721736561505\n",
            "Progress: 94.6% ... Training loss: 0.056 ... Validation loss: 0.1460.05924132098537879\n",
            "0.13531881839681595\n",
            "Progress: 94.6% ... Training loss: 0.059 ... Validation loss: 0.1350.055582795413478076\n",
            "0.1380984381002843\n",
            "Progress: 94.7% ... Training loss: 0.055 ... Validation loss: 0.1380.05612672778407965\n",
            "0.15109234162097654\n",
            "Progress: 94.7% ... Training loss: 0.056 ... Validation loss: 0.1510.055637658735724856\n",
            "0.14628399178938495\n",
            "Progress: 94.7% ... Training loss: 0.055 ... Validation loss: 0.1460.055640049578210574\n",
            "0.1489546058653514\n",
            "Progress: 94.7% ... Training loss: 0.055 ... Validation loss: 0.1480.06167386328715506\n",
            "0.12852416996670965\n",
            "Progress: 94.7% ... Training loss: 0.061 ... Validation loss: 0.1280.05734077891779471\n",
            "0.14316074468223788\n",
            "Progress: 94.8% ... Training loss: 0.057 ... Validation loss: 0.1430.05801559226594961\n",
            "0.1360481890443831\n",
            "Progress: 94.8% ... Training loss: 0.058 ... Validation loss: 0.1360.055955584871173594\n",
            "0.14530679558122606\n",
            "Progress: 94.8% ... Training loss: 0.055 ... Validation loss: 0.1450.0646178568688753\n",
            "0.13206952818763604\n",
            "Progress: 94.8% ... Training loss: 0.064 ... Validation loss: 0.1320.057532664314647666\n",
            "0.14488417247999316\n",
            "Progress: 94.8% ... Training loss: 0.057 ... Validation loss: 0.1440.05621172055324594\n",
            "0.14571401981099552\n",
            "Progress: 94.9% ... Training loss: 0.056 ... Validation loss: 0.1450.05591702693859206\n",
            "0.1415732768442649\n",
            "Progress: 94.9% ... Training loss: 0.055 ... Validation loss: 0.1410.05507781941623775\n",
            "0.143930167757618\n",
            "Progress: 94.9% ... Training loss: 0.055 ... Validation loss: 0.1430.05547543700685302\n",
            "0.1449060812334954\n",
            "Progress: 94.9% ... Training loss: 0.055 ... Validation loss: 0.1440.059085572743686664\n",
            "0.15647145119364464\n",
            "Progress: 94.9% ... Training loss: 0.059 ... Validation loss: 0.1560.05995235423792409\n",
            "0.14035201440337344\n",
            "Progress: 95.0% ... Training loss: 0.059 ... Validation loss: 0.1400.05825999606211602\n",
            "0.13669001734995784\n",
            "Progress: 95.0% ... Training loss: 0.058 ... Validation loss: 0.1360.056278542992765454\n",
            "0.14875856199778817\n",
            "Progress: 95.0% ... Training loss: 0.056 ... Validation loss: 0.1480.058364223037865946\n",
            "0.1567447297948935\n",
            "Progress: 95.0% ... Training loss: 0.058 ... Validation loss: 0.1560.05974055110498489\n",
            "0.14712795708311216\n",
            "Progress: 95.0% ... Training loss: 0.059 ... Validation loss: 0.1470.05666910934063956\n",
            "0.13471423617202438\n",
            "Progress: 95.1% ... Training loss: 0.056 ... Validation loss: 0.1340.05629147924667595\n",
            "0.13840554717896944\n",
            "Progress: 95.1% ... Training loss: 0.056 ... Validation loss: 0.1380.05626934483979129\n",
            "0.14613146222782783\n",
            "Progress: 95.1% ... Training loss: 0.056 ... Validation loss: 0.1460.05667917366647576\n",
            "0.13641666221763427\n",
            "Progress: 95.1% ... Training loss: 0.056 ... Validation loss: 0.1360.060091859079968335\n",
            "0.1326926169140505\n",
            "Progress: 95.1% ... Training loss: 0.060 ... Validation loss: 0.1320.06914390110224676\n",
            "0.16822825641489375\n",
            "Progress: 95.2% ... Training loss: 0.069 ... Validation loss: 0.1680.05649925529617133\n",
            "0.1365314712978324\n",
            "Progress: 95.2% ... Training loss: 0.056 ... Validation loss: 0.1360.05604689871664529\n",
            "0.1342612466276093\n",
            "Progress: 95.2% ... Training loss: 0.056 ... Validation loss: 0.1340.055658167919679084\n",
            "0.13627374837293246\n",
            "Progress: 95.2% ... Training loss: 0.055 ... Validation loss: 0.1360.05666199744543254\n",
            "0.14046750623398663\n",
            "Progress: 95.2% ... Training loss: 0.056 ... Validation loss: 0.1400.05677663076712507\n",
            "0.14141009944286248\n",
            "Progress: 95.3% ... Training loss: 0.056 ... Validation loss: 0.1410.05980520592754452\n",
            "0.13387042560047455\n",
            "Progress: 95.3% ... Training loss: 0.059 ... Validation loss: 0.1330.06507838278666817\n",
            "0.1628722596065545\n",
            "Progress: 95.3% ... Training loss: 0.065 ... Validation loss: 0.1620.06932425437785973\n",
            "0.13719161554238185\n",
            "Progress: 95.3% ... Training loss: 0.069 ... Validation loss: 0.1370.05855154710488021\n",
            "0.14224842647468502\n",
            "Progress: 95.3% ... Training loss: 0.058 ... Validation loss: 0.1420.05593290353149091\n",
            "0.1404198370517829\n",
            "Progress: 95.4% ... Training loss: 0.055 ... Validation loss: 0.1400.058079387430497\n",
            "0.15343081877896103\n",
            "Progress: 95.4% ... Training loss: 0.058 ... Validation loss: 0.1530.05586622369004296\n",
            "0.14164755502894782\n",
            "Progress: 95.4% ... Training loss: 0.055 ... Validation loss: 0.1410.05598867708574407\n",
            "0.15852433689513684\n",
            "Progress: 95.4% ... Training loss: 0.055 ... Validation loss: 0.1580.05481651086796892\n",
            "0.1462562775754078\n",
            "Progress: 95.4% ... Training loss: 0.054 ... Validation loss: 0.1460.055737767912107876\n",
            "0.15437118928586122\n",
            "Progress: 95.5% ... Training loss: 0.055 ... Validation loss: 0.1540.055538891128101175\n",
            "0.15211833531366326\n",
            "Progress: 95.5% ... Training loss: 0.055 ... Validation loss: 0.1520.05618131470005083\n",
            "0.15389072740547877\n",
            "Progress: 95.5% ... Training loss: 0.056 ... Validation loss: 0.1530.058606167258231937\n",
            "0.14749601134272722\n",
            "Progress: 95.5% ... Training loss: 0.058 ... Validation loss: 0.1470.056993740478525574\n",
            "0.1501721666094286\n",
            "Progress: 95.5% ... Training loss: 0.056 ... Validation loss: 0.1500.0562244372107929\n",
            "0.15140661509059442\n",
            "Progress: 95.6% ... Training loss: 0.056 ... Validation loss: 0.1510.05593678377704304\n",
            "0.15592049781316103\n",
            "Progress: 95.6% ... Training loss: 0.055 ... Validation loss: 0.1550.06052376562393229\n",
            "0.16459570483953154\n",
            "Progress: 95.6% ... Training loss: 0.060 ... Validation loss: 0.1640.06230017614078405\n",
            "0.15749767438258921\n",
            "Progress: 95.6% ... Training loss: 0.062 ... Validation loss: 0.1570.06546179433625024\n",
            "0.15234693001008237\n",
            "Progress: 95.6% ... Training loss: 0.065 ... Validation loss: 0.1520.06789142392741496\n",
            "0.16133708023212803\n",
            "Progress: 95.7% ... Training loss: 0.067 ... Validation loss: 0.1610.05683133662263741\n",
            "0.14684952054675432\n",
            "Progress: 95.7% ... Training loss: 0.056 ... Validation loss: 0.1460.056818216771135445\n",
            "0.1424791801530207\n",
            "Progress: 95.7% ... Training loss: 0.056 ... Validation loss: 0.1420.05609492262334495\n",
            "0.14441237799008352\n",
            "Progress: 95.7% ... Training loss: 0.056 ... Validation loss: 0.1440.05605482974839975\n",
            "0.13904010270582445\n",
            "Progress: 95.7% ... Training loss: 0.056 ... Validation loss: 0.1390.05564512805358821\n",
            "0.14577542662130533\n",
            "Progress: 95.8% ... Training loss: 0.055 ... Validation loss: 0.1450.05963692721905774\n",
            "0.15304672100830027\n",
            "Progress: 95.8% ... Training loss: 0.059 ... Validation loss: 0.1530.056562832794630584\n",
            "0.145269490781773\n",
            "Progress: 95.8% ... Training loss: 0.056 ... Validation loss: 0.1450.05819758647703721\n",
            "0.14581829317371447\n",
            "Progress: 95.8% ... Training loss: 0.058 ... Validation loss: 0.1450.05637767801775452\n",
            "0.13550719585111914\n",
            "Progress: 95.8% ... Training loss: 0.056 ... Validation loss: 0.1350.060457632195409554\n",
            "0.15386929821682663\n",
            "Progress: 95.9% ... Training loss: 0.060 ... Validation loss: 0.1530.058433228137022515\n",
            "0.1582628971712712\n",
            "Progress: 95.9% ... Training loss: 0.058 ... Validation loss: 0.1580.06027472338599089\n",
            "0.1697947889231043\n",
            "Progress: 95.9% ... Training loss: 0.060 ... Validation loss: 0.1690.0557612958408955\n",
            "0.1457546607979999\n",
            "Progress: 95.9% ... Training loss: 0.055 ... Validation loss: 0.1450.05501093442666412\n",
            "0.14054696112518594\n",
            "Progress: 95.9% ... Training loss: 0.055 ... Validation loss: 0.1400.05628011341169515\n",
            "0.1519290747575771\n",
            "Progress: 96.0% ... Training loss: 0.056 ... Validation loss: 0.1510.06194402043383062\n",
            "0.1606277607273753\n",
            "Progress: 96.0% ... Training loss: 0.061 ... Validation loss: 0.1600.055094069160226446\n",
            "0.13911047410508215\n",
            "Progress: 96.0% ... Training loss: 0.055 ... Validation loss: 0.1390.06083217544585642\n",
            "0.1580933802251328\n",
            "Progress: 96.0% ... Training loss: 0.060 ... Validation loss: 0.1580.05759271681165726\n",
            "0.14024823104415177\n",
            "Progress: 96.0% ... Training loss: 0.057 ... Validation loss: 0.1400.05638580160356386\n",
            "0.1343876419339315\n",
            "Progress: 96.1% ... Training loss: 0.056 ... Validation loss: 0.1340.05635591419252603\n",
            "0.1444772520611249\n",
            "Progress: 96.1% ... Training loss: 0.056 ... Validation loss: 0.1440.06171236793757151\n",
            "0.13924816555730252\n",
            "Progress: 96.1% ... Training loss: 0.061 ... Validation loss: 0.1390.06076656638125334\n",
            "0.1627819343352783\n",
            "Progress: 96.1% ... Training loss: 0.060 ... Validation loss: 0.1620.05879578976719428\n",
            "0.14194943776366356\n",
            "Progress: 96.1% ... Training loss: 0.058 ... Validation loss: 0.1410.06140930197322168\n",
            "0.14166960542168866\n",
            "Progress: 96.2% ... Training loss: 0.061 ... Validation loss: 0.1410.05703057737726493\n",
            "0.14198492308610475\n",
            "Progress: 96.2% ... Training loss: 0.057 ... Validation loss: 0.1410.05702595359468722\n",
            "0.1459723274241916\n",
            "Progress: 96.2% ... Training loss: 0.057 ... Validation loss: 0.1450.05657443211167242\n",
            "0.14207256296218884\n",
            "Progress: 96.2% ... Training loss: 0.056 ... Validation loss: 0.1420.05974794562980109\n",
            "0.13696101523355508\n",
            "Progress: 96.2% ... Training loss: 0.059 ... Validation loss: 0.1360.06768979675860173\n",
            "0.18986911667023346\n",
            "Progress: 96.3% ... Training loss: 0.067 ... Validation loss: 0.1890.05748726279348278\n",
            "0.13531819468353726\n",
            "Progress: 96.3% ... Training loss: 0.057 ... Validation loss: 0.1350.06068246370491482\n",
            "0.15893266929297098\n",
            "Progress: 96.3% ... Training loss: 0.060 ... Validation loss: 0.1580.05515822709989136\n",
            "0.14119281643649867\n",
            "Progress: 96.3% ... Training loss: 0.055 ... Validation loss: 0.1410.056204573921146764\n",
            "0.14693455859765667\n",
            "Progress: 96.3% ... Training loss: 0.056 ... Validation loss: 0.1460.05517961239174121\n",
            "0.14861569324604124\n",
            "Progress: 96.4% ... Training loss: 0.055 ... Validation loss: 0.1480.056830449041642264\n",
            "0.13779157156565056\n",
            "Progress: 96.4% ... Training loss: 0.056 ... Validation loss: 0.1370.05615910571520428\n",
            "0.1387880820410989\n",
            "Progress: 96.4% ... Training loss: 0.056 ... Validation loss: 0.1380.05757074312237805\n",
            "0.14578601433387578\n",
            "Progress: 96.4% ... Training loss: 0.057 ... Validation loss: 0.1450.05759408941176753\n",
            "0.13665168812098524\n",
            "Progress: 96.4% ... Training loss: 0.057 ... Validation loss: 0.1360.06244513647404522\n",
            "0.13616391201091244\n",
            "Progress: 96.5% ... Training loss: 0.062 ... Validation loss: 0.1360.059104394036927944\n",
            "0.13305550601846916\n",
            "Progress: 96.5% ... Training loss: 0.059 ... Validation loss: 0.1330.05573152113130335\n",
            "0.13641715942658517\n",
            "Progress: 96.5% ... Training loss: 0.055 ... Validation loss: 0.1360.05494075093836615\n",
            "0.1410454460442233\n",
            "Progress: 96.5% ... Training loss: 0.054 ... Validation loss: 0.1410.057055622642091595\n",
            "0.14949059015247276\n",
            "Progress: 96.5% ... Training loss: 0.057 ... Validation loss: 0.1490.05660545212051523\n",
            "0.1574288551409816\n",
            "Progress: 96.6% ... Training loss: 0.056 ... Validation loss: 0.1570.05714392235960909\n",
            "0.15717967654553822\n",
            "Progress: 96.6% ... Training loss: 0.057 ... Validation loss: 0.1570.05594082430629002\n",
            "0.1472565407148842\n",
            "Progress: 96.6% ... Training loss: 0.055 ... Validation loss: 0.1470.055367280422987976\n",
            "0.14301527526771154\n",
            "Progress: 96.6% ... Training loss: 0.055 ... Validation loss: 0.1430.06352288577796861\n",
            "0.1428430068017957\n",
            "Progress: 96.6% ... Training loss: 0.063 ... Validation loss: 0.1420.055621536657070605\n",
            "0.1458982521584242\n",
            "Progress: 96.7% ... Training loss: 0.055 ... Validation loss: 0.1450.0556023606710504\n",
            "0.14860628721118235\n",
            "Progress: 96.7% ... Training loss: 0.055 ... Validation loss: 0.1480.059172513046610115\n",
            "0.16697405137242607\n",
            "Progress: 96.7% ... Training loss: 0.059 ... Validation loss: 0.1660.05581357469204291\n",
            "0.15485186557714542\n",
            "Progress: 96.7% ... Training loss: 0.055 ... Validation loss: 0.1540.05561666031652232\n",
            "0.14901467826614814\n",
            "Progress: 96.7% ... Training loss: 0.055 ... Validation loss: 0.1490.0575435112909978\n",
            "0.1402209595535545\n",
            "Progress: 96.8% ... Training loss: 0.057 ... Validation loss: 0.1400.057079862668234414\n",
            "0.1394193801526362\n",
            "Progress: 96.8% ... Training loss: 0.057 ... Validation loss: 0.1390.05687408460596833\n",
            "0.1487616552015802\n",
            "Progress: 96.8% ... Training loss: 0.056 ... Validation loss: 0.1480.056805909331723844\n",
            "0.14444457051697415\n",
            "Progress: 96.8% ... Training loss: 0.056 ... Validation loss: 0.1440.05661085649918141\n",
            "0.1403426081139593\n",
            "Progress: 96.8% ... Training loss: 0.056 ... Validation loss: 0.1400.05795065436281826\n",
            "0.1498197788573388\n",
            "Progress: 96.9% ... Training loss: 0.057 ... Validation loss: 0.1490.05762372979677296\n",
            "0.15515062195915308\n",
            "Progress: 96.9% ... Training loss: 0.057 ... Validation loss: 0.1550.05703812453297857\n",
            "0.15248669139101198\n",
            "Progress: 96.9% ... Training loss: 0.057 ... Validation loss: 0.1520.06545756653694679\n",
            "0.18786196461629873\n",
            "Progress: 96.9% ... Training loss: 0.065 ... Validation loss: 0.1870.06244617355417496\n",
            "0.18065033097462022\n",
            "Progress: 96.9% ... Training loss: 0.062 ... Validation loss: 0.1800.05729625702297642\n",
            "0.1523941630238201\n",
            "Progress: 97.0% ... Training loss: 0.057 ... Validation loss: 0.1520.05650825412649762\n",
            "0.14115798912194757\n",
            "Progress: 97.0% ... Training loss: 0.056 ... Validation loss: 0.1410.05615538812368635\n",
            "0.14766051878257125\n",
            "Progress: 97.0% ... Training loss: 0.056 ... Validation loss: 0.1470.0558031006351649\n",
            "0.1568224195395394\n",
            "Progress: 97.0% ... Training loss: 0.055 ... Validation loss: 0.1560.057591310384993905\n",
            "0.14346317136305659\n",
            "Progress: 97.0% ... Training loss: 0.057 ... Validation loss: 0.1430.05553824497625031\n",
            "0.14849896310824726\n",
            "Progress: 97.1% ... Training loss: 0.055 ... Validation loss: 0.1480.05733533422812639\n",
            "0.14918267991438613\n",
            "Progress: 97.1% ... Training loss: 0.057 ... Validation loss: 0.1490.05749221219757502\n",
            "0.16433139492421145\n",
            "Progress: 97.1% ... Training loss: 0.057 ... Validation loss: 0.1640.07368655691911119\n",
            "0.19749976777828304\n",
            "Progress: 97.1% ... Training loss: 0.073 ... Validation loss: 0.1970.05649315614814793\n",
            "0.13769137551332772\n",
            "Progress: 97.1% ... Training loss: 0.056 ... Validation loss: 0.1370.05823439483484999\n",
            "0.14579465516511603\n",
            "Progress: 97.2% ... Training loss: 0.058 ... Validation loss: 0.1450.05578900966319439\n",
            "0.14712959854575286\n",
            "Progress: 97.2% ... Training loss: 0.055 ... Validation loss: 0.1470.06674640191868215\n",
            "0.137213049949971\n",
            "Progress: 97.2% ... Training loss: 0.066 ... Validation loss: 0.1370.08869023109101877\n",
            "0.14183332160934442\n",
            "Progress: 97.2% ... Training loss: 0.088 ... Validation loss: 0.1410.05919797563289855\n",
            "0.1614919804226052\n",
            "Progress: 97.2% ... Training loss: 0.059 ... Validation loss: 0.1610.05717832235970724\n",
            "0.14249048148910612\n",
            "Progress: 97.3% ... Training loss: 0.057 ... Validation loss: 0.1420.061029345691308606\n",
            "0.15480638805757635\n",
            "Progress: 97.3% ... Training loss: 0.061 ... Validation loss: 0.1540.055607731881039346\n",
            "0.15172975092782215\n",
            "Progress: 97.3% ... Training loss: 0.055 ... Validation loss: 0.1510.06629301158044555\n",
            "0.14191447882859073\n",
            "Progress: 97.3% ... Training loss: 0.066 ... Validation loss: 0.1410.05637909414307757\n",
            "0.1602102699940367\n",
            "Progress: 97.3% ... Training loss: 0.056 ... Validation loss: 0.1600.06318295925238214\n",
            "0.18782086460333527\n",
            "Progress: 97.4% ... Training loss: 0.063 ... Validation loss: 0.1870.056898269103113824\n",
            "0.14263175255954552\n",
            "Progress: 97.4% ... Training loss: 0.056 ... Validation loss: 0.1420.05528826349570647\n",
            "0.15334186227850624\n",
            "Progress: 97.4% ... Training loss: 0.055 ... Validation loss: 0.1530.05679234991980942\n",
            "0.14408463852523803\n",
            "Progress: 97.4% ... Training loss: 0.056 ... Validation loss: 0.1440.05914946264666616\n",
            "0.15814935092840016\n",
            "Progress: 97.4% ... Training loss: 0.059 ... Validation loss: 0.1580.056227423624816045\n",
            "0.14043630765285953\n",
            "Progress: 97.5% ... Training loss: 0.056 ... Validation loss: 0.1400.061814428172739834\n",
            "0.14165074369062106\n",
            "Progress: 97.5% ... Training loss: 0.061 ... Validation loss: 0.1410.058593911379944574\n",
            "0.16220064830122552\n",
            "Progress: 97.5% ... Training loss: 0.058 ... Validation loss: 0.1620.05633270825191527\n",
            "0.1584145026308554\n",
            "Progress: 97.5% ... Training loss: 0.056 ... Validation loss: 0.1580.05618930591694126\n",
            "0.16126716099894184\n",
            "Progress: 97.5% ... Training loss: 0.056 ... Validation loss: 0.1610.07336436838326707\n",
            "0.14078251808594114\n",
            "Progress: 97.6% ... Training loss: 0.073 ... Validation loss: 0.1400.054808758698089696\n",
            "0.14268371673718816\n",
            "Progress: 97.6% ... Training loss: 0.054 ... Validation loss: 0.1420.05551833610880897\n",
            "0.13574632322434776\n",
            "Progress: 97.6% ... Training loss: 0.055 ... Validation loss: 0.1350.056558821614679705\n",
            "0.14380627413024968\n",
            "Progress: 97.6% ... Training loss: 0.056 ... Validation loss: 0.1430.05817070733735263\n",
            "0.14673157118735156\n",
            "Progress: 97.6% ... Training loss: 0.058 ... Validation loss: 0.1460.05533779655223305\n",
            "0.1391630129618873\n",
            "Progress: 97.7% ... Training loss: 0.055 ... Validation loss: 0.1390.05836099590905068\n",
            "0.13961662740702616\n",
            "Progress: 97.7% ... Training loss: 0.058 ... Validation loss: 0.1390.05606928972738309\n",
            "0.14973017887144396\n",
            "Progress: 97.7% ... Training loss: 0.056 ... Validation loss: 0.1490.0575656873719816\n",
            "0.1441733977597345\n",
            "Progress: 97.7% ... Training loss: 0.057 ... Validation loss: 0.1440.05560531781145128\n",
            "0.13677288465938892\n",
            "Progress: 97.7% ... Training loss: 0.055 ... Validation loss: 0.1360.05738602460020158\n",
            "0.14787207825637533\n",
            "Progress: 97.8% ... Training loss: 0.057 ... Validation loss: 0.1470.059835985575949956\n",
            "0.15883802781999407\n",
            "Progress: 97.8% ... Training loss: 0.059 ... Validation loss: 0.1580.05778641347127793\n",
            "0.1330278091633654\n",
            "Progress: 97.8% ... Training loss: 0.057 ... Validation loss: 0.1330.05639122067119765\n",
            "0.13066629311301786\n",
            "Progress: 97.8% ... Training loss: 0.056 ... Validation loss: 0.1300.0558180562405106\n",
            "0.13069861806439842\n",
            "Progress: 97.8% ... Training loss: 0.055 ... Validation loss: 0.1300.05878960446587864\n",
            "0.15042219182239047\n",
            "Progress: 97.9% ... Training loss: 0.058 ... Validation loss: 0.1500.06069298659661258\n",
            "0.13272103882147354\n",
            "Progress: 97.9% ... Training loss: 0.060 ... Validation loss: 0.1320.05721807521328506\n",
            "0.13057669464510857\n",
            "Progress: 97.9% ... Training loss: 0.057 ... Validation loss: 0.1300.05867773252569581\n",
            "0.14508011682109953\n",
            "Progress: 97.9% ... Training loss: 0.058 ... Validation loss: 0.1450.05494699169832839\n",
            "0.1351453170926202\n",
            "Progress: 97.9% ... Training loss: 0.054 ... Validation loss: 0.1350.05689734817800871\n",
            "0.12874300242899483\n",
            "Progress: 98.0% ... Training loss: 0.056 ... Validation loss: 0.1280.05525566135066942\n",
            "0.14087452329413824\n",
            "Progress: 98.0% ... Training loss: 0.055 ... Validation loss: 0.1400.05519087321054589\n",
            "0.13441908148479334\n",
            "Progress: 98.0% ... Training loss: 0.055 ... Validation loss: 0.1340.06410788547003211\n",
            "0.12832382039001117\n",
            "Progress: 98.0% ... Training loss: 0.064 ... Validation loss: 0.1280.058121887290608604\n",
            "0.1402405418623411\n",
            "Progress: 98.0% ... Training loss: 0.058 ... Validation loss: 0.1400.06123886656089461\n",
            "0.13539630315514553\n",
            "Progress: 98.1% ... Training loss: 0.061 ... Validation loss: 0.1350.054159003873568375\n",
            "0.13436902736909453\n",
            "Progress: 98.1% ... Training loss: 0.054 ... Validation loss: 0.1340.05654825753308496\n",
            "0.14629630049682413\n",
            "Progress: 98.1% ... Training loss: 0.056 ... Validation loss: 0.1460.055720977467631674\n",
            "0.13487727592567758\n",
            "Progress: 98.1% ... Training loss: 0.055 ... Validation loss: 0.1340.05754885049047648\n",
            "0.14644039320365887\n",
            "Progress: 98.1% ... Training loss: 0.057 ... Validation loss: 0.1460.062023022752871096\n",
            "0.1571743388509059\n",
            "Progress: 98.2% ... Training loss: 0.062 ... Validation loss: 0.1570.05665715795043113\n",
            "0.1346364476949443\n",
            "Progress: 98.2% ... Training loss: 0.056 ... Validation loss: 0.1340.055127148877486994\n",
            "0.14317002738742118\n",
            "Progress: 98.2% ... Training loss: 0.055 ... Validation loss: 0.1430.0554548890606938\n",
            "0.1421870627347459\n",
            "Progress: 98.2% ... Training loss: 0.055 ... Validation loss: 0.1420.057714839900903155\n",
            "0.13156144242203863\n",
            "Progress: 98.2% ... Training loss: 0.057 ... Validation loss: 0.1310.05616664320680018\n",
            "0.1294125060545412\n",
            "Progress: 98.3% ... Training loss: 0.056 ... Validation loss: 0.1290.059095766888951705\n",
            "0.12772527973506953\n",
            "Progress: 98.3% ... Training loss: 0.059 ... Validation loss: 0.1270.0556985491299066\n",
            "0.13550758953863773\n",
            "Progress: 98.3% ... Training loss: 0.055 ... Validation loss: 0.1350.055562039458096485\n",
            "0.13401262912067038\n",
            "Progress: 98.3% ... Training loss: 0.055 ... Validation loss: 0.1340.056990888568372\n",
            "0.13294465758208682\n",
            "Progress: 98.3% ... Training loss: 0.056 ... Validation loss: 0.1320.05708212275460704\n",
            "0.14633702521791178\n",
            "Progress: 98.4% ... Training loss: 0.057 ... Validation loss: 0.1460.06180445886725782\n",
            "0.1569213687223489\n",
            "Progress: 98.4% ... Training loss: 0.061 ... Validation loss: 0.1560.05740666912554301\n",
            "0.1427331408789218\n",
            "Progress: 98.4% ... Training loss: 0.057 ... Validation loss: 0.1420.05597806315286546\n",
            "0.13422554066639622\n",
            "Progress: 98.4% ... Training loss: 0.055 ... Validation loss: 0.1340.05584901714341597\n",
            "0.13858407727191357\n",
            "Progress: 98.4% ... Training loss: 0.055 ... Validation loss: 0.1380.06214000996781353\n",
            "0.1556128771721997\n",
            "Progress: 98.5% ... Training loss: 0.062 ... Validation loss: 0.1550.05695026003612675\n",
            "0.13642630688294982\n",
            "Progress: 98.5% ... Training loss: 0.056 ... Validation loss: 0.1360.05934743015059408\n",
            "0.13954796195007324\n",
            "Progress: 98.5% ... Training loss: 0.059 ... Validation loss: 0.1390.05838286880433494\n",
            "0.13625671707840467\n",
            "Progress: 98.5% ... Training loss: 0.058 ... Validation loss: 0.1360.056667676042223065\n",
            "0.13783974416486022\n",
            "Progress: 98.5% ... Training loss: 0.056 ... Validation loss: 0.1370.058295600455443776\n",
            "0.14879375321382002\n",
            "Progress: 98.6% ... Training loss: 0.058 ... Validation loss: 0.1480.055392561533878046\n",
            "0.1343099093317908\n",
            "Progress: 98.6% ... Training loss: 0.055 ... Validation loss: 0.1340.055428478994037085\n",
            "0.13531773098806174\n",
            "Progress: 98.6% ... Training loss: 0.055 ... Validation loss: 0.1350.07169301618465151\n",
            "0.14382578920913738\n",
            "Progress: 98.6% ... Training loss: 0.071 ... Validation loss: 0.1430.0614137673605242\n",
            "0.1342452822721407\n",
            "Progress: 98.6% ... Training loss: 0.061 ... Validation loss: 0.1340.058678693268690775\n",
            "0.1437921162712184\n",
            "Progress: 98.7% ... Training loss: 0.058 ... Validation loss: 0.1430.056989216261006635\n",
            "0.13765324276551522\n",
            "Progress: 98.7% ... Training loss: 0.056 ... Validation loss: 0.1370.06117953190921506\n",
            "0.1566064158418614\n",
            "Progress: 98.7% ... Training loss: 0.061 ... Validation loss: 0.1560.056031524326768084\n",
            "0.14274623711906234\n",
            "Progress: 98.7% ... Training loss: 0.056 ... Validation loss: 0.1420.06042213058179002\n",
            "0.16226182966009806\n",
            "Progress: 98.7% ... Training loss: 0.060 ... Validation loss: 0.1620.056818977724947047\n",
            "0.15197599056896446\n",
            "Progress: 98.8% ... Training loss: 0.056 ... Validation loss: 0.1510.06012410422365118\n",
            "0.156527676193551\n",
            "Progress: 98.8% ... Training loss: 0.060 ... Validation loss: 0.1560.057040830265089854\n",
            "0.14393219025410697\n",
            "Progress: 98.8% ... Training loss: 0.057 ... Validation loss: 0.1430.05683027677172478\n",
            "0.14549095781634117\n",
            "Progress: 98.8% ... Training loss: 0.056 ... Validation loss: 0.1450.06216886575147826\n",
            "0.15226496870562825\n",
            "Progress: 98.8% ... Training loss: 0.062 ... Validation loss: 0.1520.05524556393652136\n",
            "0.1344480044897746\n",
            "Progress: 98.9% ... Training loss: 0.055 ... Validation loss: 0.1340.057812894039635604\n",
            "0.1407503388129235\n",
            "Progress: 98.9% ... Training loss: 0.057 ... Validation loss: 0.1400.05950157337356433\n",
            "0.1406214446426862\n",
            "Progress: 98.9% ... Training loss: 0.059 ... Validation loss: 0.1400.06135069665717171\n",
            "0.1393734926423943\n",
            "Progress: 98.9% ... Training loss: 0.061 ... Validation loss: 0.1390.05578774669622902\n",
            "0.1449347330267794\n",
            "Progress: 98.9% ... Training loss: 0.055 ... Validation loss: 0.1440.05587355393128735\n",
            "0.13741075168783226\n",
            "Progress: 99.0% ... Training loss: 0.055 ... Validation loss: 0.1370.057272124892697174\n",
            "0.14184894649304958\n",
            "Progress: 99.0% ... Training loss: 0.057 ... Validation loss: 0.1410.060077050689833635\n",
            "0.14714175510141583\n",
            "Progress: 99.0% ... Training loss: 0.060 ... Validation loss: 0.1470.05621286172879292\n",
            "0.14354028778284714\n",
            "Progress: 99.0% ... Training loss: 0.056 ... Validation loss: 0.1430.05550091980334055\n",
            "0.14123024934870396\n",
            "Progress: 99.0% ... Training loss: 0.055 ... Validation loss: 0.1410.055796908159410095\n",
            "0.13973335386679506\n",
            "Progress: 99.1% ... Training loss: 0.055 ... Validation loss: 0.1390.057212459076974996\n",
            "0.13972916065601207\n",
            "Progress: 99.1% ... Training loss: 0.057 ... Validation loss: 0.1390.056580540146933464\n",
            "0.13462127252151412\n",
            "Progress: 99.1% ... Training loss: 0.056 ... Validation loss: 0.1340.060315210717181995\n",
            "0.14277213591880822\n",
            "Progress: 99.1% ... Training loss: 0.060 ... Validation loss: 0.1420.061584200367802065\n",
            "0.14536250492096087\n",
            "Progress: 99.1% ... Training loss: 0.061 ... Validation loss: 0.1450.055601911481483475\n",
            "0.14143658617409446\n",
            "Progress: 99.2% ... Training loss: 0.055 ... Validation loss: 0.1410.05936986245737577\n",
            "0.13464353429998965\n",
            "Progress: 99.2% ... Training loss: 0.059 ... Validation loss: 0.1340.05817662160149191\n",
            "0.15922551458633444\n",
            "Progress: 99.2% ... Training loss: 0.058 ... Validation loss: 0.1590.05693090739281696\n",
            "0.13663924343731787\n",
            "Progress: 99.2% ... Training loss: 0.056 ... Validation loss: 0.1360.05721019492446256\n",
            "0.15029686421217214\n",
            "Progress: 99.2% ... Training loss: 0.057 ... Validation loss: 0.1500.055897781544582914\n",
            "0.13164180281435925\n",
            "Progress: 99.3% ... Training loss: 0.055 ... Validation loss: 0.1310.057556229451906755\n",
            "0.1319586621120244\n",
            "Progress: 99.3% ... Training loss: 0.057 ... Validation loss: 0.1310.0574375206821519\n",
            "0.1340923220641226\n",
            "Progress: 99.3% ... Training loss: 0.057 ... Validation loss: 0.1340.05817061831940542\n",
            "0.15984920204399927\n",
            "Progress: 99.3% ... Training loss: 0.058 ... Validation loss: 0.1590.05481441550011605\n",
            "0.14173318693528741\n",
            "Progress: 99.3% ... Training loss: 0.054 ... Validation loss: 0.1410.0546509349581545\n",
            "0.1435861791034999\n",
            "Progress: 99.4% ... Training loss: 0.054 ... Validation loss: 0.1430.05536726725532266\n",
            "0.14434952373795049\n",
            "Progress: 99.4% ... Training loss: 0.055 ... Validation loss: 0.1440.05825044894748225\n",
            "0.16034646987402573\n",
            "Progress: 99.4% ... Training loss: 0.058 ... Validation loss: 0.1600.057012765793397605\n",
            "0.13802673307469732\n",
            "Progress: 99.4% ... Training loss: 0.057 ... Validation loss: 0.1380.054033749307884495\n",
            "0.14050137356088832\n",
            "Progress: 99.4% ... Training loss: 0.054 ... Validation loss: 0.1400.05525327971838597\n",
            "0.1375625255152711\n",
            "Progress: 99.5% ... Training loss: 0.055 ... Validation loss: 0.1370.05683934692397706\n",
            "0.13817208748735144\n",
            "Progress: 99.5% ... Training loss: 0.056 ... Validation loss: 0.1380.056063995361549324\n",
            "0.14242723124983225\n",
            "Progress: 99.5% ... Training loss: 0.056 ... Validation loss: 0.1420.05502083444396073\n",
            "0.13635310838330236\n",
            "Progress: 99.5% ... Training loss: 0.055 ... Validation loss: 0.1360.05583827856261735\n",
            "0.13960087108945093\n",
            "Progress: 99.5% ... Training loss: 0.055 ... Validation loss: 0.1390.055987462480086576\n",
            "0.15626375332406775\n",
            "Progress: 99.6% ... Training loss: 0.055 ... Validation loss: 0.1560.05629930624429953\n",
            "0.14800159368013785\n",
            "Progress: 99.6% ... Training loss: 0.056 ... Validation loss: 0.1480.0570513299141639\n",
            "0.14883380431851512\n",
            "Progress: 99.6% ... Training loss: 0.057 ... Validation loss: 0.1480.056595729829806686\n",
            "0.1465509460676247\n",
            "Progress: 99.6% ... Training loss: 0.056 ... Validation loss: 0.1460.058098025811161684\n",
            "0.1504154390000199\n",
            "Progress: 99.6% ... Training loss: 0.058 ... Validation loss: 0.1500.05727096859282204\n",
            "0.13401365004756455\n",
            "Progress: 99.7% ... Training loss: 0.057 ... Validation loss: 0.1340.05831729188020126\n",
            "0.14459437160646132\n",
            "Progress: 99.7% ... Training loss: 0.058 ... Validation loss: 0.1440.059337750146320166\n",
            "0.1347946974002643\n",
            "Progress: 99.7% ... Training loss: 0.059 ... Validation loss: 0.1340.05633362237275998\n",
            "0.1567025914055541\n",
            "Progress: 99.7% ... Training loss: 0.056 ... Validation loss: 0.1560.06349113250060943\n",
            "0.13941324156413185\n",
            "Progress: 99.7% ... Training loss: 0.063 ... Validation loss: 0.1390.05529042469049716\n",
            "0.14362391134427055\n",
            "Progress: 99.8% ... Training loss: 0.055 ... Validation loss: 0.1430.05521195543945021\n",
            "0.14527640518667037\n",
            "Progress: 99.8% ... Training loss: 0.055 ... Validation loss: 0.1450.05693470459667417\n",
            "0.15806794290429957\n",
            "Progress: 99.8% ... Training loss: 0.056 ... Validation loss: 0.1580.0569247569524459\n",
            "0.15193966237944967\n",
            "Progress: 99.8% ... Training loss: 0.056 ... Validation loss: 0.1510.05653980086613667\n",
            "0.13838081594996096\n",
            "Progress: 99.8% ... Training loss: 0.056 ... Validation loss: 0.1380.05594833608596514\n",
            "0.13673630563050246\n",
            "Progress: 99.9% ... Training loss: 0.055 ... Validation loss: 0.1360.054885018081552016\n",
            "0.13971626728508002\n",
            "Progress: 99.9% ... Training loss: 0.054 ... Validation loss: 0.1390.05510565219979098\n",
            "0.14830653015988468\n",
            "Progress: 99.9% ... Training loss: 0.055 ... Validation loss: 0.1480.05585280733274664\n",
            "0.15209501075998025\n",
            "Progress: 99.9% ... Training loss: 0.055 ... Validation loss: 0.1520.06017530102619898\n",
            "0.14171158835554148\n",
            "Progress: 99.9% ... Training loss: 0.060 ... Validation loss: 0.141\n",
            "Finally\n",
            "Finished training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b5xUtKbTxFX1",
        "outputId": "45f15aa4-31fd-4800-dac5-39e64b3eee62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(losses['train'], label='Training loss')\n",
        "plt.plot(losses['validation'], label='Validation loss')\n",
        "plt.legend()\n",
        "plt.ylim(ymax=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0018055289219656226, 2.0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM3GPLHZxFX3"
      },
      "source": [
        "## Check out your predictions\n",
        "\n",
        "Here, use the test data to view how well your network is modeling the data. If something is completely wrong here, make sure each step in your network is implemented correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "7HIQG4JTxFX3",
        "outputId": "a8b76033-73d4-4d0e-bab0-45ad367b770d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8,4))\n",
        "rides =pd.read_csv('hour.csv')\n",
        "mean, std = scaled_features['cnt']\n",
        "predictions = network.run(test_features)*std + mean\n",
        "ax.plot(predictions[0], label='Prediction')\n",
        "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
        "ax.set_xlim(right=len(predictions))\n",
        "ax.legend()\n",
        "\n",
        "dates = pd.to_datetime(rides.loc[test_data.index]['dteday'])\n",
        "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
        "ax.set_xticks(np.arange(len(dates))[12::24])\n",
        "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-pAS6YIOb-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}